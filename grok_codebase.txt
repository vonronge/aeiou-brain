# CODEBASE DUMP - D:\AEIOU_Brain
# Generated: 2026-01-03 17:22:29
# Max File Size: 100KB
================================================================================

--- FILE: config.json ---
{
  "system_name": "AEIOU_v3.1",
  "evolution_settings": {
    "shadow_mode_enabled": false,
    "commando_protocol": true,
    "mutation_rate": "high"
  }
}

--- FILE: diffusion_history.json ---
["D:/Training_Data", "D:/Top Comics", "D:/Top books training", "D:/New folder"]

--- FILE: genesis.py ---
import os
import json


def create_file(path, content=""):
    if not os.path.exists(path):
        with open(path, "w", encoding='utf-8') as f:
            f.write(content)
        print(f"   + Created file: {path}")


def create_folder(path):
    if not os.path.exists(path):
        os.makedirs(path)
        print(f"   + Created folder: {path}")


def genesis():
    print("⚡ AEIOU SYSTEM REPAIR PROTOCOL INITIATED...")
    root = os.getcwd()

    # 1. ESSENTIAL DIRECTORIES
    dirs = [
        "lobes",
        "Genetics",
        "Organelles",
        "Plugins",
        "memories",  # <--- The lost folder
        "system/lobegrams",
        "system/genegrams",
        "library/engrams",
        "library/socialgrams"
    ]

    for d in dirs:
        create_folder(os.path.join(root, d))

    # 2. PYTHON PACKAGE MARKERS (Crucial for imports)
    pkgs = ["Genetics", "Organelles", "Plugins", "lobes"]
    for p in pkgs:
        create_file(os.path.join(root, p, "__init__.py"), "")

    # 3. RESTORE CONFIG (Colors)
    config_path = os.path.join(root, "settings.json")
    if not os.path.exists(config_path):
        print("   ! settings.json missing. Restoring default Dark Theme...")
        default_conf = {
            "system_name": "AEIOU_v16",
            "colors": {
                "BG_MAIN": "#0b0f19", "BG_CARD": "#131620",
                "FG_TEXT": "#E3E3E3", "FG_DIM": "#8e9198",
                "ACCENT": "#A8C7FA", "BTN": "#1E222D",
                "BTN_ACT": "#2B3042", "SUCCESS": "#81C995",
                "ERROR": "#F28B82", "WARN": "#FDD663",
                "BORDER": "#444444", "GRID": "#333333",
                "SCROLL": "#2B3042"
            },
            "evolution_settings": {
                "shadow_mode_enabled": False,
                "commando_protocol": True
            }
        }
        create_file(config_path, json.dumps(default_conf, indent=2))

    # 4. MEMORY INDEX PLACEHOLDER
    # Prevents Hippocampus from crashing on first load
    mem_index = os.path.join(root, "memories", "replay_buffer.jsonl")
    create_file(mem_index, "")

    print("==========================================")
    print("✨ SYSTEM REPAIRED. ALL SYSTEMS ONLINE.")
    print("==========================================")


if __name__ == "__main__":
    genesis()

--- FILE: gpu_check.py ---
import torch

print("System Check:")
print(f"   > PyTorch Version: {torch.__version__}")
print(f"   > CUDA Available:  {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"   > GPU Detected:    {torch.cuda.get_device_name(0)}")
    print(f"   > VRAM Total:      {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    # Quick Math Test
    x = torch.rand(5, 3).cuda()
    print(f"   > Test Tensor:     Allocated successfully on GPU.")
else:
    print("   ❌ ERROR: You are running on CPU. The 3080 Ti is sleeping.")

--- FILE: GUI.py ---
import tkinter as tk
from tkinter import ttk, messagebox
import os
import sys
import json
import torch
import importlib.util
import traceback
import threading

# --- PATH SETUP ---
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# --- CONFIG ---
BRAIN_DIR = os.path.join(current_dir, "lobes")
GENETICS_DIR = os.path.join(current_dir, "Genetics")
PLUGINS_DIR = os.path.join(current_dir, "Plugins")
ORGANELLES_DIR = os.path.join(current_dir, "Organelles")
MEMORIES_DIR = os.path.join(current_dir, "memories")
DATA_DIR = os.path.join(current_dir, "Training_Data")
CONFIG_FILE = os.path.join(current_dir, "settings.json")

# Ensure core dirs exist
for d in [BRAIN_DIR, GENETICS_DIR, PLUGINS_DIR, ORGANELLES_DIR, MEMORIES_DIR]:
    if not os.path.exists(d): os.makedirs(d)

# --- FALLBACK DATA DIR CREATION ---
if not os.path.exists(DATA_DIR):
    try:
        os.makedirs(DATA_DIR)
        print(f"[SYS] Created default Training_Data folder at: {DATA_DIR}")
    except Exception as e:
        print(f"[ERR] Failed to create Data Dir: {e}")

# --- IMPORTS ---
try:
    from Organelles.ribosome import Organelle_Ribosome
except ImportError as e:
    messagebox.showerror("Critical Error", f"Could not import Organelles/ribosome.py\n\nError: {e}")
    sys.exit()

try:
    from Organelles.hippocampus import Organelle_Hippocampus
except ImportError as e:
    print(f"Warning: Hippocampus disabled. ({e})")
    Organelle_Hippocampus = None


class DraggableButton(tk.Button):
    def __init__(self, parent, app, text, command):
        super().__init__(parent, text=text, command=command,
                         font=("Segoe UI", 10), relief="flat", anchor="w", padx=15, pady=8, cursor="hand2")
        self.parent = parent
        self.app = app
        self.text_val = text
        self.command_func = command

        self._drag_data = {"y": 0}

        self.bind("<Button-1>", self._on_drag_start)
        self.bind("<B1-Motion>", self._on_drag_motion)
        self.bind("<ButtonRelease-1>", self._on_drag_stop)

    def _on_drag_start(self, event):
        self._drag_data["y"] = event.y
        if self.command_func: self.command_func()

    def _on_drag_motion(self, event):
        delta = event.y - self._drag_data["y"]
        if abs(delta) < 10: return

        x, y = self.winfo_pointerxy()
        target = self.winfo_containing(x, y)

        if isinstance(target, DraggableButton) and target != self:
            try:
                my_idx = self.app.sidebar_order.index(self)
                target_idx = self.app.sidebar_order.index(target)
                self.app.sidebar_order[my_idx], self.app.sidebar_order[target_idx] = \
                    self.app.sidebar_order[target_idx], self.app.sidebar_order[my_idx]
                self.app._repack_sidebar()
            except ValueError:
                pass

    def _on_drag_stop(self, event):
        self.app._save_sidebar_order()


class BrainApp(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("AEIOU Brain - The Unified Cortex (v24.5 Configurable)")

        if os.name == 'nt':
            try:
                self.update()
                import ctypes
                ctypes.windll.dwmapi.DwmSetWindowAttribute(
                    ctypes.windll.user32.GetParent(self.winfo_id()), 20, ctypes.byref(ctypes.c_int(2)), 4
                )
            except:
                pass

        self.paths = {
            "root": current_dir, "lobes": BRAIN_DIR, "genetics": GENETICS_DIR,
            "plugins": PLUGINS_DIR, "memories": MEMORIES_DIR, "data": DATA_DIR
        }

        # --- STATE ---
        self.plugins = {}
        self.plugin_frames = {}
        self.sidebar_buttons = {}
        self.sidebar_order = []

        # Device Detection
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        self.gpu_lock = threading.Lock()

        # Organelles
        self.ribosome = Organelle_Ribosome(self.device)
        self.hippocampus = Organelle_Hippocampus(MEMORIES_DIR, self.device) if Organelle_Hippocampus else None

        # Lobes
        self.lobes = {1: None, 2: None, 3: None, 4: None}
        self.lobe_genomes = {1: "gpt2", 2: "gpt2", 3: "gpt2", 4: "gpt2"}
        self.lobe_types = {1: None, 2: None, 3: None, 4: None}
        self.optimizers = {1: None, 2: None, 3: None, 4: None}
        self.scalers = {1: None, 2: None, 3: None, 4: None}
        self.active_lobe = tk.IntVar(value=1)
        self.graph_data = {}
        self.lobe_btns = {}

        # Colors
        self.colors = {
            "BG_MAIN": "#0b0f19", "BG_CARD": "#131620", "FG_TEXT": "#E3E3E3",
            "FG_DIM": "#8e9198", "ACCENT": "#A8C7FA", "BTN": "#1E222D",
            "BTN_ACT": "#2B3042", "SUCCESS": "#81C995", "ERROR": "#F28B82",
            "WARN": "#FDD663", "BORDER": "#444444", "GRID": "#333333", "SCROLL": "#2B3042"
        }

        # Load Config (UPDATES self.paths["data"] if set)
        self._load_config()
        self.configure(bg=self.colors["BG_MAIN"])

        w, h = 1600, 900
        x = (self.winfo_screenwidth() - w) // 2
        y = 50
        self.geometry(f'{w}x{h}+{int(x)}+{int(y)}')
        self.protocol("WM_DELETE_WINDOW", self._on_close)

        self._setup_layout()
        self.apply_theme()
        self._load_plugins()
        self.load_state()

    def _load_config(self):
        if os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r') as f:
                    data = json.load(f)
                    self.colors.update(data.get("colors", {}))

                    # Custom Data Directory Support
                    custom_data = data.get("data_dir")
                    if custom_data:
                        # Handle absolute vs relative path
                        if os.path.isabs(custom_data):
                            self.paths["data"] = custom_data
                        else:
                            self.paths["data"] = os.path.join(self.paths["root"], custom_data)

                        # Auto-create if missing
                        if not os.path.exists(self.paths["data"]):
                            try:
                                os.makedirs(self.paths["data"])
                                print(f"[SYS] Created custom data dir: {self.paths['data']}")
                            except:
                                pass
            except Exception as e:
                print(f"[ERR] Config Load Error: {e}")

    # ... [Rest of methods: _setup_layout, _add_plugin, etc. remain unchanged] ...
    # (Including _setup_layout, _on_sb_configure, _on_mousewheel, _setup_header,
    #  _add_plugin, _show_plugin, _repack_sidebar, _load_plugins,
    #  _save/_restore_sidebar_order, _load_single_lobe, save/load_state, apply_theme)

    def _setup_layout(self):
        self.main_split = tk.Frame(self, bg=self.colors["BG_MAIN"])
        self.main_split.pack(fill="both", expand=True)

        self.sidebar_container = tk.Frame(self.main_split, bg=self.colors["BG_CARD"], width=240)
        self.sidebar_container.pack(side="left", fill="y")
        self.sidebar_container.pack_propagate(False)

        self.sb_scroll = ttk.Scrollbar(self.sidebar_container, orient="vertical")
        self.sb_scroll.pack(side="right", fill="y")

        self.sb_canvas = tk.Canvas(self.sidebar_container, bg=self.colors["BG_CARD"],
                                   highlightthickness=0, yscrollcommand=self.sb_scroll.set)
        self.sb_canvas.pack(side="left", fill="both", expand=True)
        self.sb_scroll.config(command=self.sb_canvas.yview)

        self.sidebar_frame = tk.Frame(self.sb_canvas, bg=self.colors["BG_CARD"])
        self.sb_window = self.sb_canvas.create_window((0, 0), window=self.sidebar_frame, anchor="nw")

        self.sidebar_frame.bind("<Configure>", self._on_sb_configure)
        self.sb_canvas.bind("<Configure>", self._on_sb_canvas_configure)

        if os.name == 'nt' or sys.platform == 'darwin':
            self.sidebar_frame.bind_all("<MouseWheel>", self._on_mousewheel)
        else:
            self.sidebar_frame.bind_all("<Button-4>", self._on_mousewheel)
            self.sidebar_frame.bind_all("<Button-5>", self._on_mousewheel)

        lbl = tk.Label(self.sidebar_frame, text="NEURAL CORE", bg=self.colors["BG_CARD"],
                       fg=self.colors["FG_DIM"], font=("Segoe UI", 9, "bold"), pady=15)
        lbl.pack(fill="x")

        self.content_area = tk.Frame(self.main_split, bg=self.colors["BG_MAIN"])
        self.content_area.pack(side="right", fill="both", expand=True)

        self._setup_header()

        self.plugin_container = tk.Frame(self.content_area, bg=self.colors["BG_MAIN"])
        self.plugin_container.pack(fill="both", expand=True)
        self.plugin_container.grid_rowconfigure(0, weight=1)
        self.plugin_container.grid_columnconfigure(0, weight=1)

    def _on_sb_configure(self, event):
        self.sb_canvas.configure(scrollregion=self.sb_canvas.bbox("all"))

    def _on_sb_canvas_configure(self, event):
        self.sb_canvas.itemconfig(self.sb_window, width=event.width)

    def _on_mousewheel(self, event):
        x, y = self.winfo_pointerxy()
        widget = self.winfo_containing(x, y)
        if str(widget).startswith(str(self.sidebar_container)):
            if sys.platform == 'darwin':
                self.sb_canvas.yview_scroll(int(-1 * event.delta), "units")
            elif hasattr(event, 'num') and event.num == 4:
                self.sb_canvas.yview_scroll(-1, "units")
            elif hasattr(event, 'num') and event.num == 5:
                self.sb_canvas.yview_scroll(1, "units")
            else:
                self.sb_canvas.yview_scroll(int(-1 * (event.delta / 120)), "units")

    def _setup_header(self):
        header = tk.Frame(self.content_area, bg=self.colors["BG_MAIN"], height=50)
        header.pack(fill="x", side="top", pady=(0, 5))

        tk.Label(header, text="ACTIVE LOBE:", bg=self.colors["BG_MAIN"], fg=self.colors["ACCENT"],
                 font=("Segoe UI", 10, "bold")).pack(side="left", padx=15)

        for i in range(1, 5):
            btn = ttk.Radiobutton(header, text=f"LOBE {i}", variable=self.active_lobe, value=i, style="Lobe.TButton")
            btn.pack(side="left", padx=5)
            self.lobe_btns[i] = btn

        tk.Label(header, text=f"Running on: {self.device.upper()}", bg=self.colors["BG_MAIN"],
                 fg=self.colors["FG_DIM"]).pack(side="right", padx=20)

    def _add_plugin(self, name, plugin_class):
        frame = ttk.Frame(self.plugin_container)
        frame.grid(row=0, column=0, sticky="nsew")
        instance = plugin_class(frame, self)
        display_name = instance.name
        self.plugins[name] = instance
        self.plugin_frames[name] = frame
        btn = DraggableButton(self.sidebar_frame, self, text=display_name, command=lambda: self._show_plugin(name))
        self.sidebar_buttons[name] = btn
        self.sidebar_order.append(btn)
        return instance

    def _show_plugin(self, name):
        frame = self.plugin_frames.get(name)
        if frame: frame.tkraise()
        self.notebook = type('MockNotebook', (), {})()
        self.notebook.select = lambda: name
        self.notebook.tab = lambda x, option: self.plugins[name].name if option == "text" else None
        for n, btn in self.sidebar_buttons.items():
            if n == name:
                btn.config(bg=self.colors["BG_MAIN"], fg=self.colors["ACCENT"], relief="sunken")
            else:
                btn.config(bg=self.colors["BG_CARD"], fg=self.colors["FG_TEXT"], relief="flat")

    def _repack_sidebar(self):
        for btn in self.sidebar_buttons.values(): btn.pack_forget()
        for btn in self.sidebar_order: btn.pack(fill="x", pady=1)

    def _load_plugins(self):
        order = [
            "tab_cortex.py", "tab_playground.py", "tab_memory.py", "tab_memory_agent.py",
            "tab_rlm.py", "tab_trainer.py", "tab_diffusion_trainer.py", "tab_dream.py",
            "tab_factory.py", "tab_video_factory.py", "tab_comic.py", "tab_symbiosis.py",
            "tab_council.py", "tab_graphs.py", "tab_settings.py"
        ]
        found_files = [f for f in os.listdir(PLUGINS_DIR) if f.startswith("tab_") and f.endswith(".py")]
        found_files.sort(key=lambda x: order.index(x) if x in order else 999)
        first_plugin = None
        for filename in found_files:
            try:
                module_name = filename[:-3]
                path = os.path.join(PLUGINS_DIR, filename)
                spec = importlib.util.spec_from_file_location(module_name, path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, "Plugin"):
                    self._add_plugin(module_name, module.Plugin)
                    if not first_plugin: first_plugin = module_name
                    print(f"[SYS] Loaded Plugin: {module_name}")
            except Exception as e:
                print(f"[ERR] Failed to load {filename}: {e}")
                traceback.print_exc()
        self._restore_sidebar_order()
        self._repack_sidebar()
        if first_plugin: self._show_plugin(first_plugin)

    def _save_sidebar_order(self):
        try:
            data = {}
            if os.path.exists(CONFIG_FILE):
                with open(CONFIG_FILE, 'r') as f: data = json.load(f)
            ordered_names = []
            btn_to_name = {v: k for k, v in self.sidebar_buttons.items()}
            for btn in self.sidebar_order:
                name = btn_to_name.get(btn)
                if name: ordered_names.append(name)
            data["sidebar_order"] = ordered_names
            with open(CONFIG_FILE, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            print(f"Sidebar Save Error: {e}")

    def _restore_sidebar_order(self):
        try:
            if not os.path.exists(CONFIG_FILE): return
            with open(CONFIG_FILE, 'r') as f:
                data = json.load(f)
            saved = data.get("sidebar_order", [])
            if not saved: return
            new_order = []
            for name in saved:
                if name in self.sidebar_buttons:
                    new_order.append(self.sidebar_buttons[name])
            for btn in self.sidebar_order:
                if btn not in new_order: new_order.append(btn)
            self.sidebar_order = new_order
        except:
            pass

    def _load_single_lobe(self, lobe_id, path, silent=False):
        try:
            data = torch.load(path, map_location=self.device)
            genome_name = data.get("genome", "gpt2") if isinstance(data, dict) else "gpt2"
            model_type = data.get("model_type") if isinstance(data, dict) else None
            if model_type is None:
                model_type = "diffusion" if "diffusion" in genome_name.lower() else "ar"
            self.lobe_types[lobe_id] = model_type
            cortex = self.plugins.get('tab_cortex')
            if cortex and genome_name in cortex.available_genetics:
                module = cortex.available_genetics[genome_name]
            else:
                if not silent: messagebox.showerror("Error", f"Genetics '{genome_name}' not found.")
                return
            state_dict = data["state_dict"] if isinstance(data, dict) else data
            self.lobe_genomes[lobe_id] = genome_name
            brain = module.Model(module.NucleusConfig()).to(self.device)
            brain.load_state_dict(state_dict, strict=False)
            self.lobes[lobe_id] = brain
            if "Muon" in genome_name:
                from Genetics.muon import Muon
                self.optimizers[lobe_id] = Muon(brain.parameters(), lr=0.0005, momentum=0.95)
            else:
                self.optimizers[lobe_id] = torch.optim.AdamW(brain.parameters(), lr=2e-5)
            if self.device == "cuda":
                self.scalers[lobe_id] = torch.cuda.amp.GradScaler()
            else:
                self.scalers[lobe_id] = None
            if hasattr(brain, "tokenizer"): self.ribosome.set_tokenizer(brain.tokenizer)
            self.refresh_header()
            if not silent: print(f"[SYS] Loaded Lobe {lobe_id} ({genome_name} | {model_type})")
        except Exception as e:
            self.lobes[lobe_id] = None
            self.refresh_header()
            if not silent: messagebox.showerror("Load Failed", str(e))

    def save_state(self):
        data = {}
        if os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r') as f:
                    data = json.load(f)
            except:
                pass
        data["last_active_lobe"] = self.active_lobe.get()
        try:
            with open(CONFIG_FILE, 'w') as f:
                json.dump(data, f, indent=2)
        except:
            pass
        self._save_sidebar_order()

    def load_state(self):
        if os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r') as f:
                    data = json.load(f)
                self.active_lobe.set(data.get("last_active_lobe", 1))
            except:
                pass
        self.refresh_header()

    def refresh_header(self):
        for i in range(1, 5):
            style = "LobeLoaded.TButton" if self.lobes[i] else "Lobe.TButton"
            self.lobe_btns[i].configure(style=style)

    def _on_close(self):
        try:
            self.save_state()
            if self.hippocampus: self.hippocampus.save_memory()
        except:
            pass
        self.destroy()

    def apply_theme(self):
        style = ttk.Style()
        style.theme_use('clam')
        c = self.colors

        style.configure(".", background=c["BG_MAIN"], foreground=c["FG_TEXT"], borderwidth=0)
        style.configure("TLabel", background=c["BG_MAIN"], foreground=c["FG_TEXT"], font=("Segoe UI", 9))
        style.configure("Card.TLabel", background=c["BG_CARD"], foreground=c["FG_TEXT"], font=("Segoe UI", 9))
        style.configure("TButton", background=c["BTN"], foreground=c["FG_TEXT"], borderwidth=0, padding=(15, 8),
                        font=("Segoe UI", 9, "bold"))
        style.map("TButton", background=[("active", c["BTN_ACT"]), ("pressed", c["ACCENT"])],
                  foreground=[("pressed", c["BG_MAIN"])])
        style.configure("Lobe.TButton", background=c["BG_CARD"], foreground=c["FG_DIM"], font=("Segoe UI", 10, "bold"),
                        bordercolor=c["BORDER"], borderwidth=1)
        style.map("Lobe.TButton", background=[("selected", c["ACCENT"]), ("active", c["BTN_ACT"])],
                  foreground=[("selected", c["BG_MAIN"])])
        style.configure("LobeLoaded.TButton", background=c["BG_CARD"], foreground=c["SUCCESS"],
                        font=("Segoe UI", 10, "bold"), bordercolor=c["SUCCESS"], borderwidth=1)
        style.map("LobeLoaded.TButton", background=[("selected", c["ACCENT"]), ("active", c["BTN_ACT"])],
                  foreground=[("selected", c["BG_MAIN"])])
        style.configure("TEntry", fieldbackground=c["BG_CARD"], foreground=c["FG_TEXT"], insertcolor=c["ACCENT"],
                        borderwidth=1, bordercolor=c["BORDER"], padding=5)
        style.configure("TLabelframe", background=c["BG_CARD"], borderwidth=1, relief="solid", bordercolor=c["BORDER"])
        style.configure("TLabelframe.Label", background=c["BG_CARD"], foreground=c["ACCENT"],
                        font=("Segoe UI", 9, "bold"))
        style.configure("Card.TFrame", background=c["BG_CARD"])
        style.configure("Treeview", background=c["BG_MAIN"], foreground=c["FG_TEXT"], fieldbackground=c["BG_MAIN"],
                        borderwidth=1, bordercolor=c["BORDER"], font=("Consolas", 9))
        style.configure("Treeview.Heading", background=c["BG_CARD"], foreground=c["FG_TEXT"], borderwidth=1,
                        bordercolor=c["BORDER"], padding=10, relief="flat")
        style.map("Treeview.Heading", background=[("active", c["BTN_ACT"])], foreground=[("active", c["ACCENT"])])
        style.configure("Vertical.TScrollbar", gripcount=0, background=c["SCROLL"], troughcolor=c["BG_CARD"],
                        bordercolor=c["BG_CARD"], lightcolor=c["BG_CARD"], darkcolor=c["BG_CARD"], arrowsize=0)
        style.configure("TCheckbutton", background=c["BG_CARD"], foreground=c["FG_TEXT"], focuscolor=c["BG_CARD"])
        style.map("TCheckbutton", indicatorcolor=[("selected", c["ACCENT"])], background=[("active", c["BG_CARD"])])
        style.configure("TSpinbox", fieldbackground=c["BG_MAIN"], foreground=c["FG_TEXT"], background=c["BTN"],
                        arrowcolor=c["FG_TEXT"], borderwidth=1, bordercolor=c["BORDER"])
        style.configure("TCombobox", fieldbackground=c["BG_CARD"], background=c["BTN"], foreground=c["FG_TEXT"],
                        arrowcolor=c["FG_TEXT"], borderwidth=1)
        style.configure("TSeparator", background=c["BORDER"])

        for name, plugin in self.plugins.items():
            if hasattr(plugin, "on_theme_change"):
                try:
                    plugin.on_theme_change()
                except:
                    pass

        if hasattr(self, 'sidebar_frame'):
            self.sidebar_frame.config(bg=c["BG_CARD"])
            self.sidebar_container.config(bg=c["BG_CARD"])
            self.sb_canvas.config(bg=c["BG_CARD"])
            for btn in self.sidebar_buttons.values():
                btn.config(bg=c["BG_CARD"], fg=c["FG_TEXT"], activebackground=c["BTN_ACT"])


if __name__ == "__main__":
    if not hasattr(sys, 'frozen'):
        app = BrainApp()
        app.mainloop()

--- FILE: settings.json ---
{
  "system_name": "AEIOU_v16",
  "colors": {
    "BG_MAIN": "#0b0f19",
    "BG_CARD": "#131620",
    "FG_TEXT": "#E3E3E3",
    "FG_DIM": "#8e9198",
    "ACCENT": "#A8C7FA",
    "BTN": "#1E222D",
    "BTN_ACT": "#2B3042",
    "SUCCESS": "#81C995",
    "ERROR": "#F28B82",
    "WARN": "#FDD663",
    "BORDER": "#444444",
    "GRID": "#333333",
    "SCROLL": "#2B3042"
  },
  "evolution_settings": {
    "shadow_mode_enabled": false,
    "commando_protocol": true,
    "mutation_rate": "high"
  },
  "last_active_lobe": 3,
  "tab_order": [
    "Cortex Control",
    "Playground",
    "Memory Graph",
    "Memory Agent",
    "Transformer Trainer",
    "Dream State",
    "Lecture Factory",
    "Video Timeline Factory",
    "Comic Scanner",
    "Symbiosis",
    "The Council",
    "Telemetry",
    "System Config",
    "Diffusion Director",
    "RLM Reasoning"
  ],
  "sidebar_order": [
    "tab_cortex",
    "tab_trainer",
    "tab_diffusion_trainer",
    "tab_playground",
    "tab_rlm",
    "tab_dream",
    "tab_memory",
    "tab_memory_agent",
    "tab_symbiosis",
    "tab_council",
    "tab_factory",
    "tab_video_factory",
    "tab_comic",
    "tab_graphs",
    "tab_settings"
  ]
}

--- FILE: telepathy.py ---
import argparse
import os
import sys
import threading
import torch
import time
import importlib.util

# Ensure project root is in path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)


# --- HEADLESS INFRASTRUCTURE ---
class MockVar:
    """Replaces tk.StringVar/IntVar when running headless"""

    def __init__(self, value=None): self._val = value

    def set(self, value): self._val = value

    def get(self): return self._val


class HeadlessApp:
    """Mocks the BrainApp class for plugins"""

    def __init__(self, lobe_id, data_path):
        self.paths = {
            "root": current_dir,
            "lobes": os.path.join(current_dir, "lobes"),
            "genetics": os.path.join(current_dir, "Genetics"),
            "plugins": os.path.join(current_dir, "Plugins"),
            "memories": os.path.join(current_dir, "memories"),
            "data": data_path
        }
        # Dummy colors to prevent plugin crashes
        self.colors = {k: "" for k in ["BG_MAIN", "FG_TEXT", "ACCENT", "BG_CARD", "FG_DIM", "SUCCESS", "WARN", "ERROR"]}

        # --- DEVICE DETECTION (MPS ADDED) ---
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        self.gpu_lock = threading.Lock()

        # Components
        from Organelles.ribosome import Organelle_Ribosome
        self.ribosome = Organelle_Ribosome(self.device)

        self.lobes = {1: None, 2: None, 3: None, 4: None}
        self.lobe_genomes = {}
        self.lobe_types = {}
        self.optimizers = {1: None, 2: None, 3: None, 4: None}
        self.scalers = {1: None, 2: None, 3: None, 4: None}

        self.active_lobe = MockVar(lobe_id)
        self.graph_data = {}

        self._load_lobe(lobe_id)

    def _load_lobe(self, lobe_id):
        path = os.path.join(self.paths['lobes'], f"brain_lobe_{lobe_id}.pt")
        if not os.path.exists(path):
            print(f"[Telepathy] Error: Lobe {lobe_id} not found at {path}")
            sys.exit(1)

        print(f"[Telepathy] Loading Lobe {lobe_id} from disk...")
        try:
            data = torch.load(path, map_location=self.device)
            genome = data.get("genome", "gpt2")
            m_type = data.get("model_type", "ar")

            # Dynamic Import Genetics
            gen_path = os.path.join(self.paths['genetics'], f"{genome}.py")
            if not os.path.exists(gen_path):
                # Fallback search
                found = [f for f in os.listdir(self.paths['genetics']) if f.lower() == f"{genome.lower()}.py"]
                if found: gen_path = os.path.join(self.paths['genetics'], found[0])

            spec = importlib.util.spec_from_file_location(genome, gen_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            config = module.NucleusConfig()
            brain = module.Model(config).to(self.device)
            brain.load_state_dict(data["state_dict"], strict=False)

            self.lobes[lobe_id] = brain
            self.lobe_genomes[lobe_id] = genome
            self.lobe_types[lobe_id] = m_type

            if "Muon" in genome:
                from Genetics.muon import Muon
                self.optimizers[lobe_id] = Muon(brain.parameters(), lr=0.0005, momentum=0.95)
            else:
                self.optimizers[lobe_id] = torch.optim.AdamW(brain.parameters(), lr=2e-5)

            # Scaler handling for MPS vs CUDA
            if self.device == "cuda":
                self.scalers[lobe_id] = torch.cuda.amp.GradScaler()
            else:
                self.scalers[lobe_id] = None

            if hasattr(brain, "tokenizer"):
                self.ribosome.set_tokenizer(brain.tokenizer)

            print(f"[Telepathy] Lobe {lobe_id} Online ({genome} | {m_type}) on {self.device.upper()}")

        except Exception as e:
            print(f"[Telepathy] Load Failed: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


# --- ENTRY POINT ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AEIOU Brain Telepathy Interface")
    parser.add_argument("--headless", action="store_true", help="Run without GUI")
    parser.add_argument("--mode", choices=["train", "diffusion"], help="Operation mode")
    parser.add_argument("--lobe", type=int, default=1, help="Lobe ID to load (1-4)")
    parser.add_argument("--data", default=os.path.join(current_dir, "Training_Data"), help="Path to training data")
    parser.add_argument("--epochs", type=int, default=10, help="Target epochs")

    args = parser.parse_args()

    if not args.headless:
        # GUI Mode
        from GUI import BrainApp

        app = BrainApp()
        app.mainloop()
    else:
        # Headless Mode
        print("--- AEIOU TELEPATHY (HEADLESS) ---")
        if not args.mode:
            print("Error: --mode is required for headless operation.")
            sys.exit(1)

        app = HeadlessApp(args.lobe, args.data)

        if args.mode == "train":
            from Plugins.tab_trainer import Plugin as Trainer

            print(f"[Telepathy] Initializing Transformer Trainer on {args.data}...")

            trainer = Trainer(None, app)  # Pass None as parent

            # Configure
            trainer.folder_path.set(args.data)
            trainer.target_epochs.set(args.epochs)
            trainer.nursery_autofit.set(True)
            trainer.autosave_enabled.set(True)

            trainer._scan_files()
            if len(trainer.training_queue) > 0:
                print(f"[Telepathy] Starting training loop for {args.epochs} epochs...")
                trainer._start_training()
                try:
                    while trainer.is_training:
                        time.sleep(1)
                except KeyboardInterrupt:
                    print("\n[Telepathy] Interrupted. Saving...")
                    trainer._save_session_state()
            else:
                print("[Telepathy] Queue empty. Exiting.")

        elif args.mode == "diffusion":
            from Plugins.tab_diffusion_trainer import Plugin as DiffTrainer

            print(f"[Telepathy] Initializing Diffusion Director on {args.data}...")

            trainer = DiffTrainer(None, app)
            trainer.folder_path.set(args.data)
            trainer.target_epochs.set(args.epochs)

            trainer._scan_files()
            if len(trainer.training_queue) > 0:
                print(f"[Telepathy] Diffusing...")
                trainer._start_training()
                try:
                    while trainer.is_training:
                        time.sleep(1)
                except KeyboardInterrupt:
                    print("\n[Telepathy] Interrupted.")

--- FILE: Genetics\base_multimodal.py ---
import torch
import torch.nn as nn
from Genetics.common import RotaryEmbedding


class MultimodalBase(nn.Module):
    """
    The Foundation for all AEIOU Architectures.
    Enforces consistent sensory fusion order:
    Visual (V) -> Audio (A) -> Control (C) -> Text (T)
    """

    def __init__(self, config):
        super().__init__()
        self.config = config

        # --- UNIFIED PROJECTION LAYERS ---
        self.tok_emb = nn.Embedding(config.vocab_size, config.embed_dim)

        # We check attributes to allow flexible configs, defaulting to standard AEIOU sizes
        vis_dim = getattr(config, 'vis_dim', 768)
        aud_dim = getattr(config, 'aud_dim', 128)
        mot_dim = getattr(config, 'mot_dim', 64)

        self.vis_proj = nn.Linear(vis_dim, config.embed_dim)
        self.aud_proj = nn.Linear(aud_dim, config.embed_dim)
        self.mot_proj = nn.Linear(mot_dim, config.embed_dim)

        # --- SHARED ROPE GENERATOR ---
        # Calculates the frequency matrix once for the maximum context
        # Assumes config has n_heads. If not, defaults to 12.
        n_heads = getattr(config, 'n_heads', 12)
        head_dim = config.embed_dim // n_heads

        # Max seq len 16384 covers standard 8k models + vision tokens
        self.rope = RotaryEmbedding(head_dim, max_seq_len=16384)

    def embed_inputs(self, v, a, t, c=None):
        """
        Fuses modalities into a single causal stream.
        """
        # Safety clamp for text tokens (prevents index errors if vocab size changes)
        if t.max() >= self.config.vocab_size:
            t = torch.clamp(t, 0, self.config.vocab_size - 1)

        parts = []

        # 1. Visual
        if v is not None and v.numel() > 0:
            parts.append(self.vis_proj(v))

        # 2. Audio
        if a is not None and a.numel() > 0:
            parts.append(self.aud_proj(a))

        # 3. Control / Motion
        if c is not None and c.numel() > 0:
            parts.append(self.mot_proj(c))

        # 4. Text (Always present)
        parts.append(self.tok_emb(t))

        # Concatenate along sequence dimension (dim=1)
        x = torch.cat(parts, dim=1)
        return x

    def get_positional_embeddings(self, x):
        """
        Generates RoPE cos/sin tables for the current sequence length.
        """
        device = x.device
        seq_len = x.shape[1]

        # Get pre-computed cos/sin from common.RotaryEmbedding
        rope_cos, rope_sin = self.rope(seq_len)

        return rope_cos.to(device), rope_sin.to(device)

--- FILE: Genetics\common.py ---
import torch
import torch.nn as nn
import torch.nn.functional as F


class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        var = torch.mean(x ** 2, dim=-1, keepdim=True)
        return self.weight * x * torch.rsqrt(var + self.eps)


class SwiGLU(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))


class RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_seq_len=8192):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        t = torch.arange(max_seq_len).float()
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos", emb.cos())
        self.register_buffer("sin", emb.sin())

    def forward(self, seq_len):
        return self.cos[:seq_len, :], self.sin[:seq_len, :]


def apply_rope(x, cos, sin):
    cos = cos.unsqueeze(0).unsqueeze(2)
    sin = sin.unsqueeze(0).unsqueeze(2)
    head_dim = x.shape[-1]
    x1 = x[..., :head_dim // 2]
    x2 = x[..., head_dim // 2:]
    rotated = torch.cat([-x2, x1], dim=-1)
    return (x * cos) + (rotated * sin)


# --- GAME THEORETIC PRUNING LAYER ---
class StrategicLinear(nn.Linear):
    def __init__(self, in_features, out_features, cost_coefficient=1e-4, **kwargs):
        super().__init__(in_features, out_features, **kwargs)
        self.alpha_logits = nn.Parameter(torch.randn(out_features) * 0.1 + 5.0)
        self.cost_coefficient = cost_coefficient

    def get_participation(self):
        return torch.sigmoid(self.alpha_logits)

    def forward(self, input):
        alpha = self.get_participation()
        masked_weight = self.weight * alpha.unsqueeze(1)
        return F.linear(input, masked_weight, self.bias)

    def game_loss(self):
        return self.cost_coefficient * torch.sum(self.get_participation())


# --- MANIFOLD CONSTRAINED UTILS ---
def sinkhorn_knopp(log_matrix, iterations=5):
    # SAFETY CLAMP: Prevent exp() from exploding
    # 30.0 is safe for float32 (e^30 is large but finite)
    # 10.0 is safe for float16
    safe_log = torch.clamp(log_matrix, max=10.0)

    M = torch.exp(safe_log)
    for _ in range(iterations):
        M = M / (M.sum(dim=-1, keepdim=True) + 1e-6)
        M = M / (M.sum(dim=-2, keepdim=True) + 1e-6)
    return M


# --- DEEP DELTA LEARNING OPERATOR ---
class DeltaOperator(nn.Module):
    def __init__(self, dim, beta_scale=2.0):
        super().__init__()
        self.beta_proj = nn.Linear(dim, 1)
        self.k_proj = nn.Linear(dim, dim)
        self.beta_scale = beta_scale

        nn.init.constant_(self.beta_proj.weight, 0)
        nn.init.constant_(self.beta_proj.bias, -2.0)

    def forward(self, x):
        beta = torch.sigmoid(self.beta_proj(x)) * self.beta_scale
        k_raw = self.k_proj(x)
        k_norm = torch.norm(k_raw, dim=-1, keepdim=True) + 1e-6
        k = k_raw / k_norm
        proj_scalar = torch.sum(k * x, dim=-1, keepdim=True)
        rank1_term = proj_scalar * k
        return (1 - beta) * x + beta * rank1_term

--- FILE: Genetics\delta_gpt2.py ---
import torch
import torch.nn as nn
from Genetics.common import RMSNorm, StrategicLinear, sinkhorn_knopp, DeltaOperator
from Genetics.base_multimodal import MultimodalBase

INFO = {
    "name": "Delta-mHC-Muon",
    "desc": "Deep Delta Learning + mHC. Refactored. (v23.1)",
    "vram_train": "7 GB",
    "vram_run": "3 GB"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 72000  # Standardized Safety Buffer
        self.embed_dim = 768
        self.context_len = 1024
        self.n_layers = 12
        self.n_heads = 12
        self.dropout = 0.1
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64
        self.cost_coefficient = 1e-4
        self.num_streams = 4


class DeltaMHCBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.n = config.num_streams
        self.dim = config.embed_dim
        self.stream_dim = self.dim // self.n

        # --- DELTA & MHC ---
        self.delta_op = DeltaOperator(self.dim)
        self.gate_proj = nn.Linear(self.dim, 3 * self.n)
        self.phi_res = nn.Parameter(torch.randn(self.n, self.n) * 0.02)

        self.ln_1 = RMSNorm(self.dim)
        self.ln_2 = RMSNorm(self.dim)

        # --- GAME THEORETIC MLP ---
        self.mlp = nn.Sequential(
            StrategicLinear(self.dim, 4 * self.dim, cost_coefficient=config.cost_coefficient),
            nn.GELU(),
            StrategicLinear(4 * self.dim, self.dim, cost_coefficient=config.cost_coefficient),
            nn.Dropout(config.dropout)
        )
        self.attn = nn.MultiheadAttention(self.dim, config.n_heads, batch_first=True)

    def forward(self, x, rope_cos, rope_sin):
        # 1. DEEP DELTA UPDATE
        x_delta = self.delta_op(x)

        # 2. mHC ROUTING
        x_state = x_delta.mean(dim=1)
        gates = self.gate_proj(x_state)
        g_res = torch.sigmoid(gates[:, 2 * self.n:]).unsqueeze(1).unsqueeze(-1)
        H_res = sinkhorn_knopp(self.phi_res, iterations=15)

        x_streams = x_delta.view(x.shape[0], x.shape[1], self.n, self.stream_dim)
        x_perm = (x_streams * g_res).permute(0, 1, 3, 2)
        res_act = (x_perm @ H_res.T).permute(0, 1, 3, 2)
        mhc_out = res_act.reshape(x.shape[0], x.shape[1], self.dim)

        # 3. ATTENTION
        normed = self.ln_1(x_delta)
        attn_out, _ = self.attn(normed, normed, normed, is_causal=True)

        # 4. MLP
        normed2 = self.ln_2(x_delta + attn_out)
        ffn_out = self.mlp(normed2)

        # 5. FUSION
        return x_delta + attn_out + ffn_out + mhc_out


class Model(MultimodalBase):
    def __init__(self, config=None):
        if config is None: config = NucleusConfig()
        super().__init__(config)

        # Hybrid Positional Embeddings (Standard AEIOU Pattern)
        self.pos_emb = nn.Embedding(config.context_len * 8, config.embed_dim)

        self.blocks = nn.ModuleList([DeltaMHCBlock(config) for _ in range(config.n_layers)])
        self.ln_f = RMSNorm(config.embed_dim)

        self.head = StrategicLinear(config.embed_dim, config.vocab_size,
                                    cost_coefficient=config.cost_coefficient,
                                    bias=False)

    def forward(self, v, a, t, c=None):
        device = t.device

        # Base Class Embed
        x = self.embed_inputs(v, a, t, c)

        # Positional Add
        curr_len = x.shape[1]
        if curr_len > self.pos_emb.num_embeddings:
            x = x[:, -self.pos_emb.num_embeddings:, :]
        positions = torch.arange(0, x.shape[1], device=device)
        x = x + self.pos_emb(positions)

        # RoPE Gen
        rope_cos, rope_sin = self.get_positional_embeddings(x)

        for block in self.blocks:
            x = block(x, rope_cos, rope_sin)

        x = self.ln_f(x)
        return self.head(x), None, None

--- FILE: Genetics\diffusion_mhc.py ---
import torch
import torch.nn as nn
import torch.nn.functional as F
from Genetics.common import RMSNorm, StrategicLinear, sinkhorn_knopp, DeltaOperator
import math
import random

INFO = {
    "name": "MaskedDiffusion-mHC",
    "desc": "Bidirectional Discrete Diffusion. Vocab-aware masking. Ideal for Dreaming/Refinement.",
    "vram_train": "9 GB",
    "vram_run": "5 GB"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 72000
        self.embed_dim = 768
        self.n_layers = 12
        self.n_heads = 12
        self.dropout = 0.1
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64
        self.cost_coefficient = 1e-4
        self.num_streams = 4

        # Diffusion Config
        self.inference_steps = 32
        self.mask_token_id = 71999

        # Vocab Boundaries (Must match Ribosome)
        self.vocab_img_start = 50257
        self.vocab_aud_start = 66641

        # Scheduling
        self.use_modality_masking = False
        self.use_cross_modal_masking = False
        self.cross_modal_prob = 0.25

        self.cross_modal_min_modalities = 1
        self.cross_modal_max_modalities = 2

        self.vis_mask_rate = 0.80
        self.aud_mask_rate = 0.60
        self.mot_mask_rate = 0.30
        self.text_mask_rate = 0.15


class DiffusionMHCBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dim = config.embed_dim
        self.n = config.num_streams
        self.stream_dim = self.dim // self.n

        self.time_proj = nn.Linear(config.embed_dim, config.embed_dim)

        self.delta_op = DeltaOperator(self.dim)
        self.gate_proj = nn.Linear(self.dim, 3 * self.n)
        self.phi_res = nn.Parameter(torch.randn(self.n, self.n) * 0.02)

        self.ln_1 = RMSNorm(self.dim)
        self.ln_2 = RMSNorm(self.dim)

        self.mlp = nn.Sequential(
            StrategicLinear(self.dim, 4 * self.dim, cost_coefficient=config.cost_coefficient),
            nn.GELU(),
            StrategicLinear(4 * self.dim, self.dim, cost_coefficient=config.cost_coefficient),
            nn.Dropout(config.dropout)
        )

        self.attn = nn.MultiheadAttention(self.dim, config.n_heads, batch_first=True)

    def forward(self, x, t_emb):
        if t_emb is not None:
            x = x + self.time_proj(t_emb)

        x_delta = self.delta_op(x)

        x_state = x_delta.mean(dim=1)
        gates = self.gate_proj(x_state)
        g_res = torch.sigmoid(gates[:, 2 * self.n:]).unsqueeze(1).unsqueeze(-1)
        H_res = sinkhorn_knopp(self.phi_res, iterations=15)

        x_streams = x_delta.view(x.shape[0], x.shape[1], self.n, self.stream_dim)
        x_perm = (x_streams * g_res).permute(0, 1, 3, 2)
        res_act = (x_perm @ H_res.T).permute(0, 1, 3, 2)
        mhc_out = res_act.reshape(x.shape[0], x.shape[1], self.dim)

        normed = self.ln_1(x_delta)
        attn_out, _ = self.attn(normed, normed, normed, is_causal=False)

        normed2 = self.ln_2(x_delta + attn_out)
        ffn_out = self.mlp(normed2)

        return x_delta + attn_out + ffn_out + mhc_out


class Model(nn.Module):
    def __init__(self, config=None):
        super().__init__()
        if config is None: config = NucleusConfig()
        self.config = config

        self.tok_emb = nn.Embedding(config.vocab_size + 1, config.embed_dim)
        self.vis_proj = nn.Linear(config.vis_dim, config.embed_dim)
        self.aud_proj = nn.Linear(config.aud_dim, config.embed_dim)
        self.mot_proj = nn.Linear(config.mot_dim, config.embed_dim)

        self.timestep_emb = nn.Embedding(1001, config.embed_dim)
        self.pos_emb = nn.Embedding(4096, config.embed_dim)

        self.blocks = nn.ModuleList([DiffusionMHCBlock(config) for _ in range(config.n_layers)])
        self.ln_f = RMSNorm(config.embed_dim)
        self.head = StrategicLinear(config.embed_dim, config.vocab_size,
                                    cost_coefficient=config.cost_coefficient, bias=False)

        self.mask_token_id = config.mask_token_id

    def _compute_modality_lengths_from_tokens(self, t):
        tokens = t.flatten()
        vis_mask = (tokens >= self.config.vocab_img_start) & (tokens < self.config.vocab_aud_start)
        vis_len = vis_mask.sum().item()
        aud_mask = (tokens >= self.config.vocab_aud_start)
        aud_len = aud_mask.sum().item()
        text_len = t.shape[1] - vis_len - aud_len
        mot_len = 0
        return vis_len, aud_len, mot_len, text_len

    def _get_section_starts(self, t, vis_len, aud_len, mot_len, text_len):
        starts = [0]
        starts.append(starts[-1] + vis_len)
        starts.append(starts[-1] + aud_len)
        starts.append(starts[-1] + mot_len)
        return starts

    def _apply_masking(self, seq, vis_len, aud_len, mot_len, text_len, mask_ratio=None):
        B, T = seq.shape
        device = seq.device
        mask = torch.zeros(T, device=device, dtype=torch.bool)
        masked_seq = seq.clone()

        section_starts = self._get_section_starts(seq, vis_len, aud_len, mot_len, text_len)

        if mask_ratio is not None:
            num_mask = int(T * mask_ratio)
            mask_idx = torch.randperm(T, device=device)[:num_mask]
            mask[mask_idx] = True

        elif self.config.use_cross_modal_masking and random.random() < self.config.cross_modal_prob:
            sections = [
                ('vis', section_starts[0], section_starts[1]),
                ('aud', section_starts[1], section_starts[2]),
                ('mot', section_starts[2], section_starts[3]),
                ('text', section_starts[3], T)
            ]
            present = [s for s in sections if s[2] - s[1] > 0]

            if len(present) >= 2:
                num_to_mask = random.randint(self.config.cross_modal_min_modalities,
                                             min(self.config.cross_modal_max_modalities, len(present) - 1))
                to_mask = random.sample(present, num_to_mask)
                for _, start, end in to_mask:
                    mask[start:end] = True
            else:
                num_mask = int(T * 0.4)
                mask_idx = torch.randperm(T, device=device)[:num_mask]
                mask[mask_idx] = True

        elif self.config.use_modality_masking:
            def mask_section(start, end, rate):
                length = end - start
                if length == 0: return
                num_mask = int(length * rate)
                section_idx = torch.randperm(length, device=device)[:num_mask]
                mask[start + section_idx] = True

            mask_section(section_starts[0], section_starts[1], self.config.vis_mask_rate)
            mask_section(section_starts[1], section_starts[2], self.config.aud_mask_rate)
            mask_section(section_starts[2], section_starts[3], self.config.mot_mask_rate)
            mask_section(section_starts[3], T, self.config.text_mask_rate)

        else:
            num_mask = int(T * 0.3)
            mask_idx = torch.randperm(T, device=device)[:num_mask]
            mask[mask_idx] = True

        # --- FIX: Ensure 2D mask for [B, T] ---
        # If mask is 1D [T], expand it to [B, T]
        if mask.ndim == 1:
            mask = mask.unsqueeze(0).expand(B, -1)

        masked_seq[mask] = self.mask_token_id
        return masked_seq, mask

    def forward(self, v, a, t, c=None, timestep=None, mask_ratio=None):
        device = t.device
        seq = t
        vis_len, aud_len, mot_len, text_len = self._compute_modality_lengths_from_tokens(seq)

        seq_masked, mask = self._apply_masking(seq, vis_len, aud_len, mot_len, text_len, mask_ratio)

        x = self.tok_emb(seq_masked)

        positions = torch.arange(0, x.shape[1], device=device)
        if x.shape[1] > self.pos_emb.num_embeddings:
            x = x[:, :self.pos_emb.num_embeddings]
            positions = positions[:self.pos_emb.num_embeddings]
        x = x + self.pos_emb(positions)

        if timestep is None: timestep = torch.zeros(x.shape[0], dtype=torch.long, device=device)
        t_emb = self.timestep_emb(timestep).unsqueeze(1)

        for block in self.blocks:
            x = block(x, t_emb)

        x = self.ln_f(x)
        logits = self.head(x)

        return logits, mask, None

    @torch.no_grad()
    def generate(self, prompt_tokens=None, max_length=1024, steps=None, temperature=1.0, top_k=50,
                 force_sample_remaining=True):
        device = next(self.parameters()).device
        steps = steps or self.config.inference_steps

        # --- SHAPE SAFETY INITIALIZATION ---
        if prompt_tokens is not None:
            if isinstance(prompt_tokens, list):
                seq = torch.tensor(prompt_tokens, device=device).unsqueeze(0)
            elif isinstance(prompt_tokens, torch.Tensor):
                if prompt_tokens.ndim == 1:
                    seq = prompt_tokens.unsqueeze(0)
                else:
                    seq = prompt_tokens

            # Setup Padding and Mask
            known_mask = torch.zeros_like(seq, dtype=torch.bool)
            pad_len = max_length - seq.shape[1]
            if pad_len > 0:
                pad = torch.full((1, pad_len), self.mask_token_id, device=device)
                seq = torch.cat([seq, pad], dim=1)
                # Pad known_mask with Falses (unknowns)
                pad_mask = torch.zeros((1, pad_len), dtype=torch.bool, device=device)
                known_mask = torch.cat([known_mask, pad_mask], dim=1)
        else:
            seq = torch.full((1, max_length), self.mask_token_id, dtype=torch.long, device=device)
            known_mask = torch.zeros((1, max_length), dtype=torch.bool, device=device)

        # --- ITERATIVE REFINEMENT ---
        for step in range(steps):
            ratio = math.cos((step / steps) * math.pi / 2)
            ts_val = int(ratio * 1000)
            timestep = torch.tensor([ts_val], device=device)

            # Manual Forward Pass (Bypass masking logic)
            # 1. Embed
            x = self.tok_emb(seq)
            positions = torch.arange(0, x.shape[1], device=device)
            x = x + self.pos_emb(positions)
            t_emb = self.timestep_emb(timestep).unsqueeze(1)

            # 2. Denoise
            for block in self.blocks:
                x = block(x, t_emb)
            x = self.ln_f(x)
            logits = self.head(x)

            # 3. Sampling
            logits = logits / temperature
            if top_k > 0:
                v_top, _ = torch.topk(logits, top_k)
                logits[logits < v_top[..., [-1]]] = -float('inf')

            probs = F.softmax(logits, dim=-1)
            confidences, predicted = torch.max(probs, dim=-1)

            # Ignore what we already know (set confidence to -inf so we don't pick it)
            confidences = confidences.masked_fill(known_mask, -float('inf'))

            # Calculate how many to unmask
            num_remaining = (~known_mask).sum().item()
            num_unmask = max(1, int(num_remaining * (1 - ratio)))

            # Greedy Unmasking
            _, indices = torch.topk(confidences.flatten(), num_unmask)

            # Scatter updates
            # Create a flat mask for update
            flat_mask = torch.zeros_like(confidences.flatten(), dtype=torch.bool)
            flat_mask[indices] = True
            update_mask = flat_mask.view_as(known_mask)

            # Update sequence and known_mask
            seq[update_mask] = predicted[update_mask]
            known_mask = known_mask | update_mask

        # --- FORCE SAMPLE REMAINING (Anti-Blank Fix) ---
        if force_sample_remaining and not known_mask.all():
            remaining = ~known_mask
            # Final clean pass
            timestep = torch.zeros(1, device=device, dtype=torch.long)

            x = self.tok_emb(seq)
            positions = torch.arange(0, x.shape[1], device=device)
            x = x + self.pos_emb(positions)
            t_emb = self.timestep_emb(timestep).unsqueeze(1)
            for block in self.blocks:
                x = block(x, t_emb)
            x = self.ln_f(x)
            final_logits = self.head(x)

            # Sample
            final_logits = final_logits / temperature
            probs = F.softmax(final_logits[remaining], dim=-1)
            if top_k > 0:
                v, _ = torch.topk(probs, top_k)
                probs[probs < v[..., [-1]]] = 0
                probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-6)

            sampled = torch.multinomial(probs, 1).squeeze(1)
            seq[remaining] = sampled

        return seq[0].cpu().tolist()

--- FILE: Genetics\game_gpt2.py ---
import torch
import torch.nn as nn
import torch.nn.functional as F
from Genetics.common import RMSNorm

# --- GUI METADATA ---
INFO = {
    "name": "Equilibrium-GPT2",
    "desc": "Game-Theoretic Pruning. Weights play a non-cooperative game to justify their existence.",
    "vram_train": "6 GB",
    "vram_run": "2 GB"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 50257
        self.embed_dim = 768
        self.context_len = 1024
        self.n_layers = 12
        self.n_heads = 12
        self.dropout = 0.1
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64
        self.cost_coefficient = 1e-4  # The cost of playing the game


class StrategicLinear(nn.Linear):
    """
    A Linear layer where weights must 'pay' to participate.
    Based on: 'Pruning as a Game: Equilibrium-Driven Sparsification' (2025)
    """

    def __init__(self, in_features, out_features, cost_coefficient=1e-4, **kwargs):
        super().__init__(in_features, out_features, **kwargs)

        # The "Participation Variable" (alpha).
        # Initialized to >1.0 (fully participating) using inverse sigmoid logic.
        # logits of 5.0 -> sigmoid(5.0) ~= 0.993
        self.alpha_logits = nn.Parameter(torch.randn(out_features) * 0.1 + 5.0)
        self.cost_coefficient = cost_coefficient

    def get_participation(self):
        # Sigmoid ensures alpha is strictly [0, 1]
        return torch.sigmoid(self.alpha_logits)

    def forward(self, input):
        alpha = self.get_participation()

        # Strategic Interaction: Mask the weights column-wise
        # alpha shape: [out_features] -> unsqueeze -> [out_features, 1] for broadcasting
        masked_weight = self.weight * alpha.unsqueeze(1)

        return F.linear(input, masked_weight, self.bias)

    def game_loss(self):
        # The Payoff: Minimize sum of alphas (Sparsity) scaled by cost
        alpha = self.get_participation()
        return self.cost_coefficient * torch.sum(alpha)


class StrategicBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.embed_dim)
        self.attn = nn.MultiheadAttention(config.embed_dim, config.n_heads, batch_first=True)

        # We assume standard Attention doesn't need pruning logic for now,
        # or we could replace the internal QKV projections if we manually implemented Attention.
        # For simplicity, we apply the Game only to the MLP layers below.

        self.ln_2 = nn.LayerNorm(config.embed_dim)

        # Replace standard Linears with StrategicLinears
        self.mlp = nn.Sequential(
            StrategicLinear(config.embed_dim, 4 * config.embed_dim, cost_coefficient=config.cost_coefficient),
            nn.GELU(),
            StrategicLinear(4 * config.embed_dim, config.embed_dim, cost_coefficient=config.cost_coefficient),
            nn.Dropout(config.dropout),
        )

    def forward(self, x):
        attn_mask = torch.triu(torch.full((x.shape[1], x.shape[1]), float('-inf'), device=x.device), diagonal=1)

        # Standard Attention (Cooperative Phase)
        attn_out, _ = self.attn(self.ln_1(x), self.ln_1(x), self.ln_1(x),
                                attn_mask=attn_mask, is_causal=False)
        x = x + attn_out

        # Strategic MLP (Competitive Phase)
        x = x + self.mlp(self.ln_2(x))
        return x


class Model(nn.Module):
    def __init__(self, config=None):
        super().__init__()
        if config is None: config = NucleusConfig()
        self.config = config

        self.tok_emb = nn.Embedding(config.vocab_size, config.embed_dim)
        self.pos_emb = nn.Embedding(config.context_len * 8, config.embed_dim)

        # Adapters
        self.vis_proj = nn.Linear(config.vis_dim, config.embed_dim)
        self.aud_proj = nn.Linear(config.aud_dim, config.embed_dim)
        self.mot_proj = nn.Linear(config.mot_dim, config.embed_dim)

        # Strategic Blocks
        self.blocks = nn.ModuleList([StrategicBlock(config) for _ in range(config.n_layers)])

        self.ln_f = nn.LayerNorm(config.embed_dim)
        # The Final Head is also a player in the game
        self.head = StrategicLinear(config.embed_dim, config.vocab_size, cost_coefficient=config.cost_coefficient,
                                    bias=False)

    def forward(self, v, a, t, c=None):
        B, T = t.shape
        device = t.device

        # Strict Order: V -> A -> C -> T
        parts = []
        if v is not None and v.numel() > 0: parts.append(self.vis_proj(v))
        if a is not None and a.numel() > 0: parts.append(self.aud_proj(a))
        if c is not None and c.numel() > 0: parts.append(self.mot_proj(c))
        parts.append(self.tok_emb(t))

        x = torch.cat(parts, dim=1)

        curr_len = x.shape[1]
        positions = torch.arange(0, curr_len, device=device)
        if curr_len > self.pos_emb.num_embeddings:
            x = x[:, -self.pos_emb.num_embeddings:, :]
            positions = positions[:self.pos_emb.num_embeddings]

        x = x + self.pos_emb(positions)

        for block in self.blocks: x = block(x)
        x = self.ln_f(x)
        return self.head(x), None, None

--- FILE: Genetics\gpt2-g95ld5hp.py ---
import torch
import torch.nn as nn
from Genetics.common import RMSNorm

# --- GUI METADATA ---
INFO = {
    "name": "Tetra-GPT2",
    "desc": "Classic Transformer. Standardized Fusion Order.",
    "vram_train": "6 GB",
    "vram_run": "2 GB"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 50257
        self.embed_dim = 768
        self.context_len = 1024
        self.n_layers = 12
        self.n_heads = 12
        self.dropout = 0.1
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64


class GPT2Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.embed_dim)
        self.attn = nn.MultiheadAttention(config.embed_dim, config.n_heads, batch_first=True)
        self.ln_2 = nn.LayerNorm(config.embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(config.embed_dim, 4 * config.embed_dim),
            nn.GELU(),
            nn.Linear(4 * config.embed_dim, config.embed_dim),
            nn.Dropout(config.dropout),
        )

    def forward(self, x):
        B, T, C = x.shape
        attn_mask = torch.triu(torch.full((T, T), float('-inf'), device=x.device), diagonal=1)
        attn_out, _ = self.attn(self.ln_1(x), self.ln_1(x), self.ln_1(x),
                                attn_mask=attn_mask,
                                is_causal=False)
        x = x + attn_out
        x = x + self.mlp(self.ln_2(x))
        return x


class Model(nn.Module):
    def __init__(self, config=None):
        super().__init__()
        if config is None: config = NucleusConfig()
        self.config = config
        self.tok_emb = nn.Embedding(config.vocab_size, config.embed_dim)
        self.pos_emb = nn.Embedding(config.context_len * 8, config.embed_dim)
        self.vis_proj = nn.Linear(config.vis_dim, config.embed_dim)
        self.aud_proj = nn.Linear(config.aud_dim, config.embed_dim)
        self.mot_proj = nn.Linear(config.mot_dim, config.embed_dim)
        self.blocks = nn.ModuleList([GPT2Block(config) for _ in range(config.n_layers)])
        self.ln_f = nn.LayerNorm(config.embed_dim)
        self.head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)

    def forward(self, v, a, t, c=None):
        B, T = t.shape
        device = t.device

        # Strict Order: V -> A -> C -> T
        parts = []
        if v is not None and v.numel() > 0: parts.append(self.vis_proj(v))
        if a is not None and a.numel() > 0: parts.append(self.aud_proj(a))
        if c is not None and c.numel() > 0: parts.append(self.mot_proj(c))
        parts.append(self.tok_emb(t))

        x = torch.cat(parts, dim=1)

        # GPT2 Absolute Positional Embeddings
        # We must extend positions to cover the new total length
        curr_len = x.shape[1]
        positions = torch.arange(0, curr_len, device=device)
        if curr_len > self.pos_emb.num_embeddings:
            # If too long, slice the input (this is a rough truncate, but prevents crash)
            x = x[:, -self.pos_emb.num_embeddings:, :]
            positions = positions[:self.pos_emb.num_embeddings]

        x = x + self.pos_emb(positions)

        for block in self.blocks: x = block(x)
        x = self.ln_f(x)
        return self.head(x), None, None

--- FILE: Genetics\llama-g8ds04g.py ---
import torch
import torch.nn as nn
import torch.nn.functional as F
from Genetics.common import RMSNorm, SwiGLU, RotaryEmbedding, apply_rope

# --- GUI METADATA ---
INFO = {
    "name": "Tetra-Llama",
    "desc": "Multimodal Llama (RoPE + SwiGLU). Standardized Fusion Order.",
    "vram_train": "8 GB",
    "vram_run": "4 GB"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 50257
        self.embed_dim = 768
        self.context_len = 1024
        self.n_layers = 12
        self.n_heads = 12
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64


class LlamaBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.n_heads = config.n_heads
        self.head_dim = config.embed_dim // config.n_heads
        self.attn_norm = RMSNorm(config.embed_dim)
        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.o_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.ffn_norm = RMSNorm(config.embed_dim)
        self.ffn = SwiGLU(config.embed_dim, int(config.embed_dim * 2.6))

    def forward(self, x, rope_cos, rope_sin):
        B, T, C = x.shape
        h = self.attn_norm(x)
        q = self.q_proj(h).view(B, T, self.n_heads, self.head_dim)
        k = self.k_proj(h).view(B, T, self.n_heads, self.head_dim)
        v = self.v_proj(h).view(B, T, self.n_heads, self.head_dim)

        cos = rope_cos[:T, :]
        sin = rope_sin[:T, :]

        q = apply_rope(q, cos, sin).transpose(1, 2)
        k = apply_rope(k, cos, sin).transpose(1, 2)
        v = v.transpose(1, 2)

        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        x = x + self.o_proj(y)
        x = x + self.ffn(self.ffn_norm(x))
        return x


class Model(nn.Module):
    def __init__(self, config=None):
        super().__init__()
        if config is None: config = NucleusConfig()
        self.config = config
        self.tok_emb = nn.Embedding(config.vocab_size, config.embed_dim)
        self.vis_proj = nn.Linear(config.vis_dim, config.embed_dim)
        self.aud_proj = nn.Linear(config.aud_dim, config.embed_dim)
        self.mot_proj = nn.Linear(config.mot_dim, config.embed_dim)
        self.layers = nn.ModuleList([LlamaBlock(config) for _ in range(config.n_layers)])
        self.norm = RMSNorm(config.embed_dim)
        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)

        self.rope = RotaryEmbedding(config.embed_dim // config.n_heads, max_seq_len=4096)

    def forward(self, v, a, t, c=None):
        B, T = t.shape
        device = t.device
        x = self.tok_emb(t)

        # STANDARDIZED FUSION ORDER: Visual -> Audio -> Motion -> Text
        if v is not None and v.numel() > 0:
            v_emb = self.vis_proj(v)
            x = torch.cat([v_emb, x], dim=1)

        if a is not None and a.numel() > 0:
            a_emb = self.aud_proj(a)
            x = torch.cat([x, a_emb], dim=1)

        if c is not None and c.numel() > 0:
            c_emb = self.mot_proj(c)
            x = torch.cat([x, c_emb], dim=1)

        seq_len = x.shape[1]
        rope_cos, rope_sin = self.rope(seq_len)
        rope_cos, rope_sin = rope_cos.to(device), rope_sin.to(device)

        for layer in self.layers:
            x = layer(x, rope_cos, rope_sin)

        x = self.norm(x)
        return self.lm_head(x), None, None

--- FILE: Genetics\llama_8k-aD983ND.py ---
import torch
import torch.nn as nn
import torch.nn.functional as F
from Genetics.common import RMSNorm, SwiGLU, apply_rope
from Genetics.base_multimodal import MultimodalBase

INFO = {
    "name": "Llama-8k",
    "desc": "High Context Variant (8192). Refactored Base. (v23.1)",
    "vram_train": "10 GB",
    "vram_run": "6 GB"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 72000
        self.embed_dim = 768
        self.context_len = 1024  # Internal block context
        self.n_layers = 12
        self.n_heads = 12
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64


class LlamaBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.n_heads = config.n_heads
        self.head_dim = config.embed_dim // config.n_heads

        self.attn_norm = RMSNorm(config.embed_dim)
        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.o_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)

        self.ffn_norm = RMSNorm(config.embed_dim)
        self.ffn = SwiGLU(config.embed_dim, int(config.embed_dim * 2.6))

    def forward(self, x, rope_cos, rope_sin):
        B, T, C = x.shape

        # 1. Attention
        h = self.attn_norm(x)
        q = self.q_proj(h).view(B, T, self.n_heads, self.head_dim)
        k = self.k_proj(h).view(B, T, self.n_heads, self.head_dim)
        v = self.v_proj(h).view(B, T, self.n_heads, self.head_dim)

        # Apply RoPE (Passed from Base Class)
        # Slicing for safety if rope cache is larger than current T
        cos = rope_cos[:T, :]
        sin = rope_sin[:T, :]

        q = apply_rope(q, cos, sin).transpose(1, 2)
        k = apply_rope(k, cos, sin).transpose(1, 2)
        v = v.transpose(1, 2)

        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        x = x + self.o_proj(y)

        # 2. Feed Forward
        x = x + self.ffn(self.ffn_norm(x))
        return x


class Model(MultimodalBase):
    def __init__(self, config=None):
        if config is None: config = NucleusConfig()
        super().__init__(config)

        self.layers = nn.ModuleList([LlamaBlock(config) for _ in range(config.n_layers)])
        self.norm = RMSNorm(config.embed_dim)
        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)

    def forward(self, v, a, t, c=None):
        # 1. Embed (Unified)
        x = self.embed_inputs(v, a, t, c)

        # 2. RoPE (Unified)
        rope_cos, rope_sin = self.get_positional_embeddings(x)

        # 3. Llama Blocks
        for layer in self.layers:
            x = layer(x, rope_cos, rope_sin)

        x = self.norm(x)
        return self.lm_head(x), None, None

--- FILE: Genetics\mhc_gpt2.py ---
import torch
import torch.nn as nn
from Genetics.common import RMSNorm, StrategicLinear, sinkhorn_knopp
from Genetics.base_multimodal import MultimodalBase

INFO = {
    "name": "mHC-Muon-GPT2",
    "desc": "Manifold-Constrained Hyper-Connections. Refactored. (v23.1)",
    "vram_train": "7 GB",
    "vram_run": "3 GB"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 72000
        self.embed_dim = 768
        self.context_len = 1024
        self.n_layers = 12
        self.n_heads = 12
        self.dropout = 0.1
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64
        self.cost_coefficient = 1e-4
        self.num_streams = 4


class mHCBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.n = config.num_streams
        self.dim = config.embed_dim
        self.stream_dim = self.dim // self.n

        # --- GATING ---
        self.gate_proj = nn.Linear(self.dim, 3 * self.n)
        self.phi_res = nn.Parameter(torch.randn(self.n, self.n) * 0.02)

        self.ln_1 = RMSNorm(self.dim)
        self.ln_2 = RMSNorm(self.dim)

        self.mlp = nn.Sequential(
            StrategicLinear(self.dim, 4 * self.dim, cost_coefficient=config.cost_coefficient),
            nn.GELU(),
            StrategicLinear(4 * self.dim, self.dim, cost_coefficient=config.cost_coefficient),
            nn.Dropout(config.dropout)
        )
        self.attn = nn.MultiheadAttention(self.dim, config.n_heads, batch_first=True)

    def forward(self, x, rope_cos, rope_sin):
        B, T, D = x.shape

        # 1. mHC ROUTING
        x_state = x.mean(dim=1)
        gates = self.gate_proj(x_state)
        g_res = torch.sigmoid(gates[:, 2 * self.n:]).unsqueeze(1).unsqueeze(-1)
        H_res = sinkhorn_knopp(self.phi_res, iterations=15)

        x_streams = x.view(B, T, self.n, self.stream_dim)
        x_perm = (x_streams * g_res).permute(0, 1, 3, 2)
        res_act = (x_perm @ H_res.T).permute(0, 1, 3, 2)
        mhc_out = res_act.reshape(B, T, D)

        # 2. ATTENTION
        normed = self.ln_1(x)
        attn_out, _ = self.attn(normed, normed, normed, is_causal=True)
        x2 = x + attn_out

        # 3. MLP
        normed2 = self.ln_2(x2)
        ffn_out = self.mlp(normed2)

        # 4. FUSION
        return x2 + ffn_out + mhc_out


class Model(MultimodalBase):
    def __init__(self, config=None):
        if config is None: config = NucleusConfig()
        super().__init__(config)

        self.pos_emb = nn.Embedding(config.context_len * 8, config.embed_dim)
        self.blocks = nn.ModuleList([mHCBlock(config) for _ in range(config.n_layers)])
        self.ln_f = RMSNorm(config.embed_dim)

        self.head = StrategicLinear(config.embed_dim, config.vocab_size,
                                    cost_coefficient=config.cost_coefficient,
                                    bias=False)

    def forward(self, v, a, t, c=None):
        device = t.device
        x = self.embed_inputs(v, a, t, c)

        curr_len = x.shape[1]
        if curr_len > self.pos_emb.num_embeddings:
            x = x[:, -self.pos_emb.num_embeddings:, :]
        positions = torch.arange(0, x.shape[1], device=device)
        x = x + self.pos_emb(positions)

        rope_cos, rope_sin = self.get_positional_embeddings(x)

        for block in self.blocks:
            x = block(x, rope_cos, rope_sin)

        x = self.ln_f(x)
        return self.head(x), None, None

--- FILE: Genetics\muon.py ---
import torch
import torch.optim as optim


def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):
    """
    Newton-Schulz iteration with dimensional folding and NaN safety.
    """
    if G is None or torch.isnan(G).any():
        return torch.zeros_like(G)  # Fail gracefully

    assert G.ndim == 2

    # Folding for tall matrices
    if G.size(0) > G.size(1):
        return zeroth_power_via_newtonschulz5(G.T, steps, eps).T

    # Cast to float32 for stability
    orig_dtype = G.dtype
    X = G.float()

    # Normalization
    trace_est = torch.sum(X ** 2)
    if trace_est <= 0 or torch.isnan(trace_est):
        return torch.zeros_like(G)  # Catch empty/nan trace

    X = X / (torch.sqrt(trace_est) + eps)

    # Iteration
    for _ in range(steps):
        A = X @ X.T
        B = 3.0 * torch.eye(A.shape[0], device=X.device, dtype=X.dtype) - A
        X = 0.5 * B @ X

        # Mid-loop safety check
        if torch.isnan(X).any():
            return torch.zeros_like(G)

    return X.to(orig_dtype)


class Muon(optim.Optimizer):
    def __init__(self, params, lr=0.001, momentum=0.95, nesterov=True, ns_steps=5, adamw_lr=0.0001, weight_decay=0.01):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps, adamw_lr=adamw_lr,
                        weight_decay=weight_decay)

        muon_params = []
        adam_params = []

        for p in params:
            if p.ndim == 2 and p.size(0) > 32 and p.size(1) > 32:
                muon_params.append(p)
            else:
                adam_params.append(p)

        self.adamw = optim.AdamW(adam_params, lr=adamw_lr, weight_decay=weight_decay, betas=(0.95, 0.95))

        super().__init__(muon_params, defaults)

    def step(self):
        self.adamw.step()

        for group in self.param_groups:
            lr = group['lr']
            mu = group['momentum']
            wd = group['weight_decay']

            for p in group['params']:
                if p.grad is None: continue

                # Input NaN Guard
                if torch.isnan(p.grad).any():
                    continue

                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(p.grad)

                buf = state['momentum_buffer']
                g = p.grad

                buf.mul_(mu).add_(g)

                if group['nesterov']:
                    update_m = g.add(buf, alpha=mu)
                else:
                    update_m = buf

                if p.ndim == 2:
                    O = zeroth_power_via_newtonschulz5(update_m, steps=group['ns_steps'])

                    # If O came back all zeros (failed), skip update
                    if (O == 0).all():
                        continue

                    scale_factor = max(p.size(0), p.size(1)) ** 0.5
                    scaled_update = O * scale_factor * 0.2

                    if wd > 0:
                        scaled_update.add_(p.data, alpha=wd)

                    p.data.add_(scaled_update, alpha=-lr)

--- FILE: Genetics\pretrained_adapter.py ---
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import tkinter as tk
from tkinter import ttk

# --- GUI METADATA ---
INFO = {
    "name": "HuggingFace Adapter",
    "desc": "Universal Loader. Select model variant upon initialization.",
    "vram_train": "Varies",
    "vram_run": "Varies"
}


class NucleusConfig:
    def __init__(self):
        self.vocab_size = 50257
        self.embed_dim = 4096
        self.context_len = 8192


class Model(nn.Module):
    def __init__(self, config=None):
        super().__init__()

        # --- INTERACTIVE MODEL SELECTOR ---
        self.model_id = self._ask_user_for_model()
        if not self.model_id:
            raise ValueError("No model selected.")

        print(f"[Adapter] Loading {self.model_id} in 4-bit...")

        # 4-Bit Quantization Config
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
        )

        # Load Real Model
        # device_map="auto" AUTOMATICALLY puts this on the GPU
        self.hf_model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, trust_remote_code=True)
        self.tok_emb = nn.Identity()

    def _ask_user_for_model(self):
        """ Pops up a dialog to select the HF model """
        selector = tk.Toplevel()
        selector.title("Select Pretrained Model")
        selector.geometry("400x350")
        selector.transient()
        selector.grab_set()

        selected_model = tk.StringVar(value="Qwen/Qwen2.5-7B-Instruct")

        ttk.Label(selector, text="Choose a Hugging Face Model:", font=("Segoe UI", 10, "bold")).pack(pady=10)

        options = [
            ("Qwen 2.5 (14B) - Best for 3080 Ti", "Qwen/Qwen2.5-14B-Instruct"),
            ("Qwen 2.5 (7B) - Fast & Light", "Qwen/Qwen2.5-7B-Instruct"),
            ("Qwen 2.5 (32B) - Requires 24GB+ VRAM", "Qwen/Qwen2.5-32B-Instruct"),
            ("Llama 3.1 (8B) - Solid Generalist", "meta-llama/Meta-Llama-3.1-8B-Instruct"),
            ("Mistral Nemo (12B) - High Context", "mistralai/Mistral-Nemo-Instruct-2407"),
            ("Phi-3.5 Mini (3.8B) - Very Fast", "microsoft/Phi-3.5-mini-instruct")
        ]

        for label, mid in options:
            ttk.Radiobutton(selector, text=label, variable=selected_model, value=mid).pack(anchor="w", padx=20, pady=2)

        ttk.Separator(selector, orient="horizontal").pack(fill="x", padx=10, pady=10)
        ttk.Label(selector, text="Or type custom HF ID:").pack()
        ttk.Entry(selector, textvariable=selected_model).pack(fill="x", padx=20)

        def confirm():
            selector.destroy()

        ttk.Button(selector, text="LOAD MODEL", command=confirm).pack(pady=15)

        selector.wait_window()
        return selected_model.get()

    def forward(self, v, a, t, c=None):
        outputs = self.hf_model(input_ids=t, output_hidden_states=True)
        return outputs.logits, None, None

    def generate(self, prompt, max_new_tokens=50):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.hf_model.device)
        outputs = self.hf_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    # --- CRITICAL FIX FOR 4-BIT MODELS ---
    def to(self, device):
        """
        Override .to() to prevent bitsandbytes error.
        The HF model is already on GPU via device_map='auto', so we ignore this call.
        """
        return self

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        return {"model_id": self.model_id}

    def load_state_dict(self, state_dict, strict=True):
        pass

--- FILE: Genetics\Qwen-Omni-MoE-D94ht8s.py ---
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# --- GUI METADATA ---
INFO = {
    "name": "Qwen-Omni-MoE (8k)",
    "desc": "Native Multimodal MoE. Uses Top-2 Routing & M-RoPE logic. 8192 ctx.",
    "vram_train": "12 GB",
    "vram_run": "8 GB"
}


# --- SHARED COMPONENTS (Local copy for self-containment) ---
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        var = torch.mean(x ** 2, dim=-1, keepdim=True)
        return self.weight * x * torch.rsqrt(var + self.eps)


class RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_seq_len=16384):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        t = torch.arange(max_seq_len).float()
        freqs = torch.outer(t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos", emb.cos())
        self.register_buffer("sin", emb.sin())

    def forward(self, seq_len):
        return self.cos[:seq_len, :], self.sin[:seq_len, :]


def apply_rope(x, cos, sin):
    cos = cos.unsqueeze(0).unsqueeze(2)
    sin = sin.unsqueeze(0).unsqueeze(2)
    head_dim = x.shape[-1]
    x1 = x[..., :head_dim // 2]
    x2 = x[..., head_dim // 2:]
    rotated = torch.cat([-x2, x1], dim=-1)
    return (x * cos) + (rotated * sin)


# --- ARCHITECTURE CONFIG ---
class NucleusConfig:
    def __init__(self):
        self.vocab_size = 50257
        self.embed_dim = 768
        self.context_len = 1024  # Internal attention window
        self.n_layers = 12
        self.n_heads = 12

        # MoE Settings (DeepSeek-VL2 Style)
        self.num_experts = 8
        self.active_experts = 2

        # Modality Dimensions
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64


# --- MOE LAYER (The Expert System) ---
class SwiGLUExpert(nn.Module):
    def __init__(self, dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, dim, bias=False)
        self.w3 = nn.Linear(dim, hidden_dim, bias=False)

    def forward(self, x):
        return self.w2(F.silu(self.w1(x)) * self.w3(x))


class SparseMoE(nn.Module):
    def __init__(self, dim, num_experts=8, active_experts=2):
        super().__init__()
        self.num_experts = num_experts
        self.active_experts = active_experts
        self.router = nn.Linear(dim, num_experts, bias=False)

        # Create 8 small experts instead of 1 giant MLP
        # Scale hidden dim down slightly per expert to keep param count sane
        expert_dim = int(dim * 1.5)
        self.experts = nn.ModuleList([SwiGLUExpert(dim, expert_dim) for _ in range(num_experts)])

    def forward(self, x):
        # x: [Batch, Seq, Dim]
        B, T, C = x.shape
        x_flat = x.view(-1, C)

        # Router logits: [Batch*Seq, NumExperts]
        gate_logits = self.router(x_flat)

        # Select Top-K Experts
        weights, indices = torch.topk(gate_logits, self.active_experts, dim=-1)
        weights = F.softmax(weights, dim=-1)

        output = torch.zeros_like(x_flat)

        # Iterate over active experts and compute
        # Note: This is a naive sequential loop implementation (easier to read/debug than optimized kernels)
        for i in range(self.active_experts):
            expert_idx = indices[:, i]
            expert_weight = weights[:, i, None]

            for e_idx in range(self.num_experts):
                # Find tokens assigned to this expert at this rank
                mask = (expert_idx == e_idx)
                if mask.any():
                    tokens = x_flat[mask]
                    processed = self.experts[e_idx](tokens)
                    output[mask] += processed * expert_weight[mask]

        return output.view(B, T, C)


# --- MAIN BLOCK ---
class OmniBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.n_heads = config.n_heads
        self.head_dim = config.embed_dim // config.n_heads
        self.attn_norm = RMSNorm(config.embed_dim)

        # Attention
        self.q_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.k_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.v_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)
        self.o_proj = nn.Linear(config.embed_dim, config.embed_dim, bias=False)

        # MoE Feed Forward
        self.ffn_norm = RMSNorm(config.embed_dim)
        self.moe = SparseMoE(config.embed_dim, config.num_experts, config.active_experts)

    def forward(self, x, rope_cos, rope_sin):
        B, T, C = x.shape
        h = self.attn_norm(x)

        q = self.q_proj(h).view(B, T, self.n_heads, self.head_dim)
        k = self.k_proj(h).view(B, T, self.n_heads, self.head_dim)
        v = self.v_proj(h).view(B, T, self.n_heads, self.head_dim)

        # Apply RoPE
        # Safety slicing for variable sequence lengths
        curr_cos = rope_cos[:T, :]
        curr_sin = rope_sin[:T, :]

        q = apply_rope(q, curr_cos, curr_sin).transpose(1, 2)
        k = apply_rope(k, curr_cos, curr_sin).transpose(1, 2)
        v = v.transpose(1, 2)

        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
        y = y.transpose(1, 2).contiguous().view(B, T, C)

        x = x + self.o_proj(y)

        # Apply MoE
        x = x + self.moe(self.ffn_norm(x))
        return x


# --- THE MODEL ---
class Model(nn.Module):
    def __init__(self, config=None):
        super().__init__()
        if config is None: config = NucleusConfig()
        self.config = config

        # Core Embeddings
        self.tok_emb = nn.Embedding(config.vocab_size, config.embed_dim)

        # Multimodal Adapters (Qwen2-VL style "Naive" projection)
        self.vis_proj = nn.Linear(config.vis_dim, config.embed_dim)
        self.aud_proj = nn.Linear(config.aud_dim, config.embed_dim)
        self.mot_proj = nn.Linear(config.mot_dim, config.embed_dim)

        # M-RoPE Simulators: Modality Type Embeddings
        # 0=Text, 1=Vis, 2=Aud, 3=Mot
        self.type_emb = nn.Embedding(4, config.embed_dim)

        self.layers = nn.ModuleList([OmniBlock(config) for _ in range(config.n_layers)])
        self.norm = RMSNorm(config.embed_dim)

        # Dual Heads (NExT-OMNI style)
        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)
        self.velocity_head = nn.Linear(config.embed_dim, config.embed_dim, bias=False)  # For future Flow Matching

        # Match legacy saves (8192)
        self.rope = RotaryEmbedding(config.embed_dim // config.n_heads, max_seq_len=8192)

    def forward(self, v, a, t, c=None):
        B, T = t.shape
        device = t.device

        # 1. Embed Text
        x = self.tok_emb(t) + self.type_emb(torch.tensor(0, device=device))

        # 2. Early Fusion with Type Embeddings
        if a is not None and a.numel() > 0:
            a_emb = self.aud_proj(a) + self.type_emb(torch.tensor(2, device=device))
            x = torch.cat([a_emb, x], dim=1)

        if v is not None and v.numel() > 0:
            v_emb = self.vis_proj(v) + self.type_emb(torch.tensor(1, device=device))
            x = torch.cat([x, v_emb], dim=1)

        if c is not None and c.numel() > 0:
            c_emb = self.mot_proj(c) + self.type_emb(torch.tensor(3, device=device))
            x = torch.cat([x, c_emb], dim=1)

        # 3. RoPE Generation
        seq_len = x.shape[1]
        rope_cos, rope_sin = self.rope(seq_len)
        rope_cos, rope_sin = rope_cos.to(device), rope_sin.to(device)

        # 4. Deep Transformer Flow
        for layer in self.layers:
            x = layer(x, rope_cos, rope_sin)

        x = self.norm(x)

        # 5. Multi-Head Output
        logits = self.lm_head(x)

        # We can optionally return velocity fields here if we were doing Flow Matching training
        # For now, we stick to AR logits
        return logits, None, None

--- FILE: Genetics\unified_mhc_gpt2.py ---
import torch
import torch.nn as nn
from Genetics.common import RMSNorm, StrategicLinear, sinkhorn_knopp, DeltaOperator
from Genetics.base_multimodal import MultimodalBase

INFO = {
    "name": "Unified-mHC-Muon",
    "desc": "Refactored Base + Manual Masking Fix. (v23.2)",
    "vram_train": "8 GB",
    "vram_run": "4 GB"
}


class NucleusConfig:
    def __init__(self):
        # VOCAB SAFETY BUFFER
        self.vocab_size = 72000
        self.embed_dim = 768
        self.context_len = 2048
        self.n_layers = 12
        self.n_heads = 12
        self.dropout = 0.1
        self.vis_dim = 768
        self.aud_dim = 128
        self.mot_dim = 64
        self.cost_coefficient = 1e-4
        self.num_streams = 4


class UnifiedBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.n = config.num_streams
        self.dim = config.embed_dim
        self.stream_dim = self.dim // self.n

        # --- DELTA & MHC COMPONENTS ---
        self.delta_op = DeltaOperator(self.dim)
        self.gate_proj = nn.Linear(self.dim, 3 * self.n)
        self.phi_res = nn.Parameter(torch.randn(self.n, self.n) * 0.02)

        self.ln_1 = RMSNorm(self.dim)
        self.ln_2 = RMSNorm(self.dim)

        # --- GAME THEORETIC MLP ---
        self.mlp = nn.Sequential(
            StrategicLinear(self.dim, 4 * self.dim, cost_coefficient=config.cost_coefficient),
            nn.GELU(),
            StrategicLinear(4 * self.dim, self.dim, cost_coefficient=config.cost_coefficient),
            nn.Dropout(config.dropout)
        )

        self.attn = nn.MultiheadAttention(self.dim, config.n_heads, batch_first=True)

    def forward(self, x, rope_cos, rope_sin):
        # 1. Delta Update
        x_delta = self.delta_op(x)

        # 2. mHC Mixing
        x_state = x_delta.mean(dim=1)
        gates = self.gate_proj(x_state)

        g_res = torch.sigmoid(gates[:, 2 * self.n:]).unsqueeze(1).unsqueeze(-1)
        H_res = sinkhorn_knopp(self.phi_res, iterations=15)

        x_streams = x_delta.view(x.shape[0], x.shape[1], self.n, self.stream_dim)
        x_perm = (x_streams * g_res).permute(0, 1, 3, 2)
        res_act = (x_perm @ H_res.T).permute(0, 1, 3, 2)
        mhc_out = res_act.reshape(x.shape[0], x.shape[1], self.dim)

        # 3. Attention (FIXED: Explicit Causal Masking)
        normed = self.ln_1(x_delta)
        T = x.shape[1]

        # Create upper-triangular mask with -inf (Look-ahead mask)
        # This prevents the model from "cheating" by seeing future tokens
        attn_mask = torch.triu(torch.full((T, T), float('-inf'), device=x.device), diagonal=1)

        # We must set is_causal=False because we are providing the mask manually
        attn_out, _ = self.attn(normed, normed, normed, attn_mask=attn_mask, is_causal=False)

        # 4. MLP
        normed2 = self.ln_2(x_delta + attn_out)
        ffn_out = self.mlp(normed2)

        # 5. Fusion
        return x_delta + attn_out + ffn_out + mhc_out


class Model(MultimodalBase):
    def __init__(self, config=None):
        if config is None: config = NucleusConfig()
        super().__init__(config)

        self.pos_emb = nn.Embedding(config.context_len * 2, config.embed_dim)

        self.blocks = nn.ModuleList([UnifiedBlock(config) for _ in range(config.n_layers)])
        self.ln_f = RMSNorm(config.embed_dim)

        self.head = StrategicLinear(config.embed_dim, config.vocab_size,
                                    cost_coefficient=config.cost_coefficient,
                                    bias=False)

    def forward(self, v, a, t, c=None):
        device = t.device

        # 1. STANDARDIZED EMBEDDING
        x = self.embed_inputs(v, a, t, c)

        # 2. Add Absolute Position
        curr_len = x.shape[1]
        if curr_len > self.pos_emb.num_embeddings:
            x = x[:, -self.pos_emb.num_embeddings:, :]

        positions = torch.arange(0, x.shape[1], device=device)
        x = x + self.pos_emb(positions)

        # 3. Generate RoPE
        rope_cos, rope_sin = self.get_positional_embeddings(x)

        # 4. Deep Transformer Flow
        for block in self.blocks:
            x = block(x, rope_cos, rope_sin)

        x = self.ln_f(x)
        return self.head(x), None, None

--- FILE: Genetics\__init__.py ---


--- FILE: library\socialgrams\user_fred.json ---
{
  "name": "Fred",
  "trust_level": 1.0,
  "notes": "Creator. Prefer concise, high-density updates."
}

--- FILE: Organelles\hippocampus.py ---
import torch
import torch.nn.functional as F
import os
import json
import yaml
import time
import uuid
import numpy as np
from datetime import datetime

# --- OPTIONAL: FAISS FOR SCALE ---
try:
    import faiss

    HAS_FAISS = True
except ImportError:
    HAS_FAISS = False
    print(" ! Hippocampus: FAISS not found. Using PyTorch fallback (slower at >10k nodes).")
    print("   Run: pip install faiss-cpu")


class EntityNode:
    """
    Represents a single node in the Knowledge Graph.
    """

    def __init__(self, entity_name, entity_type, content):
        self.id = str(uuid.uuid4())
        self.entity = entity_name
        self.type = entity_type
        self.data = content  # The YAML dictionary
        self.embedding = None  # Tensor [1, D] (Kept for syncing)
        self.last_accessed = time.time()


class Organelle_Hippocampus:
    def __init__(self, memory_dir, device="cuda"):
        self.memory_dir = memory_dir
        self.device = device

        # Paths
        self.graph_path = os.path.join(memory_dir, "knowledge_graph.yaml")
        self.vector_path = os.path.join(memory_dir, "semantic_index.faiss")
        self.meta_path = os.path.join(memory_dir, "index_meta.json")

        # In-Memory Stores
        self.nodes = {}  # {entity_name: EntityNode}

        # Vector Store State
        self.vector_map = []  # Maps index ID -> entity_name
        self.faiss_index = None
        self.torch_index = None
        self.dimension = 768  # Default embedding size

        self._init_vector_store()
        self.load_memory()

        # Bootstrap
        if "My Knowledge Graph" not in self.nodes:
            self._bootstrap_meta_memory()

    def _init_vector_store(self):
        if HAS_FAISS:
            # Inner Product (Cosine sim if vectors are normalized)
            self.faiss_index = faiss.IndexFlatIP(self.dimension)
        else:
            self.torch_index = torch.empty(0, self.dimension).to(self.device)

    def _bootstrap_meta_memory(self):
        seed_yaml = {
            "entity": "My Knowledge Graph",
            "type": "System/Meta",
            "core_summary": "The central index of all persistent memories stored by the AI.",
            "key_facts": ["Tracks Entity Counts", "Maintains Global Timeline"],
            "relations": [],
            "timeline": [f"{datetime.now().strftime('%Y-%m-%d')}: System Genesis"],
            "last_updated": datetime.now().strftime('%Y-%m-%d'),
            "confidence": "high"
        }
        # Create without embedding initially
        self.create_memory_node(seed_yaml, "My Knowledge Graph", "System/Meta", None)

    # --- IO OPERATIONS ---
    def load_memory(self):
        # 1. Load Graph (YAML)
        if os.path.exists(self.graph_path):
            try:
                with open(self.graph_path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f) or {}
                    for name, node_data in data.items():
                        node = EntityNode(name, node_data.get('type', 'Unknown'), node_data)
                        self.nodes[name] = node
                print(f" > Hippocampus: Loaded {len(self.nodes)} semantic nodes.")
            except Exception as e:
                print(f" ! Graph Load Error: {e}")

        # 2. Load Vector Index
        if HAS_FAISS and os.path.exists(self.vector_path):
            try:
                self.faiss_index = faiss.read_index(self.vector_path)
                if os.path.exists(self.meta_path):
                    with open(self.meta_path, 'r') as f:
                        self.vector_map = json.load(f)
            except Exception as e:
                print(f" ! FAISS Load Error: {e}")
        elif not HAS_FAISS and os.path.exists(self.vector_path + ".pt"):
            # Fallback for torch
            try:
                pkg = torch.load(self.vector_path + ".pt", map_location=self.device)
                self.torch_index = pkg['vectors']
                self.vector_map = pkg['map']
            except:
                pass

    def save_memory(self):
        # 1. Save Graph
        export_data = {name: node.data for name, node in self.nodes.items()}
        try:
            with open(self.graph_path, 'w', encoding='utf-8') as f:
                yaml.dump(export_data, f, sort_keys=False, allow_unicode=True)
        except Exception as e:
            print(f" ! Graph Save Error: {e}")

        # 2. Save Vectors
        try:
            if HAS_FAISS and self.faiss_index:
                faiss.write_index(self.faiss_index, self.vector_path)
                with open(self.meta_path, 'w') as f:
                    json.dump(self.vector_map, f)
            elif self.torch_index is not None:
                torch.save({'vectors': self.torch_index, 'map': self.vector_map}, self.vector_path + ".pt")
        except Exception as e:
            print(f" ! Vector Save Error: {e}")

        print(" > Hippocampus: Consolidated.")

    # --- CORE FUNCTIONS ---

    def create_memory_node(self, yaml_dict, entity_name, entity_type, embedding_tensor):
        # Create Node
        node = EntityNode(entity_name, entity_type, yaml_dict)
        self.nodes[entity_name] = node

        # Update Vectors
        if embedding_tensor is not None:
            self._add_vector(entity_name, embedding_tensor)

        return f"Created: {entity_name}"

    def update_memory_node(self, entity_name, new_yaml_dict, new_embedding=None):
        if entity_name not in self.nodes: return "Node missing."

        node = self.nodes[entity_name]
        node.data = new_yaml_dict
        node.last_updated = time.time()

        if new_embedding is not None:
            self._add_vector(entity_name,
                             new_embedding)  # FAISS doesn't support easy update, we append and rely on map logic

        return f"Updated: {entity_name}"

    def _add_vector(self, name, tensor):
        # Normalize for Cosine Similarity
        tensor = F.normalize(tensor, p=2, dim=1)

        if HAS_FAISS:
            np_vec = tensor.cpu().numpy().astype('float32')
            self.faiss_index.add(np_vec)
            self.vector_map.append(name)
        else:
            self.torch_index = torch.cat([self.torch_index, tensor.to(self.device)], dim=0)
            self.vector_map.append(name)

    def retrieve_relevant(self, query_embedding, top_k=2, threshold=0.4, hops=1):
        """
        Retrieves nodes + 1-Hop Neighbors (Associative Recall).
        """
        if query_embedding is None: return []

        # Normalize Query
        query_embedding = F.normalize(query_embedding, p=2, dim=1)

        # 1. SEARCH
        hits = []
        if HAS_FAISS:
            q_np = query_embedding.cpu().numpy().astype('float32')
            scores, indices = self.faiss_index.search(q_np, top_k * 2)  # Grab extra candidates
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.vector_map) and score > threshold:
                    hits.append(self.vector_map[idx])
        else:
            sim = F.cosine_similarity(query_embedding, self.torch_index)
            scores, indices = torch.topk(sim, k=min(top_k * 2, len(self.torch_index)))
            for score, idx in zip(scores, indices):
                if score > threshold:
                    hits.append(self.vector_map[idx.item()])

        # Deduplicate hits (latest version of entity wins)
        # (In a naive append-only log, the last index is the newest)
        unique_hits = list(set(hits))

        # 2. GRAPH EXPANSION (Associativity)
        final_set = set(unique_hits)

        if hops > 0:
            neighbors = set()
            for entity in unique_hits:
                node = self.nodes.get(entity)
                if not node: continue

                # Check relations
                # Format: "Type -> Target"
                rels = node.data.get('relations', [])
                for rel in rels:
                    if "->" in rel:
                        target = rel.split("->")[-1].strip()
                        if target in self.nodes:
                            neighbors.add(target)

            # Add valid neighbors
            final_set.update(neighbors)

        # 3. Retrieve Nodes
        results = []
        for name in final_set:
            if name in self.nodes:
                results.append((1.0, self.nodes[name]))  # Score is dummy for expanded nodes

        return results[:top_k + 2]  # Limit context window

    def get_training_corpus(self):
        corpus = []
        for name, node in self.nodes.items():
            try:
                y_str = yaml.dump(node.data, sort_keys=False, allow_unicode=True)
                corpus.append(f"### MEMORY ENGRAM: {name} ###\n{y_str}\n<|endoftext|>")
            except:
                pass
        return corpus

    # --- PROMPTS ---

    def format_memories_for_context(self, memory_nodes):
        if not memory_nodes: return ""
        out = "### RELEVANT LONG-TERM MEMORIES (SEMANTIC GRAPH) ###\n"
        for _, node in memory_nodes:
            d = node.data
            out += f"ENTITY: {node.entity} ({node.type})\n"
            out += f"  SUMMARY: {d.get('core_summary', '')}\n"

            # Show relations to prove associativity works
            rels = d.get('relations', [])
            if rels: out += f"  LINKS: {'; '.join(rels[:4])}\n"
            out += "\n"
        return out

    def get_extraction_prompt(self, text):
        return f"""
Analyze the text and extract the MAIN Entity into the Knowledge Graph format.
Be concise.

TEMPLATE:
entity: [Name]
type: [Person/Location/Concept]
core_summary: [1-sentence definition]
relations:
  - [Relation] -> [TargetEntity]
timeline:
  - [Date]: [Event]
confidence: high

TEXT:
{text[:2500]}
"""

    def get_merge_prompt(self, existing, new_info):
        return f"""
Merge the New Info into the Existing Node.
CRITICAL: If New Info contradicts Existing, verify context. Prioritize recent data but flag low confidence.

EXISTING:
{json.dumps(existing, indent=2)}

NEW INFO:
{new_info[:2000]}

OUTPUT ONLY UPDATED YAML.
"""

--- FILE: Organelles\ribosome.py ---
# FILE: Organelles/ribosome.py
import fitz
import torch
import torch.nn as nn
from torchvision.models import vit_b_16, ViT_B_16_Weights
import tiktoken
from PIL import Image, ImageDraw, ImageFont
import warnings
import os
import io
import random
import glob
import re
import zipfile
import json
import csv
import librosa
import torchaudio
import numpy as np
from .thalamus import Organelle_Thalamus

# --- CODEC IMPORTS ---
try:
    from magvit2_pytorch import MagViT2

    HAS_MAGVIT = True
except ImportError:
    HAS_MAGVIT = False
    print(" ! Install: pip install magvit2-pytorch")

try:
    from torchaudio.models import encodec_model_24khz

    HAS_ENCODEC = True
except ImportError:
    HAS_ENCODEC = False

try:
    import cv2

    HAS_OPENCV = True
except ImportError:
    HAS_OPENCV = False

try:
    fitz.TOOLS.mupdf_display_errors(False)
except:
    pass

warnings.filterwarnings("ignore")


class RibosomeConfig:
    """Central configuration for sensory resolutions"""

    def __init__(self):
        self.image_size = 256
        self.clip_frames = 16
        # Spatial tokens per frame = (256 / 16)^2 = 16*16 = 256
        self.patch_size = 16


class Organelle_Ribosome:
    def __init__(self, device="cuda" if torch.cuda.is_available() else "cpu"):
        self.device = device
        self.config = RibosomeConfig()  # Load Default Config

        print(f" > Ribosome v23.2 (Dynamic Resolution) initializing on {device}...")

        # Visual Cortex (feature extractor)
        weights = ViT_B_16_Weights.DEFAULT
        self.retina = vit_b_16(weights=weights).to(device)

        def forward_features(x):
            x = self.retina._process_input(x)
            n = x.shape[0]
            batch_class_token = self.retina.class_token.expand(n, -1, -1)
            x = torch.cat([batch_class_token, x], dim=1)
            x = self.retina.encoder(x)
            return x

        self.retina.forward = forward_features
        self.retina.eval()
        self.visual_transform = weights.transforms()
        self.thalamus = Organelle_Thalamus(max_keep=96).to(device)

        self.tokenizer = tiktoken.get_encoding("gpt2")

        # --- VOCABULARY MAP ---
        self.text_vocab_base = 50257
        self.image_vocab_size = 16384
        self.image_vocab_base = self.text_vocab_base
        self.audio_vocab_size = 8192
        self.audio_vocab_base = self.text_vocab_base + self.image_vocab_size

        # --- VIDEO/IMAGE TOKENIZER ---
        if HAS_MAGVIT:
            try:
                # Dynamic Token Count Calculation
                spatial_tokens = (self.config.image_size // self.config.patch_size) ** 2

                # Video-aware MAGViT2 (3D for temporal)
                self.magvit = MagViT2(
                    codebook_size=self.image_vocab_size,
                    image_size=self.config.image_size,  # Dynamic Size
                    dim=512,
                    depth=8,
                    num_tokens_per_block=spatial_tokens,
                    channels=3,
                    use_3d=True  # Enable temporal (video mode)
                ).to(device)
                self.magvit.eval()
                print(f" > MAGViT-v2 Loaded (Res: {self.config.image_size}px, Clip: {self.config.clip_frames}f)")
            except Exception as e:
                print(f" ! MAGViT Init Error: {e}")
                self.magvit = None
        else:
            self.magvit = None

        # --- AUDIO TOKENIZER ---
        if HAS_ENCODEC:
            try:
                self.encodec = encodec_model_24khz().to(device)
                self.encodec.set_target_bandwidth(6.0)
                self.encodec.eval()
                print(" > EnCodec Loaded")
            except:
                self.encodec = None
        else:
            self.encodec = None

        print(" > Ribosome Online.")

    def set_tokenizer(self, new_tokenizer):
        self.tokenizer = new_tokenizer

    def _tokenize(self, text):
        text = text[:10000]
        try:
            if hasattr(self.tokenizer, "encode") and not isinstance(self.tokenizer, tiktoken.Encoding):
                toks = self.tokenizer.encode(text, add_special_tokens=False)
            else:
                toks = self.tokenizer.encode(text)

            if hasattr(toks, "ids"): toks = toks.ids
            if isinstance(toks, torch.Tensor): toks = toks.tolist()
            return toks[:1024]
        except:
            return [50256]

    # --- DECODERS ---
    def decode(self, tokens):
        """ Safe text decoder """
        text_tokens = []
        for t in tokens:
            if isinstance(t, torch.Tensor): t = t.item()
            if t < self.text_vocab_base:
                text_tokens.append(t)
        try:
            return self.tokenizer.decode(text_tokens)
        except:
            return ""

    def decode_image_tokens(self, tokens):
        if not self.magvit: return None

        indices = torch.tensor(tokens) - self.image_vocab_base
        indices = indices.clamp(0, self.image_vocab_size - 1).to(self.device)

        # Calculate target length based on image size
        # e.g., (256/16)^2 = 256 tokens
        side_tokens = self.config.image_size // self.config.patch_size
        target_len = side_tokens ** 2

        if len(indices) < target_len:
            indices = torch.cat([indices, torch.zeros(target_len - len(indices), device=self.device).long()])

        # View as [Batch, H_tokens, W_tokens]
        indices = indices[:target_len].view(1, side_tokens, side_tokens)

        with torch.no_grad():
            if hasattr(self.magvit, 'decode_from_codes'):
                rec = self.magvit.decode_from_codes(indices)
            else:
                rec = self.magvit.decode_from_indices(indices)

        # Handle 3D output if model is 3D
        if rec.ndim == 5:  # [B, C, T, H, W]
            rec = rec[:, :, 0, :, :]  # Take first frame

        rec = (rec.clamp(-1, 1) + 1) / 2
        rec = rec[0].permute(1, 2, 0).cpu().numpy() * 255
        return Image.fromarray(rec.astype(np.uint8))

    def decode_video_tokens(self, tokens, fps=8):
        if not self.magvit: return None

        indices = torch.tensor(tokens) - self.image_vocab_base
        indices = indices.clamp(0, self.image_vocab_size - 1).to(self.device)

        # Calculate Video Target Length
        # (Image_Tokens) * Clip_Frames
        side_tokens = self.config.image_size // self.config.patch_size
        tokens_per_frame = side_tokens ** 2
        target_len = self.config.clip_frames * tokens_per_frame

        if len(indices) < target_len:
            pad = target_len - len(indices)
            indices = torch.cat([indices, torch.zeros(pad, device=self.device).long()])

        # Reshape: [B, T, H_tok, W_tok]
        indices = indices[:target_len].view(1, self.config.clip_frames, side_tokens, side_tokens)

        with torch.no_grad():
            if hasattr(self.magvit, 'decode_from_codes'):
                rec = self.magvit.decode_from_codes(indices)
            else:
                rec = self.magvit.decode_from_indices(indices)

        # Output is [B, C, T, H, W]
        rec = (rec.squeeze(0).permute(1, 2, 3, 0) + 1) * 127.5  # [T, H, W, C]
        rec = rec.cpu().numpy().astype(np.uint8)

        out_path = os.path.abspath("temp_gen_video.mp4")
        writer = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), fps,
                                 (self.config.image_size, self.config.image_size))

        for frame in rec:
            writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
        writer.release()
        return out_path

    def decode_audio_tokens(self, tokens):
        if not self.encodec: return None
        codes = torch.tensor(tokens) - self.audio_vocab_base
        codes = codes.clamp(0, 1023)

        num_books = 8
        if len(codes) % num_books != 0:
            pad = num_books - (len(codes) % num_books)
            codes = torch.cat([codes, torch.zeros(pad).long()])

        T = len(codes) // num_books
        codes = codes.view(T, num_books).permute(1, 0).unsqueeze(0).to(self.device)

        with torch.no_grad():
            waveform = self.encodec.decode([codes])

        temp_path = os.path.abspath("temp_dream_audio.wav")
        torchaudio.save(temp_path, waveform[0].cpu(), 24000)
        return temp_path

    # --- INGESTION ---
    def ingest_packet(self, packet):
        v, a, t, c, kept_idx = None, None, None, None, None

        if 't' in packet:
            with open(packet['t'], 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            toks = self._tokenize(content)
            t = torch.tensor(toks).unsqueeze(0).to(self.device)

        # IMAGE HANDLING
        if 'v' in packet and packet['v'] and self.magvit:
            try:
                img = Image.open(packet['v']).convert('RGB').resize((self.config.image_size, self.config.image_size))

                # 3D MagViT expects [B, C, T, H, W]. For image, T=1.
                tens = torch.tensor(np.array(img)).permute(2, 0, 1).float().div(127.5).sub(1).unsqueeze(0).unsqueeze(
                    2).to(self.device)

                with torch.no_grad():
                    if hasattr(self.magvit, 'tokenize'):
                        indices = self.magvit.tokenize(tens)
                    else:
                        fmap = self.magvit.encode(tens)
                        indices = self.magvit.quantize(fmap)[2]

                img_tokens = indices.flatten() + self.image_vocab_base

                if t is not None:
                    t = torch.cat([t, img_tokens.unsqueeze(0)], dim=1)
                else:
                    t = img_tokens.unsqueeze(0)

                # Visual Cortex (Thalamus) still needs 2D [B, C, H, W]
                # ViT expects 224x224 usually, but accepts others if configured.
                # We use the ViT's own transform for safety.
                with torch.no_grad():
                    px = self.visual_transform(img).unsqueeze(0).to(self.device)
                    full_vis = self.retina(px)
                    v, kept_idx = self.thalamus(full_vis)
            except:
                pass

        # VIDEO HANDLING (CHRONOS)
        if 'vid' in packet and packet['vid'] and self.magvit and HAS_OPENCV:
            try:
                cap = cv2.VideoCapture(packet['vid'])
                frames = []
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret or len(frames) >= self.config.clip_frames: break
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frame = Image.fromarray(frame).resize((self.config.image_size, self.config.image_size))
                    frames.append(np.array(frame))
                cap.release()

                if frames:
                    # [B, C, T, H, W]
                    vid_tensor = torch.tensor(np.stack(frames)).permute(0, 3, 1, 2).float().div(127.5).sub(1).unsqueeze(
                        0).permute(0, 2, 1, 3, 4).to(self.device)

                    with torch.no_grad():
                        if hasattr(self.magvit, 'tokenize'):
                            indices = self.magvit.tokenize(vid_tensor)
                        else:
                            fmap = self.magvit.encode(vid_tensor)
                            indices = self.magvit.quantize(fmap)[2]

                    vid_tokens = indices.flatten() + self.image_vocab_base

                    if t is not None:
                        t = torch.cat([t, vid_tokens.unsqueeze(0)], dim=1)
                    else:
                        t = vid_tokens.unsqueeze(0)
            except Exception as e:
                print(f"Video Ingest Error: {e}")

        # AUDIO HANDLING
        audio_src = packet.get('a') or packet.get('vid')
        if audio_src and self.encodec:
            try:
                wav, sr = torchaudio.load(audio_src)
                if sr != 24000: wav = torchaudio.functional.resample(wav, sr, 24000)
                wav = wav.mean(0, keepdim=True).to(self.device).unsqueeze(0)

                with torch.no_grad():
                    encoded_frames = self.encodec.encode(wav)
                    codes = encoded_frames[0][0]
                    flat_codes = codes.permute(1, 0).flatten()
                    aud_tokens = flat_codes + self.audio_vocab_base

                    if t is not None:
                        t = torch.cat([t, aud_tokens.unsqueeze(0)], dim=1)
                    else:
                        t = aud_tokens.unsqueeze(0)
            except:
                pass

        if c is None: c = torch.zeros(1, 1, 64).to(self.device)
        if v is None: v = torch.zeros(1, 1, 768).to(self.device)
        if a is None: a = torch.zeros(1, 1, 128).to(self.device)
        if t is None: t = torch.tensor([[50256]]).to(self.device)

        return v, a, t, c, kept_idx

    def render_text_to_image(self, text):
        return Image.new('RGB', (100, 100))

    def _load_control(self, p):
        return torch.zeros(1, 1, 64).to(self.device)

    def get_engram(self, p):
        return None

    def ingest_doc(self, p):
        return self.ingest_packet({'t': p})

    def ingest_cbz(self, p):
        return self.ingest_packet({'t': p})

    def ingest(self, p):
        return self.ingest_packet({'t': p})

--- FILE: Organelles\thalamus.py ---
import torch
import torch.nn as nn
import torch.nn.functional as F


class Organelle_Thalamus(nn.Module):
    def __init__(self, dim_visual=768, dim_text=768, max_keep=96):
        super().__init__()
        self.max_keep = max_keep

        self.scorer = nn.Sequential(
            nn.Linear(dim_visual + dim_text, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

        self.register_token = nn.Parameter(torch.randn(1, 1, dim_visual))

    def forward(self, dense_visual, text_embedding=None):
        B, N, D = dense_visual.shape

        if text_embedding is not None:
            ctx = text_embedding.mean(dim=1, keepdim=True).expand(-1, N, -1)
            scorer_in = torch.cat([dense_visual, ctx], dim=-1)
        else:
            scorer_in = torch.cat([dense_visual, torch.zeros_like(dense_visual)], dim=-1)

        scores = self.scorer(scorer_in).squeeze(-1)

        num_keep = min(self.max_keep, N)
        num_keep = max(1, num_keep)

        top_k_idx = torch.topk(scores, num_keep, dim=1).indices
        top_k_idx, _ = torch.sort(top_k_idx, dim=1)

        gather_idx = top_k_idx.unsqueeze(-1).expand(-1, -1, D)
        sparse_visual = torch.gather(dense_visual, 1, gather_idx)

        register = self.register_token.expand(B, -1, -1)
        final_visual = torch.cat([sparse_visual, register], dim=1)

        return final_visual, top_k_idx

--- FILE: Organelles\__init__.py ---


--- FILE: Plugins\tab_comic.py ---
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import os
import time
import torch
from PIL import Image, ImageTk
import traceback
from datetime import datetime


# --- HEADLESS HELPER ---
class MockVar:
    def __init__(self, value=None): self._val = value

    def set(self, value): self._val = value

    def get(self): return self._val


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Comic Factory"
        self.is_running = False
        self.stop_requested = False
        self.image_refs = []  # Keep references to prevent GC

        # --- DYNAMIC PATHS ---
        default_data = self.app.paths.get("data", os.path.join(self.app.paths["root"], "Training_Data"))
        self.output_dir = os.path.join(default_data, "Comics_Output")
        if not os.path.exists(self.output_dir):
            try:
                os.makedirs(self.output_dir)
            except:
                pass

        # --- STATE ---
        if self.parent is None:
            # Headless Defaults
            self.seed_prompt = MockVar("A robot discovering a flower in a cyberpunk city")
            self.num_panels = MockVar(4)
            self.style_prompt = MockVar("comic book style, vibrant colors, detailed line art")
            self.status_var = MockVar("Ready.")
        else:
            # GUI Defaults
            self.seed_prompt = tk.StringVar(value="A robot discovering a flower in a cyberpunk city")
            self.num_panels = tk.IntVar(value=4)
            self.style_prompt = tk.StringVar(value="comic book style, vibrant colors, detailed line art")
            self.status_var = tk.StringVar(value="Ready.")

        self._setup_ui()

    def _setup_ui(self):
        if self.parent is None: return

        # Main Layout: Left (Controls) | Right (Comic Strip View)
        panel = ttk.PanedWindow(self.parent, orient="horizontal")
        panel.pack(fill="both", expand=True, padx=10, pady=10)

        left = ttk.Frame(panel, width=350)
        right = ttk.Frame(panel)
        panel.add(left, weight=1)
        panel.add(right, weight=3)

        # --- LEFT: CONTROLS ---
        fr_input = ttk.LabelFrame(left, text="Story Settings", padding=15)
        fr_input.pack(fill="x", pady=5)

        ttk.Label(fr_input, text="Story Concept:").pack(anchor="w")
        self.txt_seed = tk.Text(fr_input, height=4, font=("Segoe UI", 10), wrap="word")
        self.txt_seed.insert("1.0", self.seed_prompt.get())
        self.txt_seed.pack(fill="x", pady=5)

        ttk.Label(fr_input, text="Art Style:").pack(anchor="w", pady=(10, 0))
        ttk.Entry(fr_input, textvariable=self.style_prompt).pack(fill="x", pady=5)

        ttk.Label(fr_input, text="Panels:").pack(anchor="w", pady=(10, 0))
        ttk.Spinbox(fr_input, from_=1, to=10, textvariable=self.num_panels).pack(fill="x", pady=5)

        self.btn_gen = ttk.Button(left, text="GENERATE COMIC", command=self._start_generation)
        self.btn_gen.pack(fill="x", pady=20)

        self.lbl_status = ttk.Label(left, textvariable=self.status_var, foreground=self.app.colors["ACCENT"],
                                    wraplength=300)
        self.lbl_status.pack(pady=10)

        # --- RIGHT: PREVIEW AREA ---
        fr_view = ttk.LabelFrame(right, text="Comic Strip Preview", padding=10)
        fr_view.pack(fill="both", expand=True)

        # Scrollable Canvas for vertical strip
        self.canvas = tk.Canvas(fr_view, bg=self.app.colors["BG_CARD"], highlightthickness=0)
        sb = ttk.Scrollbar(fr_view, orient="vertical", command=self.canvas.yview)

        self.scroll_frame = ttk.Frame(self.canvas)
        self.canvas.create_window((0, 0), window=self.scroll_frame, anchor="nw")

        self.canvas.configure(yscrollcommand=sb.set)
        self.scroll_frame.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))

        self.canvas.pack(side="left", fill="both", expand=True)
        sb.pack(side="right", fill="y")

    def _update_status(self, msg):
        if self.parent:
            self.status_var.set(msg)
            self.parent.update_idletasks()
        else:
            print(f"[Comic] {msg}")

    def _start_generation(self):
        if self.is_running: return

        # Get text from widget if GUI
        if self.parent:
            seed = self.txt_seed.get("1.0", tk.END).strip()
            self.seed_prompt.set(seed)

        active_id = self.app.active_lobe.get()
        brain = self.app.lobes[active_id]

        if not brain:
            messagebox.showerror("Error", "No Lobe Loaded.")
            return

        # Check Capabilities
        self.is_diffusion = hasattr(brain, 'timestep_emb') or (self.app.lobe_types.get(active_id) == "diffusion")

        self.is_running = True
        if self.parent: self.btn_gen.config(state="disabled")

        # Clear previous
        if self.parent:
            for w in self.scroll_frame.winfo_children(): w.destroy()
            self.image_refs = []

        threading.Thread(target=self._worker_comic, daemon=True).start()

    def _worker_comic(self):
        try:
            brain = self.app.lobes[self.app.active_lobe.get()]
            ribosome = self.app.ribosome

            prompt = self.seed_prompt.get()
            style = self.style_prompt.get()
            panels = self.num_panels.get()

            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            comic_folder = os.path.join(self.output_dir, f"Comic_{ts}")
            os.makedirs(comic_folder, exist_ok=True)

            self._update_status("Drafting Storyboard...")

            # 1. Generate Storyboard (Text)
            # If Diffusion model only, skip text gen or use internal prompt logic
            storyboard = []
            if not self.is_diffusion:
                # AR Model: Ask it to split story
                story_prompt = f"Create a {panels}-panel comic script about: {prompt}. Format: Panel 1: [Desc], Panel 2: [Desc]..."
                story_text = self._generate_text(brain, ribosome, story_prompt, max_new=200)

                # Naive parse
                parts = story_text.split("Panel")
                for p in parts:
                    if len(p.strip()) > 10:
                        storyboard.append(p.strip())

                # Fallback
                if len(storyboard) < panels:
                    storyboard = [f"{prompt}, scene {i + 1}" for i in range(panels)]
            else:
                # Diffusion Model: Just iterate variations
                storyboard = [f"{prompt}, sequence {i + 1}" for i in range(panels)]

            storyboard = storyboard[:panels]

            # 2. Generate Panels
            for i, panel_desc in enumerate(storyboard):
                self._update_status(f"Rendering Panel {i + 1}/{panels}...")

                full_prompt = f"{panel_desc}, {style}"

                img = self._generate_image(brain, ribosome, full_prompt)

                if img:
                    # Save
                    fname = f"panel_{i + 1}.png"
                    save_path = os.path.join(comic_folder, fname)
                    img.save(save_path)

                    # Display
                    if self.parent:
                        self.parent.after(0, lambda image=img, txt=panel_desc: self._add_panel_to_ui(image, txt))

                time.sleep(0.5)

            self._update_status(f"Comic Complete! Saved to {comic_folder}")

        except Exception as e:
            self._update_status(f"Error: {e}")
            traceback.print_exc()
        finally:
            self.is_running = False
            if self.parent: self.parent.after(0, lambda: self.btn_gen.config(state="normal"))

    def _generate_text(self, brain, ribosome, prompt, max_new=100):
        ids = ribosome._tokenize(prompt)
        t = torch.tensor(ids, device=self.app.device).unsqueeze(0)
        v = torch.zeros(1, 1, 768).to(self.app.device)
        a = torch.zeros(1, 1, 128).to(self.app.device)

        out_ids = []
        with self.app.gpu_lock:
            with torch.no_grad():
                for _ in range(max_new):
                    logits, _, _ = brain(v, a, t)
                    next_tok = torch.argmax(logits[:, -1, :], dim=-1)
                    t = torch.cat([t, next_tok.unsqueeze(0)], dim=1)
                    out_ids.append(next_tok.item())
                    if next_tok.item() == 50256: break

        return ribosome.decode(out_ids)

    def _generate_image(self, brain, ribosome, prompt):
        # Requires a model capable of image generation (Diffusion)
        if not hasattr(brain, 'timestep_emb'):
            return None  # AR model can't draw (yet)

        ids = ribosome._tokenize(prompt)

        with self.app.gpu_lock:
            with torch.no_grad():
                # Standard Diffusion Generation Call
                # (Assumes brain.generate returns tokens containing image blocks)
                tokens = brain.generate(
                    prompt_tokens=ids,
                    max_length=1024 + len(ids),  # Enough for 32x32 patches
                    steps=25,
                    temperature=1.0
                )

        # Extract Image from tokens
        # Find visual tokens range
        vis_tokens = [t for t in tokens if t >= ribosome.image_vocab_base and t < ribosome.audio_vocab_base]

        if not vis_tokens: return None

        try:
            return ribosome.decode_image_tokens(vis_tokens)
        except:
            return None

    def _add_panel_to_ui(self, pil_img, text):
        # Resize for display
        display_img = pil_img.copy()
        display_img.thumbnail((500, 500))
        photo = ImageTk.PhotoImage(display_img)
        self.image_refs.append(photo)

        # Container
        fr = ttk.Frame(self.scroll_frame, style="Card.TFrame", padding=10)
        fr.pack(fill="x", pady=10, padx=10)

        # Image
        lbl_img = ttk.Label(fr, image=photo, background=self.app.colors["BG_CARD"])
        lbl_img.pack()

        # Caption
        lbl_cap = ttk.Label(fr, text=text, wraplength=480, justify="center",
                            background=self.app.colors["BG_CARD"], foreground=self.app.colors["FG_TEXT"])
        lbl_cap.pack(pady=(5, 0))

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'canvas'): self.canvas.config(bg=c["BG_CARD"])

--- FILE: Plugins\tab_cortex.py ---
import tkinter as tk
from tkinter import ttk, messagebox, filedialog
import os
import torch
import torch.optim as optim
from torch.cuda.amp import GradScaler
import importlib.util
import gc
import traceback
from datetime import datetime


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Cortex Control"

        self.available_genetics = {}
        self.genome_var = tk.StringVar(value="Choose Genetics")
        self.info_text = tk.StringVar(value="Select a genetic structure to initialize.")
        self.status_labels = {}

        self._setup_ui()
        self.parent.bind("<Visibility>", lambda e: self._refresh_ui())
        self._scan_genetics()
        self._refresh_ui()

    def _scan_genetics(self):
        self.available_genetics = {}
        g_dir = self.app.paths['genetics']
        if not os.path.exists(g_dir): return

        files = [f for f in os.listdir(g_dir) if f.endswith(".py") and not f.startswith("__")]
        print(f"[Cortex] Scanning genetics in {g_dir}...")

        for f in files:
            try:
                path = os.path.join(g_dir, f)
                spec = importlib.util.spec_from_file_location("dynamic_dna", path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                if hasattr(module, "INFO"):
                    name = module.INFO.get("name", f)
                    self.available_genetics[name] = module
                    print(f"   > Loaded: {name}")
                else:
                    print(f"   ! Skipped {f}: No INFO dict found.")
            except Exception as e:
                print(f"   ! FAILED to load {f}: {e}")

        if hasattr(self, 'combo'):
            self.combo['values'] = list(self.available_genetics.keys())

    def _setup_ui(self):
        frame_status = ttk.LabelFrame(self.parent, text="Cortex Status", padding=15)
        frame_status.pack(fill="x", padx=20, pady=10)

        for i in range(1, 5):
            row = ttk.Frame(frame_status)
            row.pack(fill="x", pady=2)
            lbl = ttk.Label(row, text=f"Lobe {i}: Checking...", font=("Segoe UI", 10))
            lbl.pack(side="left")
            self.status_labels[i] = lbl

        frame_mgmt = ttk.LabelFrame(self.parent, text="Genetic Engineering", padding=15)
        frame_mgmt.pack(fill="both", expand=True, padx=20, pady=10)

        ttk.Label(frame_mgmt, text="Target Genetics:").pack(anchor="w")
        self.combo = ttk.Combobox(frame_mgmt, textvariable=self.genome_var, values=list(self.available_genetics.keys()),
                                  state="readonly")
        self.combo.pack(fill="x", pady=5)
        self.combo.bind("<<ComboboxSelected>>", self._on_select)

        info_lbl = ttk.Label(frame_mgmt, textvariable=self.info_text, foreground=self.app.colors["ACCENT"],
                             justify="left")
        info_lbl.pack(pady=10, anchor="w")

        frame_pwr = ttk.LabelFrame(frame_mgmt, text="Power & Memory", padding=10)
        frame_pwr.pack(fill="x", pady=5)
        ttk.Button(frame_pwr, text="⚡ ACTIVATE LOBE", command=self._activate_brain).pack(side="left", fill="x",
                                                                                         expand=True, padx=5)
        ttk.Button(frame_pwr, text="💤 DEACTIVATE (Unload)", command=self._deactivate_brain).pack(side="left", fill="x",
                                                                                                 expand=True, padx=5)

        frame_store = ttk.LabelFrame(frame_mgmt, text="Storage Operations", padding=10)
        frame_store.pack(fill="x", pady=5)
        ttk.Button(frame_store, text="INITIALIZE NEW", command=self._init_brain).pack(side="left", fill="x",
                                                                                      expand=True, padx=5)
        ttk.Button(frame_store, text="SAVE AS...", command=self._save_brain).pack(side="left", fill="x", expand=True,
                                                                                  padx=5)
        ttk.Button(frame_store, text="🛡️ BACKUP", command=self._backup_brain).pack(side="left", fill="x", expand=True,
                                                                                   padx=5)
        ttk.Button(frame_store, text="LOAD FILE", command=self._load_brain).pack(side="left", fill="x", expand=True,
                                                                                 padx=5)

        ttk.Button(frame_mgmt, text="🔄 Rescan Genetics Folder", command=self._scan_genetics).pack(fill="x", pady=10)

    def _on_select(self, event):
        name = self.genome_var.get()
        if name in self.available_genetics:
            info = self.available_genetics[name].INFO
            self.info_text.set(f"{info.get('desc', '')}\nEst. VRAM: {info.get('vram_train', '?')}")

    def _refresh_ui(self):
        for i in range(1, 5):
            brain = self.app.lobes[i]
            lbl = self.status_labels[i]
            if brain:
                g_name = self.app.lobe_genomes.get(i, "Unknown")
                m_type = self.app.lobe_types.get(i, "Unknown")
                is_active = (self.app.active_lobe.get() == i)
                prefix = "➤ " if is_active else "   "
                color = self.app.colors["SUCCESS"] if is_active else self.app.colors["FG_TEXT"]
                font = ("Segoe UI", 10, "bold") if is_active else ("Segoe UI", 10)
                opt_name = "AdamW"
                if hasattr(self.app.optimizers[i], 'adamw'): opt_name = "Muon"
                lbl.config(text=f"{prefix}Lobe {i}: ONLINE ({g_name} | {m_type}) [{opt_name}]", foreground=color,
                           font=font)
            else:
                path = os.path.join(self.app.paths['lobes'], f"brain_lobe_{i}.pt")
                prefix = "➤ " if (self.app.active_lobe.get() == i) else "   "
                if os.path.exists(path):
                    try:
                        meta = torch.load(path, map_location="cpu")
                        g_name = meta.get("genome", "GPT2") if isinstance(meta, dict) else "GPT2"
                        lbl.config(text=f"{prefix}Lobe {i}: OFFLINE (Ready: {g_name})",
                                   foreground=self.app.colors["WARN"], font=("Segoe UI", 10))
                    except:
                        lbl.config(text=f"{prefix}Lobe {i}: OFFLINE (Corrupt File)",
                                   foreground=self.app.colors["ERROR"], font=("Segoe UI", 10))
                else:
                    lbl.config(text=f"{prefix}Lobe {i}: EMPTY", foreground=self.app.colors["FG_DIM"],
                               font=("Segoe UI", 10))

    def on_theme_change(self):
        self._refresh_ui()

    def _deactivate_brain(self):
        active_id = self.app.active_lobe.get()
        if self.app.lobes[active_id] is None: return
        self.app.lobes[active_id] = None
        self.app.optimizers[active_id] = None
        self.app.scalers[active_id] = None
        gc.collect();
        torch.cuda.empty_cache()
        self._refresh_ui()
        self.app.save_state()
        self.app.refresh_header()

    def _activate_brain(self):
        active_id = self.app.active_lobe.get()
        if self.app.lobes[active_id] is not None:
            messagebox.showinfo("Info", f"Lobe {active_id} is already active.")
            return
        path = os.path.join(self.app.paths['lobes'], f"brain_lobe_{active_id}.pt")
        if os.path.exists(path):
            self.app._load_single_lobe(active_id, path)
            self._refresh_ui()
            self.app.save_state()
        else:
            if messagebox.askyesno("Activate", f"No file for Lobe {active_id}.\nInitialize new?"):
                self._init_brain()

    def _backup_brain(self):
        active_id = self.app.active_lobe.get()
        path = os.path.join(self.app.paths['lobes'], f"brain_lobe_{active_id}.pt")
        if not os.path.exists(path): return
        backup_dir = os.path.join(self.app.paths['lobes'], "backups")
        if not os.path.exists(backup_dir): os.makedirs(backup_dir)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        bak = os.path.join(backup_dir, f"lobe_{active_id}_{ts}.pt")
        import shutil
        shutil.copy2(path, bak)
        messagebox.showinfo("Backup", f"Saved to backups folder.")

    def _init_brain(self):
        active_id = self.app.active_lobe.get()
        genome = self.genome_var.get()
        if genome not in self.available_genetics:
            messagebox.showerror("Error", "Invalid Genetics")
            return
        if self.app.lobes[active_id] and not messagebox.askyesno("Overwrite", "Lobe active. Overwrite?"): return

        try:
            module = self.available_genetics[genome]
            config = module.NucleusConfig()
            brain = module.Model(config).to(self.app.device)
            self.app.lobes[active_id] = brain
            self.app.lobe_genomes[active_id] = genome

            # --- MODEL TYPE INFERENCE ---
            model_type = "diffusion" if "diffusion" in genome.lower() else "ar"
            self.app.lobe_types[active_id] = model_type

            if "Muon" in genome:
                from Genetics.muon import Muon
                self.app.optimizers[active_id] = Muon(brain.parameters(), lr=0.0005, momentum=0.95)
                print(f"[SYS] Initialized with Muon Optimizer (Safe Mode LR=0.0005)")
            else:
                self.app.optimizers[active_id] = optim.AdamW(brain.parameters(), lr=2e-5)

            self.app.scalers[active_id] = GradScaler()

            if hasattr(brain, "tokenizer"):
                self.app.ribosome.set_tokenizer(brain.tokenizer)
            else:
                import tiktoken
                self.app.ribosome.set_tokenizer(tiktoken.get_encoding("gpt2"))

            path = os.path.join(self.app.paths['lobes'], f"brain_lobe_{active_id}.pt")
            torch.save({
                "genome": genome,
                "model_type": model_type,  # Save type
                "state_dict": brain.state_dict()
            }, path)

            self._refresh_ui()
            self.app.refresh_header()
            self.app.save_state()
        except Exception as e:
            messagebox.showerror("Init Failed", str(e))
            print(e)

    def _save_brain(self):
        active_id = self.app.active_lobe.get()
        if not self.app.lobes[active_id]: return
        f = filedialog.asksaveasfilename(initialdir=self.app.paths['lobes'], defaultextension=".pt")
        if f:
            torch.save({
                "genome": self.app.lobe_genomes[active_id],
                "model_type": self.app.lobe_types.get(active_id, "ar"),
                "state_dict": self.app.lobes[active_id].state_dict()
            }, f)

    def _load_brain(self):
        f = filedialog.askopenfilename(initialdir=self.app.paths['lobes'], filetypes=[("Brain Files", "*.pt")])
        if f:
            self.app._load_single_lobe(self.app.active_lobe.get(), f)
            self._refresh_ui()
            self.app.save_state()

--- FILE: Plugins\tab_council.py ---
import tkinter as tk
from tkinter import ttk
import threading
import itertools
import time
import torch


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "The Council"

        self.is_running = False
        self.members = {1: tk.BooleanVar(value=True), 2: tk.BooleanVar(value=True),
                        3: tk.BooleanVar(value=True), 4: tk.BooleanVar(value=True)}
        self.topic = tk.StringVar()

        self._setup_ui()

    def _setup_ui(self):
        # Controls
        ctrl = ttk.LabelFrame(self.parent, text="Chamber Controls", padding=15)
        ctrl.pack(fill="x", padx=20, pady=10)

        ttk.Label(ctrl, text="Topic:").pack(side="left")
        ttk.Entry(ctrl, textvariable=self.topic).pack(side="left", fill="x", expand=True, padx=10)

        self.btn = ttk.Button(ctrl, text="CONVENE", command=self._toggle)
        self.btn.pack(side="right")

        # Members
        mem_fr = ttk.Frame(self.parent)
        mem_fr.pack(fill="x", padx=25)
        ttk.Label(mem_fr, text="Speakers:").pack(side="left")
        for i in range(1, 5):
            ttk.Checkbutton(mem_fr, text=f"Lobe {i}", variable=self.members[i]).pack(side="left", padx=5)

        # Chat
        self.chat = tk.Text(self.parent, font=("Consolas", 11), wrap="word",
                            bg=self.app.colors["BG_MAIN"], fg=self.app.colors["FG_TEXT"],
                            borderwidth=0, padx=20, pady=20)
        self.chat.pack(fill="both", expand=True, padx=20, pady=10)

    def _toggle(self):
        if self.is_running:
            self.is_running = False
            self.btn.config(text="CONVENE")
        else:
            self.is_running = True
            self.btn.config(text="ADJOURN")
            threading.Thread(target=self._run_debate, daemon=True).start()

    def _run_debate(self):
        topic = self.topic.get() or "The nature of consciousness"
        history = f"TOPIC: {topic}\n\n"
        self.parent.after(0, lambda: self.chat.insert(tk.END, history))

        active_ids = [i for i in range(1, 5) if self.members[i].get() and self.app.lobes[i]]
        if not active_ids:
            self.parent.after(0, lambda: self.chat.insert(tk.END, "[System] No Lobes online.\n"))
            self.is_running = False
            return

        cycle = itertools.cycle(active_ids)

        # V/A/C zeros
        v = torch.zeros(1, 1, 768).to(self.app.device)
        a = torch.zeros(1, 1, 128).to(self.app.device)
        c = torch.zeros(1, 1, 64).to(self.app.device)

        while self.is_running:
            lobe_id = next(cycle)
            brain = self.app.lobes[lobe_id]
            brain.eval()

            # Generate
            t = torch.tensor(self.app.ribosome.tokenizer.encode(history)).unsqueeze(0).to(self.app.device)
            # Clip history if too long
            if t.shape[1] > 900: t = t[:, -900:]

            response_toks = []
            with torch.no_grad():
                for _ in range(64):
                    logits, _, _ = brain(v, a, t, c)
                    token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(-1)
                    t = torch.cat([t, token], dim=1)
                    response_toks.append(token.item())
                    if token.item() == 50256: break

            text = self.app.ribosome.tokenizer.decode(response_toks).strip()
            history += f"Lobe {lobe_id}: {text}\n\n"

            self.parent.after(0, lambda l=lobe_id, t=text: self._append_msg(l, t))
            time.sleep(1)

    def _append_msg(self, lobe_id, text):
        self.chat.insert(tk.END, f"LOBE {lobe_id}: {text}\n\n")
        self.chat.see(tk.END)

        def on_theme_change(self):
            # Update Text Widgets which don't use ttk styles
            c = self.app.colors
            if hasattr(self, 'log_box'):
                self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"], insertbackground=c["ACCENT"])
            if hasattr(self, 'chat_box'):
                self.chat_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"], insertbackground=c["ACCENT"])
            # Update Canvas backgrounds if needed
            if hasattr(self, 'canvas'):
                self.canvas.config(bg=c["BG_MAIN"])

--- FILE: Plugins\tab_diffusion_trainer.py ---
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import threading
import os
import time
import torch
import torch.nn.functional as F
from torch.amp import autocast
from torch.cuda.amp import GradScaler
from datetime import datetime
import random
import queue
import math
import numpy as np
import json
from collections import deque

try:
    import fitz  # PyMuPDF

    HAS_FITZ = True
except ImportError:
    HAS_FITZ = False
    print(" ! Diffusion Trainer: Install 'pymupdf' for PDF support.")


# --- HEADLESS HELPER ---
class MockVar:
    def __init__(self, value=None): self._val = value

    def set(self, value): self._val = value

    def get(self): return self._val


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Diffusion Director"
        self.is_training = False
        self.is_paused = False
        self.stop_requested = False

        # PIPELINE
        self.data_queue = queue.Queue(maxsize=50)
        self.task_queue = queue.Queue(maxsize=100)
        self.ram_cache = {}
        self.MAX_CACHE_SIZE = 25000

        # STATE
        self.training_queue = []
        self.all_scanned_packets = []
        self.train_ext_vars = {}
        self.train_type_vars = {}
        self.history_file = os.path.join(self.app.paths["root"], "diffusion_history.json")
        self.recent_folders = self._load_json(self.history_file, [])
        default_folder = self.recent_folders[0] if self.recent_folders else "D:/Training_Data"

        # Telepathy/Config override
        if hasattr(self.app.paths, "data") and os.path.exists(self.app.paths["data"]):
            default_folder = self.app.paths["data"]

        self.processed_count = 0
        self.total_items = 0
        self.loss_history = {'recon': deque(maxlen=10000), 'game': deque(maxlen=10000)}

        # --- INITIALIZE VARIABLES (GUI vs HEADLESS) ---
        if self.parent is None:
            # Headless Mode
            self.folder_path = MockVar(default_folder)
            self.use_uniform = MockVar(True)
            self.uniform_ratio_min = MockVar(0.15)
            self.uniform_ratio_max = MockVar(0.60)
            self.use_cross_modal = MockVar(False)
            self.cross_modal_prob = MockVar(0.25)
            self.use_modality_specific = MockVar(False)
            self.nursery_autofit = MockVar(True)
            self.nurse_recon = [MockVar(0.1), MockVar(8.0), MockVar(True)]
            self.nurse_game = [MockVar(0.001), MockVar(10.0), MockVar(True)]
            self.auto_scroll = MockVar(False)
            self.autosave_enabled = MockVar(True)
            self.autosave_interval = MockVar(100)
            self.target_epochs = MockVar(1)
            self.narrative_mode = MockVar(True)
            self.num_workers = MockVar(4)
        else:
            # GUI Mode
            self.folder_path = tk.StringVar(value=default_folder)
            self.use_uniform = tk.BooleanVar(value=True)
            self.uniform_ratio_min = tk.DoubleVar(value=0.15)
            self.uniform_ratio_max = tk.DoubleVar(value=0.60)
            self.use_cross_modal = tk.BooleanVar(value=False)
            self.cross_modal_prob = tk.DoubleVar(value=0.25)
            self.use_modality_specific = tk.BooleanVar(value=False)
            self.nursery_autofit = tk.BooleanVar(value=True)
            self.nurse_recon = [tk.DoubleVar(value=0.1), tk.DoubleVar(value=8.0), tk.BooleanVar(value=True)]
            self.nurse_game = [tk.DoubleVar(value=0.001), tk.DoubleVar(value=10.0), tk.BooleanVar(value=True)]
            self.auto_scroll = tk.BooleanVar(value=True)
            self.autosave_enabled = tk.BooleanVar(value=True)
            self.autosave_interval = tk.IntVar(value=100)
            self.target_epochs = tk.IntVar(value=1)
            self.narrative_mode = tk.BooleanVar(value=True)
            self.num_workers = tk.IntVar(value=4)

        self._setup_ui()

    def _setup_ui(self):
        if self.parent is None: return

        style = ttk.Style()
        style.map('TCombobox', fieldbackground=[('readonly', self.app.colors['BG_CARD'])],
                  selectbackground=[('readonly', self.app.colors['BG_CARD'])],
                  selectforeground=[('readonly', self.app.colors['FG_TEXT'])],
                  foreground=[('readonly', self.app.colors['FG_TEXT'])])

        split = ttk.PanedWindow(self.parent, orient="horizontal")
        split.pack(fill="both", expand=True, padx=10, pady=10)

        left = ttk.Frame(split)
        right = ttk.Frame(split)
        split.add(left, weight=3)
        split.add(right, weight=1)

        # 1. DATA SOURCE
        fr_src = ttk.LabelFrame(left, text="Data Feed", padding=10)
        fr_src.pack(fill="x", pady=5)

        row_src = ttk.Frame(fr_src)
        row_src.pack(fill="x", expand=True)
        self.cmb_folder = ttk.Combobox(row_src, textvariable=self.folder_path, values=self.recent_folders)
        self.cmb_folder.pack(side="left", fill="x", expand=True)
        self.cmb_folder.bind("<<ComboboxSelected>>", lambda e: self._scan_files())
        ttk.Button(row_src, text="📂", width=3, command=self._browse_folder).pack(side="left", padx=2)

        row_btns = ttk.Frame(fr_src)
        row_btns.pack(fill="x", pady=5)
        ttk.Button(row_btns, text="SCAN FOLDER", command=self._scan_files).pack(side="left", fill="x", expand=True,
                                                                                padx=2)
        ttk.Button(row_btns, text="CLEAR QUEUE", command=self._clear_queue).pack(side="left", fill="x", expand=True,
                                                                                 padx=2)

        # 2. MASKING
        fr_mask = ttk.LabelFrame(left, text="Masking Curriculum", padding=10)
        fr_mask.pack(fill="x", pady=5)

        f1 = ttk.Frame(fr_mask)
        f1.pack(fill="x", pady=2)
        ttk.Checkbutton(f1, text="Phase 1: Uniform", variable=self.use_uniform).pack(side="left")
        ttk.Label(f1, text="Ratio:").pack(side="left", padx=5)
        ttk.Entry(f1, textvariable=self.uniform_ratio_min, width=5).pack(side="left")
        ttk.Label(f1, text="-").pack(side="left")
        ttk.Entry(f1, textvariable=self.uniform_ratio_max, width=5).pack(side="left")

        f2 = ttk.Frame(fr_mask)
        f2.pack(fill="x", pady=2)
        ttk.Checkbutton(f2, text="Phase 2: Cross-Modal", variable=self.use_cross_modal).pack(side="left")
        ttk.Label(f2, text="Prob:").pack(side="left", padx=5)
        ttk.Scale(f2, from_=0.0, to=1.0, variable=self.cross_modal_prob, length=100).pack(side="left")

        f3 = ttk.Frame(fr_mask)
        f3.pack(fill="x", pady=2)
        ttk.Checkbutton(f3, text="Phase 3: Fine-Grained", variable=self.use_modality_specific).pack(side="left")

        # 3. NURSERY
        fr_nurse = ttk.LabelFrame(left, text="Nursery", padding=10)
        fr_nurse.pack(fill="x", pady=5)
        ttk.Checkbutton(fr_nurse, text="Auto-Fit", variable=self.nursery_autofit).pack(anchor="w")

        grid = ttk.Frame(fr_nurse)
        grid.pack(fill="x", pady=5)
        ttk.Label(grid, text="Channel", font=("Segoe UI", 8, "bold")).grid(row=0, column=0, sticky="w")
        ttk.Label(grid, text="Active", font=("Segoe UI", 8)).grid(row=0, column=1)
        ttk.Label(grid, text="Min", font=("Segoe UI", 8)).grid(row=0, column=2)
        ttk.Label(grid, text="Max", font=("Segoe UI", 8)).grid(row=0, column=3)

        def add_row(r, label, vars):
            ttk.Label(grid, text=label).grid(row=r, column=0, sticky="e", padx=5)
            ttk.Checkbutton(grid, variable=vars[2]).grid(row=r, column=1)
            ttk.Entry(grid, textvariable=vars[0], width=8).grid(row=r, column=2, padx=2)
            ttk.Entry(grid, textvariable=vars[1], width=8).grid(row=r, column=3, padx=2)

        add_row(1, "Reconstruction:", self.nurse_recon)
        add_row(2, "Game Penalty:", self.nurse_game)

        # 4. OPERATIONS
        fr_ctrl = ttk.LabelFrame(left, text="Operations", padding=10)
        fr_ctrl.pack(fill="x", pady=5)

        row_sets = ttk.Frame(fr_ctrl)
        row_sets.pack(fill="x", pady=5)
        ttk.Label(row_sets, text="Epochs:").pack(side="left")
        ttk.Spinbox(row_sets, from_=1, to=999, textvariable=self.target_epochs, width=5).pack(side="left", padx=5)
        ttk.Label(row_sets, text="CPU Workers:").pack(side="left", padx=(10, 0))
        ttk.Scale(row_sets, from_=1, to=16, variable=self.num_workers, orient="horizontal", length=100).pack(
            side="left", padx=5)
        ttk.Checkbutton(row_sets, text="Narrative Mode", variable=self.narrative_mode).pack(side="left", padx=10)

        row_btn = ttk.Frame(fr_ctrl)
        row_btn.pack(fill="x")
        self.btn_start = ttk.Button(row_btn, text="START TRAINING", command=self._start_training)
        self.btn_start.pack(side="left", fill="x", expand=True, padx=2)
        self.btn_pause = ttk.Button(row_btn, text="PAUSE", command=self._toggle_pause, state="disabled")
        self.btn_pause.pack(side="left", fill="x", expand=True, padx=2)

        # 5. LOGS
        fr_log = ttk.LabelFrame(left, text="Logs", padding=10)
        fr_log.pack(fill="both", expand=True, pady=5)
        self.log_box = tk.Text(fr_log, font=("Consolas", 9), bg=self.app.colors["BG_MAIN"],
                               fg=self.app.colors["FG_TEXT"])
        self.log_box.pack(fill="both", expand=True, side="left")
        sb = ttk.Scrollbar(fr_log, command=self.log_box.yview)
        self.log_box.configure(yscrollcommand=sb.set)
        sb.pack(side="right", fill="y")

        self.log_box.tag_config("info", foreground=self.app.colors["ACCENT"])
        self.log_box.tag_config("prog", foreground=self.app.colors["FG_DIM"])
        self.log_box.tag_config("save", foreground=self.app.colors["SUCCESS"], font=("Consolas", 9, "bold"))
        self.log_box.tag_config("warn", foreground=self.app.colors["WARN"])
        self.log_box.tag_config("error", foreground=self.app.colors["ERROR"])

        # 6. CENSUS
        fr_census = ttk.LabelFrame(right, text="Census (Filter)", padding=10)
        fr_census.pack(fill="both", expand=True)
        self.canvas = tk.Canvas(fr_census, width=200, bg=self.app.colors["BG_MAIN"], highlightthickness=0)
        scr = ttk.Scrollbar(fr_census, orient="vertical", command=self.canvas.yview)
        self.scroll_fr = ttk.Frame(self.canvas, style="Card.TFrame")
        self.scroll_fr.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))
        self.canvas.create_window((0, 0), window=self.scroll_fr, anchor="nw")
        self.canvas.configure(yscrollcommand=scr.set)
        self.canvas.pack(side="left", fill="both", expand=True)
        scr.pack(side="right", fill="y")

    def _load_json(self, path, default):
        if os.path.exists(path):
            try:
                return json.load(open(path, 'r'))
            except:
                pass
        return default

    def _save_history(self):
        curr = self.folder_path.get()
        if curr in self.recent_folders: self.recent_folders.remove(curr)
        self.recent_folders.insert(0, curr)
        try:
            json.dump(self.recent_folders[:10], open(self.history_file, 'w'))
        except:
            pass

    def _log_threadsafe(self, msg, tag="info"):
        if self.parent is None:
            # Headless logging
            print(f"[{tag.upper()}] {msg}")
        else:
            # GUI logging
            def _update():
                self.log_box.insert(tk.END, f"[{datetime.now().strftime('%H:%M:%S')}] {msg}\n", tag)
                if self.auto_scroll.get(): self.log_box.see(tk.END)

            self.parent.after(0, _update)

    def _toggle_pause(self):
        self.is_paused = not self.is_paused
        if self.parent:
            self.btn_pause.config(text="RESUME" if self.is_paused else "PAUSE")

    def _browse_folder(self):
        d = filedialog.askdirectory()
        if d: self.folder_path.set(d); self._save_history(); self._scan_files()

    def _clear_queue(self):
        self.training_queue = []
        self.all_scanned_packets = []
        self.ram_cache = {}
        while not self.data_queue.empty(): self.data_queue.get()
        while not self.task_queue.empty(): self.task_queue.get()

        if self.parent:
            for w in self.scroll_fr.winfo_children(): w.destroy()

        self._log_threadsafe("Queue Cleared.", "warn")

    def _scan_files(self):
        folder = self.folder_path.get()
        if not os.path.exists(folder):
            self._log_threadsafe("Folder not found.", "error")
            return

        self._clear_queue()
        self._log_threadsafe(f"Scanning {folder}...", "info")

        ext_map = {
            'v': {'.png', '.jpg', '.jpeg', '.bmp'},
            'a': {'.mp3', '.wav', '.flac'},
            'c': {'.json', '.csv'},
            't': {'.txt', '.md', '.json', '.pdf', '.epub', '.mobi', '.rtf', '.doc', '.docx', '.srt', '.vtt', '.ass'},
            'vid': {'.mp4', '.mkv', '.avi'}
        }
        all_valid_exts = set().union(*ext_map.values())
        file_sets = {}
        ext_counts = {}

        for root, _, fs in os.walk(folder):
            for f in fs:
                base, ext = os.path.splitext(f)
                lext = ext.lower()
                if lext in all_valid_exts:
                    key = os.path.join(root, base).lower()
                    if key not in file_sets: file_sets[key] = {}

                    if lext in ext_map['v']:
                        file_sets[key]['v'] = os.path.join(root, f)
                    elif lext in ext_map['a']:
                        file_sets[key]['a'] = os.path.join(root, f)
                    elif lext in ext_map['c']:
                        file_sets[key]['c'] = os.path.join(root, f)
                    elif lext in ext_map['t']:
                        file_sets[key]['t'] = os.path.join(root, f)
                    elif lext in ext_map['vid']:
                        file_sets[key]['vid'] = os.path.join(root, f)

        q, tr, p, s = 0, 0, 0, 0
        sorted_keys = sorted(file_sets.keys())

        for key in sorted_keys:
            packet = file_sets[key].copy()

            # --- FIXED: EMPTY FILE DETECTION ---
            if 't' in packet:
                try:
                    if os.path.getsize(packet['t']) < 10:
                        del packet['t']
                except:
                    pass

            has_v, has_a, has_t, has_c = 'v' in packet, 'a' in packet, 't' in packet, 'c' in packet
            has_vid = 'vid' in packet

            # --- 1. TRIPLET / QUAD (Narrative Data) ---
            if (has_vid and has_t) or (has_v and has_a and has_t):
                packet['type'] = 'triplet'
                self.all_scanned_packets.append(packet)
                tr += 1
                continue

            if has_v and has_a and has_t and has_c:
                packet['type'] = 'quad'
                self.all_scanned_packets.append(packet)
                q += 1
                continue

            # --- 2. PAIR (Partial Narrative) ---
            if (has_v and has_t) or (has_a and has_t) or (has_v and has_a):
                packet['type'] = 'pair'
                self.all_scanned_packets.append(packet)
                p += 1
                continue

            # --- 3. SINGLES (Orphaned Data) ---
            if has_vid:
                if 'vid' in packet:
                    path = packet['vid']
                    _, e = os.path.splitext(path)
                    self.all_scanned_packets.append({'type': 'single', 'vid': path, 'ext': e.lower()})
                    ext_counts[e.lower()] = ext_counts.get(e.lower(), 0) + 1
                    s += 1
                continue

            for k, path in file_sets[key].items():
                _, e = os.path.splitext(path)
                lext = e.lower()
                self.all_scanned_packets.append({'type': 'single', k: path, 'ext': lext})
                ext_counts[lext] = ext_counts.get(lext, 0) + 1
                s += 1

        # GUI Update Logic
        if self.parent:
            row = 0

            def add_chk(text, var_key, container=self.train_type_vars):
                var = tk.BooleanVar(value=True)
                container[var_key] = var
                ttk.Checkbutton(self.scroll_fr, text=text, variable=var).grid(row=row, column=0, sticky="w")

            if q > 0: add_chk(f"Quadruplets ({q})", 'quad'); row += 1
            if tr > 0: add_chk(f"Triplets ({tr})", 'triplet'); row += 1
            if p > 0: add_chk(f"Pairs ({p})", 'pair'); row += 1
            if row > 0: ttk.Separator(self.scroll_fr, orient='horizontal').grid(row=row, column=0, sticky="ew",
                                                                                pady=5); row += 1

            for ext in sorted(ext_counts.keys()):
                add_chk(f"{ext} ({ext_counts[ext]})", ext, self.train_ext_vars);
                row += 1

            self.scroll_fr.update_idletasks()
            self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        else:
            # Headless: Enable all found types
            self.train_type_vars['quad'] = MockVar(True)
            self.train_type_vars['triplet'] = MockVar(True)
            self.train_type_vars['pair'] = MockVar(True)
            for ext in ext_counts.keys():
                self.train_ext_vars[ext] = MockVar(True)

        self._log_threadsafe(f"Scan: {s} Single, {p} Pair, {tr} Trip, {q} Quad.", "success")
        if s + p + tr + q > 0: self._save_history()

    def _start_training(self):
        if self.is_training:
            self.stop_requested = True
            return

        active_id = self.app.active_lobe.get()
        brain = self.app.lobes[active_id]

        if brain is None:
            if self.parent:
                messagebox.showerror("Error", "No Brain Loaded.\nPlease load 'MaskedDiffusion-mHC'.")
            else:
                print("Error: No Brain Loaded.")
            return

        # --- SAFETY CHECK: MODEL TYPE ---
        lobe_type = self.app.lobe_types.get(active_id)
        if lobe_type != "diffusion":
            if self.parent:
                messagebox.showwarning("Mismatch",
                                       "Diffusion Trainer requires a Diffusion lobe.\nThis appears to be a standard Transformer.")
            else:
                print("Error: Model type mismatch. Requires diffusion.")
            return

        if not hasattr(brain, 'timestep_emb'):
            if self.parent:
                messagebox.showerror("Error", "Wrong Architecture.\nActive Lobe is not a Diffusion model.")
            else:
                print("Error: Architecture mismatch.")
            return

        self.training_queue = []

        # Handle MockVar vs BooleanVar
        # Helper to safely get value
        def safe_get_bool(v):
            return v.get() if v else False

        active_exts = {e for e, v in self.train_ext_vars.items() if safe_get_bool(v)}
        t_quad = safe_get_bool(self.train_type_vars.get('quad'))
        t_trip = safe_get_bool(self.train_type_vars.get('triplet'))
        t_pair = safe_get_bool(self.train_type_vars.get('pair'))

        for p in self.all_scanned_packets:
            pt = p['type']
            if pt == 'quad' and t_quad:
                self.training_queue.append(p)
            elif pt == 'triplet' and t_trip:
                self.training_queue.append(p)
            elif pt == 'pair' and t_pair:
                self.training_queue.append(p)
            elif pt == 'single' and p.get('ext') in active_exts:
                self.training_queue.append(p)

        if not self.training_queue:
            if self.parent:
                messagebox.showinfo("Info", "Queue is empty. Check your filters in the Census panel.")
            else:
                print("Queue is empty.")
            return

        if self.narrative_mode.get():
            self._log_threadsafe("Narrative Mode: Enforcing strict filename sort.", "info")
            self.training_queue.sort(key=lambda x: x.get('t', x.get('vid', x.get('v', x.get('a', '')))))

        self.total_items = len(self.training_queue) * self.target_epochs.get()
        self.is_training = True
        self.stop_requested = False

        if self.parent:
            self.btn_start.config(text="STOP")
            self.btn_pause.config(state="normal")

        while not self.data_queue.empty(): self.data_queue.get()
        while not self.task_queue.empty(): self.task_queue.get()

        threading.Thread(target=self._manager_worker, daemon=True).start()

        n_workers = self.num_workers.get()
        for i in range(n_workers):
            threading.Thread(target=self._loader_thread, args=(i,), daemon=True).start()

        threading.Thread(target=self._diffusion_worker, daemon=True).start()

    def _manager_worker(self):
        epochs = self.target_epochs.get()
        for _ in range(epochs):
            if self.narrative_mode.get():
                order = list(range(len(self.training_queue)))
            else:
                order = random.sample(range(len(self.training_queue)), len(self.training_queue))

            for idx in order:
                if self.stop_requested: break
                self.task_queue.put(self.training_queue[idx])

        for _ in range(self.num_workers.get() * 2): self.task_queue.put(None)

    def _loader_thread(self, worker_id):
        while not self.stop_requested:
            try:
                packet = self.task_queue.get(timeout=1)
            except queue.Empty:
                continue

            if packet is None: return

            pkey = packet.get('t', packet.get('vid', packet.get('v', packet.get('a'))))
            if pkey in self.ram_cache:
                self.data_queue.put((packet, self.ram_cache[pkey]))
                time.sleep(0.001)
                continue

            data = None
            if packet['type'] == 'single' and 't' in packet:
                path = packet['t']
                _, ext = os.path.splitext(path)
                if ext.lower() in ['.pdf', '.epub', '.mobi'] and HAS_FITZ:
                    try:
                        doc = fitz.open(path)
                        text = ""
                        for page in doc: text += page.get_text()

                        ribo = self.app.ribosome
                        toks = ribo._tokenize(text)

                        t_tensor = torch.tensor(toks).unsqueeze(0).to(self.app.device)
                        v = torch.zeros(1, 1, 768).to(self.app.device)
                        a = torch.zeros(1, 1, 128).to(self.app.device)
                        c = torch.zeros(1, 1, 64).to(self.app.device)
                        data = (v, a, t_tensor, c, None)
                    except:
                        pass

            if data is None:
                try:
                    data = self.app.ribosome.ingest_packet(packet)
                except:
                    continue

            if data:
                if len(self.ram_cache) < self.MAX_CACHE_SIZE:
                    self.ram_cache[pkey] = data
                self.data_queue.put((packet, data))

            time.sleep(0.005)

    def _diffusion_worker(self):
        try:
            active_id = self.app.active_lobe.get()
            brain = self.app.lobes[active_id]
            opt = self.app.optimizers[active_id]
            scaler = self.app.scalers[active_id]
            brain.train()

            while True:
                if self.stop_requested: break
                item = self.data_queue.get()
                packet, (_, _, t, _, _) = item
                if t is None or t.size(1) < 2: continue

                while self.is_paused: time.sleep(0.1)

                mask_ratio = None
                if self.use_uniform.get():
                    mask_ratio = random.uniform(self.uniform_ratio_min.get(), self.uniform_ratio_max.get())

                brain.config.use_cross_modal_masking = self.use_cross_modal.get()
                brain.config.cross_modal_prob = self.cross_modal_prob.get()
                brain.config.use_modality_masking = self.use_modality_specific.get()

                timestep = torch.randint(0, 1000, (t.shape[0],), device=self.app.device)

                with self.app.gpu_lock:
                    opt.zero_grad()
                    with autocast('cuda'):
                        logits, mask_bool, _ = brain(None, None, t, None, timestep=timestep, mask_ratio=mask_ratio)

                        if mask_bool.ndim == 1:
                            mask_bool = mask_bool.unsqueeze(0).expand(logits.shape[0], -1)

                        flat_logits = logits.reshape(-1, logits.shape[-1])
                        flat_targets = t.reshape(-1)
                        flat_mask = mask_bool.reshape(-1)

                        pred = flat_logits[flat_mask]
                        target = flat_targets[flat_mask]

                        loss_recon = F.cross_entropy(pred, target)

                        game_penalty = torch.tensor(0.0, device=self.app.device)
                        has_game = False
                        for module in brain.modules():
                            if hasattr(module, 'game_loss'):
                                game_penalty += module.game_loss()
                                has_game = True

                        raw_recon = loss_recon.item()
                        raw_game = game_penalty.item() if has_game else 0.0

                        if math.isnan(raw_recon) or math.isinf(raw_recon):
                            self._log_threadsafe("NaN detected. Skipping.", "warn")
                            continue

                        self.loss_history['recon'].append(raw_recon)
                        if has_game: self.loss_history['game'].append(raw_game)

                        if self.nursery_autofit.get() and self.processed_count % 100 == 0:
                            if len(self.loss_history['recon']) > 50:
                                max_r = np.percentile(self.loss_history['recon'], 99)
                                cur_max = self.nurse_recon[1].get()
                                new_max = cur_max * 0.9 + max_r * 2.0 * 0.1
                                self.nurse_recon[1].set(round(new_max, 4))

                        def clamp(val_tensor, setting):
                            if not setting[2].get(): return val_tensor
                            limit = torch.tensor(setting[1].get(), device=self.app.device)
                            return torch.minimum(val_tensor, limit)

                        clamped_recon = clamp(loss_recon, self.nurse_recon)
                        clamped_game = clamp(game_penalty, self.nurse_game)
                        loss = clamped_recon + clamped_game

                    scaler.scale(loss).backward()
                    scaler.unscale_(opt)
                    torch.nn.utils.clip_grad_norm_(brain.parameters(), 1.0)
                    scaler.step(opt)
                    scaler.update()

                self.processed_count += 1

                q_len = len(self.training_queue)
                current_epoch = (self.processed_count // q_len) + 1 if q_len > 0 else 1

                if current_epoch not in self.app.graph_data:
                    self.app.graph_data[current_epoch] = {
                        'total': [], 'text': [], 'vis': [], 'aud': [],
                        'raw_total': [], 'raw_text': [], 'raw_vis': [], 'raw_aud': []
                    }

                val_recon = clamped_recon.item() if isinstance(clamped_recon, torch.Tensor) else clamped_recon
                val_game = clamped_game.item() if isinstance(clamped_game, torch.Tensor) else clamped_game

                self.app.graph_data[current_epoch]['total'].append(val_recon + val_game)
                self.app.graph_data[current_epoch]['text'].append(val_recon)
                self.app.graph_data[current_epoch]['vis'].append(val_game)
                self.app.graph_data[current_epoch]['aud'].append(0)

                self.app.graph_data[current_epoch]['raw_total'].append(raw_recon + raw_game)
                self.app.graph_data[current_epoch]['raw_text'].append(raw_recon)
                self.app.graph_data[current_epoch]['raw_vis'].append(raw_game)
                self.app.graph_data[current_epoch]['raw_aud'].append(0)

                if self.processed_count % 10 == 0:
                    pct = int((self.processed_count / self.total_items) * 100) if self.total_items else 0
                    game_str = f" | Game:{raw_game:.2f}" if has_game else ""
                    name = "Unknown"
                    if 't' in packet:
                        name = os.path.basename(packet['t'])
                    elif 'vid' in packet:
                        name = os.path.basename(packet['vid'])

                    self._log_threadsafe(f"[{self.processed_count}] ({pct}%) {name} | Recon:{raw_recon:.3f}{game_str}",
                                         "prog")

                if self.autosave_enabled.get() and self.processed_count % self.autosave_interval.get() == 0:
                    path = os.path.join(self.app.paths['lobes'], f"brain_lobe_{active_id}.pt")
                    try:
                        torch.save({
                            "genome": self.app.lobe_genomes.get(active_id, "Unknown"),
                            "model_type": "diffusion",  # Explicitly save type
                            "state_dict": brain.state_dict()
                        }, path)
                        self._log_threadsafe(f"AUTO-SAVE at {self.processed_count}", "save")
                    except Exception as e:
                        print(f"Save failed: {e}")

            self._log_threadsafe("Training Complete.", "info")

        except Exception as e:
            self._log_threadsafe(f"CRASH: {e}", "error")
            import traceback
            traceback.print_exc()
        finally:
            self.is_training = False
            if self.parent:
                self.btn_start.config(text="START DIFFUSION TRAINING")
                self.btn_pause.config(state="disabled")

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'log_box'): self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])

--- FILE: Plugins\tab_dream.py ---
import tkinter as tk
from tkinter import ttk, messagebox, filedialog
import threading
import os
import time
import torch
import torch.nn.functional as F
from datetime import datetime
import traceback


# --- HEADLESS HELPER ---
class MockVar:
    def __init__(self, value=None): self._val = value

    def set(self, value): self._val = value

    def get(self): return self._val


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Dream State"
        self.is_dreaming = False
        self.stop_requested = False

        # --- DYNAMIC PATHS ---
        # Get configured data dir or default to local
        default_data = self.app.paths.get("data", os.path.join(self.app.paths["root"], "Training_Data"))

        # Chaos Buffer (Where dreams go to be sorted by Factory)
        self.chaos_dir = os.path.join(default_data, "Chaos_Buffer")
        if not os.path.exists(self.chaos_dir):
            try:
                os.makedirs(self.chaos_dir)
            except:
                pass

        # --- STATE ---
        if self.parent is None:
            # Headless Defaults
            self.temperature = MockVar(0.85)
            self.top_k = MockVar(50)
            self.max_length = MockVar(1024)
            self.refresh_rate = MockVar(1.0)
            self.autosave = MockVar(True)
            self.seed_prompt = MockVar("The nature of consciousness is")
            self.diffusion_steps = MockVar(20)
        else:
            # GUI Defaults
            self.temperature = tk.DoubleVar(value=0.85)
            self.top_k = tk.IntVar(value=50)
            self.max_length = tk.IntVar(value=1024)
            self.refresh_rate = tk.DoubleVar(value=0.5)
            self.autosave = tk.BooleanVar(value=True)
            self.seed_prompt = tk.StringVar(value="The nature of consciousness is")
            self.diffusion_steps = tk.IntVar(value=20)

        self._setup_ui()

    def _setup_ui(self):
        if self.parent is None: return

        # Layout
        panel = ttk.PanedWindow(self.parent, orient="horizontal")
        panel.pack(fill="both", expand=True, padx=10, pady=10)

        left = ttk.Frame(panel, width=300)
        right = ttk.Frame(panel)
        panel.add(left, weight=1)
        panel.add(right, weight=3)

        # --- CONTROLS ---
        fr_cfg = ttk.LabelFrame(left, text="Dream Parameters", padding=10)
        fr_cfg.pack(fill="x", pady=5)

        ttk.Label(fr_cfg, text="Seed Prompt:").pack(anchor="w")
        ttk.Entry(fr_cfg, textvariable=self.seed_prompt).pack(fill="x", pady=(0, 10))

        def add_scale(lbl, var, min_v, max_v):
            ttk.Label(fr_cfg, text=lbl).pack(anchor="w")
            ttk.Scale(fr_cfg, from_=min_v, to=max_v, variable=var, orient="horizontal").pack(fill="x", pady=(0, 10))

        add_scale("Temperature (Creativity):", self.temperature, 0.1, 2.0)
        add_scale("Top-K (Diversity):", self.top_k, 1, 200)
        add_scale("Max Length:", self.max_length, 64, 4096)

        # Diffusion Specific
        self.fr_diff = ttk.Frame(fr_cfg)
        self.fr_diff.pack(fill="x")
        ttk.Label(self.fr_diff, text="Diffusion Steps:", foreground=self.app.colors["ACCENT"]).pack(anchor="w")
        ttk.Scale(self.fr_diff, from_=1, to=100, variable=self.diffusion_steps, orient="horizontal").pack(fill="x")

        ttk.Checkbutton(fr_cfg, text="Autosave to Chaos", variable=self.autosave).pack(anchor="w", pady=5)

        self.btn_start = ttk.Button(left, text="INITIATE DREAM STATE", command=self._toggle_dream)
        self.btn_start.pack(fill="x", pady=20)

        # --- VISUALIZATION ---
        fr_vis = ttk.LabelFrame(right, text="The Mind's Eye", padding=10)
        fr_vis.pack(fill="both", expand=True, pady=5)

        self.txt_out = tk.Text(fr_vis, font=("Consolas", 11), bg=self.app.colors["BG_MAIN"],
                               fg=self.app.colors["FG_TEXT"], wrap="word", padx=15, pady=15, borderwidth=0)
        self.txt_out.pack(fill="both", expand=True)

        # Tags
        self.txt_out.tag_config("prompt", foreground="#888")
        self.txt_out.tag_config("new", foreground=self.app.colors["ACCENT"])
        self.txt_out.tag_config("diff", foreground=self.app.colors["SUCCESS"])

    def _log(self, text, tag="new"):
        if self.parent:
            self.txt_out.insert(tk.END, text, tag)
            self.txt_out.see(tk.END)
        else:
            print(text, end="", flush=True)

    def _toggle_dream(self):
        if self.is_dreaming:
            self.stop_requested = True
            if self.parent: self.btn_start.config(text="Stopping...")
        else:
            lobe_id = self.app.active_lobe.get()
            if self.app.lobes[lobe_id] is None:
                msg = "No Lobe Loaded."
                if self.parent:
                    messagebox.showerror("Error", msg)
                else:
                    print(msg)
                return

            self.is_dreaming = True
            self.stop_requested = False
            if self.parent:
                self.btn_start.config(text="WAKE UP")
                self.txt_out.delete("1.0", tk.END)

            threading.Thread(target=self._dream_loop, daemon=True).start()

    def _dream_loop(self):
        try:
            lobe_id = self.app.active_lobe.get()
            brain = self.app.lobes[lobe_id]
            ribosome = self.app.ribosome

            # Detect Type
            is_diffusion = hasattr(brain, 'timestep_emb') or (self.app.lobe_types.get(lobe_id) == "diffusion")

            if self.parent and is_diffusion:
                self.fr_diff.pack(fill="x")
            elif self.parent:
                self.fr_diff.pack_forget()

            while not self.stop_requested:
                prompt = self.seed_prompt.get()
                self._log(f"\n\n>>> SEED: {prompt}\n", "prompt")

                # Tokenize
                ids = ribosome._tokenize(prompt)

                generated_text = ""

                with self.app.gpu_lock:
                    brain.eval()
                    with torch.no_grad():
                        # --- DIFFUSION GENERATION ---
                        if is_diffusion:
                            steps = self.diffusion_steps.get()
                            # Generate
                            tokens = brain.generate(
                                prompt_tokens=ids,
                                max_length=self.max_length.get(),
                                steps=steps,
                                temperature=self.temperature.get()
                            )
                            generated_text = ribosome.decode(tokens)
                            # Remove prompt echo
                            if generated_text.startswith(prompt):
                                generated_text = generated_text[len(prompt):]

                            self._log(generated_text, "diff")

                        # --- AR GENERATION ---
                        else:
                            t = torch.tensor(ids, device=self.app.device).unsqueeze(0)
                            # Placeholder sensors
                            v = torch.zeros(1, 1, 768).to(self.app.device)
                            a = torch.zeros(1, 1, 128).to(self.app.device)

                            for _ in range(self.max_length.get()):
                                if self.stop_requested: break

                                logits, _, _ = brain(v, a, t)
                                next_logits = logits[:, -1, :] / self.temperature.get()
                                probs = F.softmax(next_logits, dim=-1)

                                # Top-K
                                k = self.top_k.get()
                                if k > 0:
                                    v_top, _ = torch.topk(probs, k)
                                    probs[probs < v_top[:, [-1]]] = 0
                                    probs = probs / probs.sum(dim=-1, keepdim=True)

                                next_tok = torch.multinomial(probs, 1)
                                t = torch.cat([t, next_tok], dim=1)

                                # Stream Decode
                                word = ribosome.decode([next_tok.item()])
                                self._log(word, "new")
                                generated_text += word

                                if next_tok.item() == 50256: break  # EOS
                                time.sleep(0.01)  # UI Breather

                # --- AUTOSAVE TO CHAOS ---
                if self.autosave.get() and generated_text.strip():
                    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                    fname = f"dream_{ts}.txt"
                    path = os.path.join(self.chaos_dir, fname)

                    try:
                        with open(path, "w", encoding="utf-8") as f:
                            f.write(f"PROMPT: {prompt}\n\n")
                            f.write(generated_text)
                        if self.parent:
                            self._log(f"\n[Saved to Chaos Buffer: {fname}]", "prompt")
                        else:
                            print(f"\n[Saved: {fname}]")
                    except Exception as e:
                        print(f"Save Error: {e}")

                if self.stop_requested: break
                time.sleep(self.refresh_rate.get())

        except Exception as e:
            if self.parent:
                self._log(f"\nCRASH: {e}", "error")
            else:
                print(f"CRASH: {e}")
            traceback.print_exc()
        finally:
            self.is_dreaming = False
            if self.parent: self.btn_start.config(text="INITIATE DREAM STATE")

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'txt_out'):
            self.txt_out.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])

--- FILE: Plugins\tab_factory.py ---
import tkinter as tk
from tkinter import ttk, messagebox, filedialog
import os
import threading
import shutil
import time
import queue
import traceback


# --- HEADLESS HELPER ---
class MockVar:
    def __init__(self, value=None): self._val = value

    def set(self, value): self._val = value

    def get(self): return self._val


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Data Factory"
        self.running = False

        # --- DYNAMIC PATHS ---
        # Get default from GUI.py logic (configurable via settings.json)
        default_data = self.app.paths.get("data", os.path.join(self.app.paths["root"], "Training_Data"))

        # Default Source: "Chaos_Buffer" inside Training_Data
        chaos_default = os.path.join(default_data, "Chaos_Buffer")
        if not os.path.exists(chaos_default):
            try:
                os.makedirs(chaos_default)
            except:
                pass

        # --- INITIALIZE VARIABLES ---
        if self.parent is None:
            # Headless Mode
            self.chaos_folder = MockVar(chaos_default)
            self.target_folder = MockVar(default_data)
            self.status_var = MockVar("Ready.")
            self.progress_var = MockVar(0.0)
        else:
            # GUI Mode
            self.chaos_folder = tk.StringVar(value=chaos_default)
            self.target_folder = tk.StringVar(value=default_data)
            self.status_var = tk.StringVar(value="Ready.")
            self.progress_var = tk.DoubleVar(value=0.0)

        self._setup_ui()

    def _setup_ui(self):
        if self.parent is None: return

        # 1. Source (Chaos)
        fr_src = ttk.LabelFrame(self.parent, text="Incoming Data (Source)", padding=15)
        fr_src.pack(fill="x", padx=10, pady=10)

        ttk.Label(fr_src, text="Source Folder (The Chaos):").pack(anchor="w")
        row_src = ttk.Frame(fr_src)
        row_src.pack(fill="x", pady=5)

        e_chaos = ttk.Entry(row_src, textvariable=self.chaos_folder)
        e_chaos.pack(side="left", fill="x", expand=True, padx=(0, 5))
        ttk.Button(row_src, text="📂", width=4, command=lambda: self._browse(self.chaos_folder)).pack(side="left")

        # 2. Target (Organized)
        fr_tgt = ttk.LabelFrame(self.parent, text="Structured Storage (Target)", padding=15)
        fr_tgt.pack(fill="x", padx=10, pady=10)

        ttk.Label(fr_tgt, text="Destination Folder:").pack(anchor="w")
        row_tgt = ttk.Frame(fr_tgt)
        row_tgt.pack(fill="x", pady=5)

        e_tgt = ttk.Entry(row_tgt, textvariable=self.target_folder)
        e_tgt.pack(side="left", fill="x", expand=True, padx=(0, 5))
        ttk.Button(row_tgt, text="📂", width=4, command=lambda: self._browse(self.target_folder)).pack(side="left")

        # 3. Operations
        fr_ops = ttk.LabelFrame(self.parent, text="Factory Operations", padding=15)
        fr_ops.pack(fill="both", expand=True, padx=10, pady=10)

        btn_grid = ttk.Frame(fr_ops)
        btn_grid.pack(fill="x", pady=10)

        self.btn_sort = ttk.Button(btn_grid, text="✨ SORT & CLEAN", command=self._start_sort)
        self.btn_sort.pack(side="left", fill="x", expand=True, padx=5)

        self.btn_flat = ttk.Button(btn_grid, text="🔨 FLATTEN DIRECTORY", command=self._start_flatten)
        self.btn_flat.pack(side="left", fill="x", expand=True, padx=5)

        # Status & Progress
        self.pb = ttk.Progressbar(fr_ops, variable=self.progress_var, maximum=100)
        self.pb.pack(fill="x", pady=(20, 5))

        lbl_stat = ttk.Label(fr_ops, textvariable=self.status_var, foreground=self.app.colors["ACCENT"],
                             font=("Segoe UI", 10))
        lbl_stat.pack()

    def _browse(self, var):
        d = filedialog.askdirectory(initialdir=var.get())
        if d: var.set(d)

    def _toggle_ui(self, state):
        if self.parent is None: return
        s = "normal" if state else "disabled"
        self.btn_sort.config(state=s)
        self.btn_flat.config(state=s)

    # --- SORT LOGIC ---
    def _start_sort(self):
        if self.running: return

        src = self.chaos_folder.get()
        tgt = self.target_folder.get()

        if not os.path.exists(src):
            if self.parent:
                messagebox.showerror("Error", "Source folder does not exist.")
            else:
                print("Error: Source folder missing.")
            return

        self.running = True
        self._toggle_ui(False)
        self.status_var.set("Initializing Sorter...")
        self.progress_var.set(0)

        threading.Thread(target=self._worker_sort, args=(src, tgt), daemon=True).start()

    def _worker_sort(self, src, tgt):
        try:
            if not os.path.exists(tgt): os.makedirs(tgt)

            # Definitions
            categories = {
                'Images': ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp', '.tiff'],
                'Audio': ['.mp3', '.wav', '.flac', '.ogg', '.m4a', '.aac'],
                'Text': ['.txt', '.md', '.pdf', '.epub', '.json', '.csv', '.doc', '.docx', '.srt', '.vtt', '.ass'],
                'Video': ['.mp4', '.mkv', '.avi', '.mov', '.webm'],
                'Code': ['.py', '.js', '.html', '.css', '.cpp', '.h', '.java'],
                'Archives': ['.zip', '.rar', '.7z', '.tar', '.gz']
            }

            # Create subfolders
            for cat in categories:
                p = os.path.join(tgt, cat)
                if not os.path.exists(p): os.makedirs(p)

            # Scan
            files_to_move = []
            for root, _, files in os.walk(src):
                for f in files:
                    files_to_move.append(os.path.join(root, f))

            total = len(files_to_move)
            if total == 0:
                if self.parent:
                    self.parent.after(0, lambda: self.status_var.set("Source is empty."))
                    self.parent.after(0, lambda: self._toggle_ui(True))
                else:
                    print("Source is empty.")
                self.running = False
                return

            moved_count = 0

            for i, file_path in enumerate(files_to_move):
                _, ext = os.path.splitext(file_path)
                lext = ext.lower()

                dest_cat = "Misc"
                for cat, exts in categories.items():
                    if lext in exts:
                        dest_cat = cat
                        break

                # Setup destination
                dest_dir = os.path.join(tgt, dest_cat)
                if not os.path.exists(dest_dir): os.makedirs(dest_dir)

                fname = os.path.basename(file_path)
                dest_path = os.path.join(dest_dir, fname)

                # Handle collision
                if os.path.exists(dest_path):
                    base, ex = os.path.splitext(fname)
                    ts = int(time.time() * 1000)
                    dest_path = os.path.join(dest_dir, f"{base}_{ts}{ex}")

                try:
                    shutil.move(file_path, dest_path)
                    moved_count += 1
                except Exception as e:
                    print(f"Move failed for {fname}: {e}")

                # UI Update
                if self.parent and i % 5 == 0:
                    pct = (i / total) * 100
                    self.parent.after(0, lambda p=pct, m=moved_count: [
                        self.progress_var.set(p),
                        self.status_var.set(f"Sorting... ({m}/{total})")
                    ])

            # Cleanup Empty Dirs in Source
            for root, dirs, files in os.walk(src, topdown=False):
                for d in dirs:
                    try:
                        os.rmdir(os.path.join(root, d))
                    except:
                        pass

            if self.parent:
                self.parent.after(0, lambda m=moved_count: self.status_var.set(f"Done! Organized {m} files."))
                self.parent.after(0, lambda: self.progress_var.set(100))
            else:
                print(f"Done! Organized {moved_count} files.")

        except Exception as e:
            traceback.print_exc()
            if self.parent:
                self.parent.after(0, lambda err=str(e): self.status_var.set(f"Error: {err}"))
        finally:
            self.running = False
            if self.parent:
                self.parent.after(0, lambda: self._toggle_ui(True))

    # --- FLATTEN LOGIC ---
    def _start_flatten(self):
        if self.running: return

        tgt = self.target_folder.get()
        if not os.path.exists(tgt): return

        if self.parent:
            if not messagebox.askyesno("Confirm Flatten",
                                       f"This will move ALL files from subfolders of:\n{tgt}\n\nInto the root of that folder.\nAre you sure?"):
                return

        self.running = True
        self._toggle_ui(False)
        self.status_var.set("Flattening...")
        self.progress_var.set(0)

        threading.Thread(target=self._worker_flatten, args=(tgt,), daemon=True).start()

    def _worker_flatten(self, root_dir):
        try:
            files_to_move = []
            # Walk topdown=False so we can delete dirs after
            for root, dirs, files in os.walk(root_dir, topdown=False):
                if root == root_dir: continue  # Skip root files, they are fine

                for f in files:
                    files_to_move.append(os.path.join(root, f))

            total = len(files_to_move)
            moved = 0

            for i, src_path in enumerate(files_to_move):
                fname = os.path.basename(src_path)
                dest_path = os.path.join(root_dir, fname)

                # Collision handling
                if os.path.exists(dest_path):
                    base, ext = os.path.splitext(fname)
                    # Use parent folder name to disambiguate
                    parent_name = os.path.basename(os.path.dirname(src_path))
                    dest_path = os.path.join(root_dir, f"{base}_{parent_name}{ext}")

                    # Double check
                    if os.path.exists(dest_path):
                        ts = int(time.time())
                        dest_path = os.path.join(root_dir, f"{base}_{ts}{ext}")

                try:
                    shutil.move(src_path, dest_path)
                    moved += 1
                except:
                    pass

                if self.parent and i % 10 == 0:
                    pct = (i / total) * 100
                    self.parent.after(0, lambda p=pct: self.progress_var.set(p))

            # Cleanup Empty Dirs
            cleaned = 0
            for root, dirs, files in os.walk(root_dir, topdown=False):
                if root == root_dir: continue
                try:
                    os.rmdir(root)
                    cleaned += 1
                except:
                    pass

            if self.parent:
                self.parent.after(0, lambda m=moved, c=cleaned: self.status_var.set(
                    f"Flattened {m} files. Removed {c} folders."))
                self.parent.after(0, lambda: self.progress_var.set(100))
            else:
                print(f"Flattened {moved} files.")

        except Exception as e:
            if self.parent:
                self.parent.after(0, lambda err=str(e): self.status_var.set(f"Error: {err}"))
        finally:
            self.running = False
            if self.parent:
                self.parent.after(0, lambda: self._toggle_ui(True))

    def on_theme_change(self):
        pass

--- FILE: Plugins\tab_graphs.py ---
# FILE: Plugins/tab_graphs.py
import tkinter as tk
from tkinter import ttk
import math
import numpy as np


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Telemetry"

        # Settings
        self.smoothing = tk.IntVar(value=20)
        self.show_trend = tk.BooleanVar(value=True)
        self.show_power_law = tk.BooleanVar(value=True)
        self.auto_refresh = tk.BooleanVar(value=True)

        self._setup_ui()
        self.parent.after(2000, self._animate)

    def _setup_ui(self):
        # Top Bar
        bar = ttk.Frame(self.parent, padding=5)
        bar.pack(fill="x")

        ttk.Label(bar, text="Smoothing:").pack(side="left")
        scale = ttk.Scale(bar, from_=1, to=200, variable=self.smoothing, orient="horizontal", length=200,
                          command=lambda v: self._update_graphs())
        scale.pack(side="left", padx=5)

        ttk.Checkbutton(bar, text="Auto-Refresh", variable=self.auto_refresh).pack(side="left", padx=10)
        ttk.Checkbutton(bar, text="Power Law Projection", variable=self.show_power_law,
                        command=self._update_graphs).pack(side="left", padx=10)

        ttk.Button(bar, text="Force Refresh", command=self._update_graphs).pack(side="right")

        # Main Graph (Step Loss - Reconstruction + Game)
        self.canv_main = tk.Canvas(self.parent, bg="#1E1E1E", height=300, highlightthickness=0)
        self.canv_main.pack(fill="both", expand=True, padx=10, pady=(0, 5))

        # Aux Graph (Epoch Reconstruction Avg + Power Law)
        self.canv_aux = tk.Canvas(self.parent, bg="#252526", height=200, highlightthickness=0)
        self.canv_aux.pack(fill="x", padx=10, pady=(0, 10))

    def _animate(self):
        if self.auto_refresh.get():
            try:
                current_tab_id = self.app.notebook.select()
                if current_tab_id:
                    current_tab_text = self.app.notebook.tab(current_tab_id, "text")
                    if current_tab_text == self.name:
                        self._update_graphs()
            except:
                pass
        self.parent.after(2000, self._animate)

    def _get_coords(self, data, width, height, min_val, max_val, offset_x=40):
        if len(data) == 0: return []
        count = len(data)
        if count < 2: return []

        x_step = (width - offset_x - 10) / (count - 1)
        y_range = max_val - min_val if max_val > min_val else 1.0
        # Drawing area height: total height - 40 (20 top padding, 20 bottom)
        draw_h = height - 40

        coords = []
        for i, val in enumerate(data):
            x = offset_x + i * x_step
            # Normalize Y (0 at bottom)
            norm_y = (val - min_val) / y_range
            # Invert for Canvas (0 at top)
            y = height - 20 - (norm_y * draw_h)
            coords.append(x)
            coords.append(y)
        return coords

    def _moving_average(self, data, window_size):
        if not data: return []
        data_arr = np.array(data)
        effective_window = min(int(window_size), len(data_arr))
        if effective_window < 2: return data_arr.tolist()
        window = np.ones(effective_window) / float(effective_window)
        smoothed = np.convolve(data_arr, window, 'valid')
        # Pad start to keep length consistent
        pad = len(data_arr) - len(smoothed)
        return np.pad(smoothed, (pad, 0), mode='edge').tolist()

    def _fit_power_law(self, epochs, losses):
        try:
            x = np.array(epochs)
            y = np.array(losses)
            mask = (x > 0) & (y > 0)
            if np.sum(mask) < 5: return None, None
            x_log = np.log(x[mask])
            y_log = np.log(y[mask])
            b, ln_a = np.polyfit(x_log, y_log, 1)
            a = np.exp(ln_a)

            def predict(ep):
                return a * (ep ** b)

            return (a, b), predict
        except:
            return None, None

    def _draw_axes_and_grid(self, canvas, width, height, min_val, max_val, h_lines=6, offset_x=40):
        # Clear previous text/lines
        canvas.delete("grid")

        y_range = max_val - min_val if max_val > min_val else 1.0
        draw_h = height - 40

        # Horizontal Grid + Labels
        for i in range(h_lines + 1):
            ratio = i / h_lines
            # Calculate value for this line
            val = min_val + (ratio * y_range)
            # Calculate Y position
            y = height - 20 - (ratio * draw_h)

            # Line
            canvas.create_line(offset_x, y, width, y, fill="#333333", dash=(2, 2), tags="grid")

            # Text Label
            canvas.create_text(offset_x - 5, y, text=f"{val:.2f}", fill="#888", anchor="e", font=("Consolas", 8),
                               tags="grid")

        # Vertical Line (Axis)
        canvas.create_line(offset_x, 20, offset_x, height - 20, fill="#666", width=1, tags="grid")
        # Horizontal Line (Axis)
        canvas.create_line(offset_x, height - 20, width, height - 20, fill="#666", width=1, tags="grid")

    def _update_graphs(self):
        # Collect Data
        raw_recon = []
        raw_game = []
        epoch_recon_avgs = []
        epoch_indices = []

        sorted_epochs = sorted(self.app.graph_data.keys())
        for ep in sorted_epochs:
            ep_data = self.app.graph_data[ep]
            r_data = ep_data.get('text', [])  # Smoothed/Scaled steps
            g_data = ep_data.get('vis', [])  # Smoothed/Scaled steps

            raw_recon.extend(r_data)
            raw_game.extend(g_data)

            # Epoch reconstruction average (Using RAW text loss, not scaled)
            recon_totals = ep_data.get('raw_text', [])
            if recon_totals:
                avg = sum(recon_totals) / len(recon_totals)
                epoch_recon_avgs.append(avg)
                epoch_indices.append(ep)

        if not raw_recon: return

        # === MAIN GRAPH (Step Loss) ===
        w = self.canv_main.winfo_width()
        h = self.canv_main.winfo_height()
        if w > 10 and h > 10:
            self.canv_main.delete("all")

            smooth = self.smoothing.get()
            plot_recon = self._moving_average(raw_recon, smooth)
            plot_game = self._moving_average(raw_game, smooth)

            all_vals = np.array(plot_recon + plot_game)
            min_y = max(0, np.min(all_vals) * 0.9)
            max_y = np.max(all_vals) * 1.1

            # Grid & Labels
            self._draw_axes_and_grid(self.canv_main, w, h, min_y, max_y, h_lines=8)

            # Lines
            coords_r = self._get_coords(plot_recon, w, h, min_y, max_y)
            if len(coords_r) > 2:
                self.canv_main.create_line(coords_r, fill="#FF5555", width=2.5, tags="line")

            coords_g = self._get_coords(plot_game, w, h, min_y, max_y)
            if len(coords_g) > 2:
                self.canv_main.create_line(coords_g, fill="#5555FF", width=2, tags="line")

            # Legend
            self.canv_main.create_text(w - 10, 10, text="Reconstruction (Red)\nGame Penalty (Blue)", fill="#AAA",
                                       anchor="ne", font=("Segoe UI", 9, "bold"))

        # === AUX GRAPH (Epoch Reconstruction Only + Power Law) ===
        w_aux = self.canv_aux.winfo_width()
        h_aux = self.canv_aux.winfo_height()
        if w_aux > 10 and h_aux > 10:
            self.canv_aux.delete("all")

            if epoch_recon_avgs:
                # Scale
                min_e = min(epoch_recon_avgs) * 0.9
                max_e = max(epoch_recon_avgs) * 1.1

                # Grid & Labels
                self._draw_axes_and_grid(self.canv_aux, w_aux, h_aux, min_e, max_e, h_lines=6)

                # Power Law
                pl_coords = []
                pl_text = ""
                if self.show_power_law.get() and len(epoch_recon_avgs) > 5:
                    params, predict_fn = self._fit_power_law(epoch_indices, epoch_recon_avgs)
                    if params:
                        a, b = params
                        future_ep = int(epoch_indices[-1] * 2)  # Project 2x current epoch
                        trend_x = np.linspace(epoch_indices[0], future_ep, 200)
                        trend_y = predict_fn(trend_x)

                        # Adjust scale for trend if it goes lower
                        # (We don't change grid here to keep actual data visible, but clamp trend)

                        # Calculate coords manually for trend to allow projection beyond current data width
                        # X Scale needs to account for future_ep
                        total_x_range = future_ep - epoch_indices[0]
                        x_step_pl = (w_aux - 50) / total_x_range
                        y_range_pl = max_e - min_e
                        draw_h_pl = h_aux - 40

                        for tx, ty in zip(trend_x, trend_y):
                            cx = 40 + (tx - epoch_indices[0]) * x_step_pl
                            # Clamp Y to view
                            ty_clamped = max(min_e, min(max_e, ty))
                            norm_ty = (ty_clamped - min_e) / y_range_pl
                            cy = h_aux - 20 - (norm_ty * draw_h_pl)
                            pl_coords.append(cx)
                            pl_coords.append(cy)

                        # Text
                        pred_1000 = predict_fn(1000)
                        pred_5000 = predict_fn(5000)
                        pl_text = f"Power Law: L = {a:.3f} × epoch^({b:.3f})\n"
                        pl_text += f"Pred @ 1k: {pred_1000:.3f}\n"
                        pl_text += f"Pred @ 5k: {pred_5000:.3f}"

                # Actual Epoch Avgs (Green)
                # Map X based on max range (either current or future if projection on)
                max_x_domain = future_ep if (self.show_power_law.get() and pl_coords) else epoch_indices[-1]
                x_range_act = max_x_domain - epoch_indices[0]
                if x_range_act == 0: x_range_act = 1

                x_step_act = (w_aux - 50) / x_range_act
                y_range_act = max_e - min_e
                draw_h_act = h_aux - 40

                act_coords = []
                for ep, val in zip(epoch_indices, epoch_recon_avgs):
                    cx = 40 + (ep - epoch_indices[0]) * x_step_act
                    norm_y = (val - min_e) / y_range_act
                    cy = h_aux - 20 - (norm_y * draw_h_act)
                    act_coords.append(cx)
                    act_coords.append(cy)

                # Render Lines
                if pl_coords:
                    self.canv_aux.create_line(pl_coords, fill="#FFD700", width=2, dash=(4, 2))
                    self.canv_aux.create_text(w_aux - 10, 20, text=pl_text, fill="#FFD700", anchor="ne",
                                              font=("Consolas", 9))

                if len(act_coords) > 2:
                    self.canv_aux.create_line(act_coords, fill="#55FF55", width=3)
                    # Dots
                    for i in range(0, len(act_coords), 2):
                        self.canv_aux.create_oval(act_coords[i] - 3, act_coords[i + 1] - 3,
                                                  act_coords[i] + 3, act_coords[i + 1] + 3,
                                                  fill="#55FF55", outline="#000")

                # Labels
                last_avg = epoch_recon_avgs[-1]
                self.canv_aux.create_text(50, 10, text=f"Avg Loss: {last_avg:.4f}",
                                          fill="#55FF55", anchor="nw", font=("Consolas", 11, "bold"))
                self.canv_aux.create_text(50, h_aux - 10, text=f"Ep {epoch_indices[0]}", fill="#888", anchor="sw")
                self.canv_aux.create_text(w_aux - 10, h_aux - 10, text=f"Ep {max_x_domain}", fill="#888", anchor="se")

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'canv_main'): self.canv_main.config(bg="#1E1E1E")
        if hasattr(self, 'canv_aux'): self.canv_aux.config(bg="#252526")

--- FILE: Plugins\tab_memory.py ---
import tkinter as tk
from tkinter import ttk, messagebox
import yaml
import threading
import torch


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Memory Graph"

        self.search_var = tk.StringVar()
        self._setup_ui()

    def _setup_ui(self):
        # Split: List vs Details
        pane = ttk.PanedWindow(self.parent, orient="horizontal")
        pane.pack(fill="both", expand=True, padx=10, pady=10)

        # LEFT: List
        left = ttk.Frame(pane, width=300)
        pane.add(left, weight=1)

        # Search Bar
        row_search = ttk.Frame(left)
        row_search.pack(fill="x", pady=5)
        ttk.Entry(row_search, textvariable=self.search_var).pack(side="left", fill="x", expand=True)
        ttk.Button(row_search, text="🔎", width=3, command=self._refresh_list).pack(side="left")

        # Treeview
        self.tree = ttk.Treeview(left, columns=("Type", "Updated"), show="headings")
        self.tree.heading("Type", text="Type")
        self.tree.heading("Updated", text="Updated")
        self.tree.pack(fill="both", expand=True)
        self.tree.bind("<<TreeviewSelect>>", self._on_select)

        # RIGHT: Editor/Viewer
        right = ttk.LabelFrame(pane, text="Entity Node Viewer")
        pane.add(right, weight=3)

        self.txt_editor = tk.Text(right, font=("Consolas", 10), bg="#1E1E1E", fg="#A8C7FA", insertbackground="white")
        self.txt_editor.pack(fill="both", expand=True, padx=5, pady=5)

        # Controls
        ctrl = ttk.Frame(right)
        ctrl.pack(fill="x", pady=5)
        ttk.Button(ctrl, text="SAVE CHANGES", command=self._save_node).pack(side="right", padx=5)
        ttk.Button(ctrl, text="DELETE NODE", command=self._delete_node).pack(side="left", padx=5)
        ttk.Button(ctrl, text="REFRESH", command=self._refresh_list).pack(side="left", padx=5)

        self._refresh_list()

    def _refresh_list(self):
        self.tree.delete(*self.tree.get_children())
        query = self.search_var.get().lower()

        nodes = self.app.hippocampus.nodes
        for name, node in nodes.items():
            if query and query not in name.lower():
                continue

            data = node.data
            updated = str(data.get('last_updated', ''))
            ntype = str(data.get('type', 'Unknown'))
            self.tree.insert("", "end", iid=name, text=name, values=(ntype, updated))

    def _on_select(self, event):
        sel = self.tree.selection()
        if not sel: return
        entity = sel[0]

        node = self.app.hippocampus.nodes.get(entity)
        if node:
            # Dump YAML to text box
            try:
                y_str = yaml.dump(node.data, sort_keys=False, allow_unicode=True)
                self.txt_editor.delete("1.0", tk.END)
                self.txt_editor.insert("1.0", y_str)
            except Exception as e:
                self.txt_editor.delete("1.0", tk.END)
                self.txt_editor.insert("1.0", f"Error parsing YAML: {e}")

    def _save_node(self):
        sel = self.tree.selection()
        if not sel: return
        entity = sel[0]

        content = self.txt_editor.get("1.0", tk.END).strip()
        try:
            # Validate YAML
            new_data = yaml.safe_load(content)
            if not isinstance(new_data, dict): raise ValueError("Must be a dictionary")

            self.app.hippocampus.update_memory_node(entity, new_data)
            self.app.hippocampus.save_memory()
            messagebox.showinfo("Success", f"Updated {entity}")
        except Exception as e:
            messagebox.showerror("YAML Error", str(e))

    def _delete_node(self):
        sel = self.tree.selection()
        if not sel: return
        entity = sel[0]

        if messagebox.askyesno("Confirm", f"Delete memory of {entity}?"):
            del self.app.hippocampus.nodes[entity]
            self.app.hippocampus.save_memory()
            self._refresh_list()
            self.txt_editor.delete("1.0", tk.END)

    def on_theme_change(self):
        pass

--- FILE: Plugins\tab_memory_agent.py ---
import tkinter as tk
from tkinter import ttk, messagebox
import threading
import time
import os
import torch
import traceback
from datetime import datetime


# --- HEADLESS HELPER ---
class MockVar:
    def __init__(self, value=None): self._val = value

    def set(self, value): self._val = value

    def get(self): return self._val


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Memory Agent"
        self.is_running = False
        self.stop_requested = False

        # --- STATE ---
        if self.parent is None:
            # Headless Defaults
            self.interval_min = MockVar(15)  # Run every 15 mins
            self.consolidation_depth = MockVar(5)  # Number of items to merge
            self.auto_prune = MockVar(True)
            self.status_var = MockVar("Idle.")
        else:
            # GUI Defaults
            self.interval_min = tk.IntVar(value=15)
            self.consolidation_depth = tk.IntVar(value=5)
            self.auto_prune = tk.BooleanVar(value=True)
            self.status_var = tk.StringVar(value="Idle.")

        self._setup_ui()

    def _setup_ui(self):
        if self.parent is None: return

        # Layout
        panel = ttk.Frame(self.parent, padding=10)
        panel.pack(fill="both", expand=True)

        # 1. Controls
        fr_ctrl = ttk.LabelFrame(panel, text="Agent Configuration", padding=10)
        fr_ctrl.pack(fill="x", pady=5)

        ttk.Label(fr_ctrl, text="Wake Interval (mins):").pack(side="left")
        ttk.Spinbox(fr_ctrl, from_=1, to=1440, textvariable=self.interval_min, width=5).pack(side="left", padx=5)

        ttk.Label(fr_ctrl, text="Batch Depth:").pack(side="left", padx=(10, 0))
        ttk.Spinbox(fr_ctrl, from_=1, to=50, textvariable=self.consolidation_depth, width=5).pack(side="left", padx=5)

        ttk.Checkbutton(fr_ctrl, text="Auto-Prune Weak Links", variable=self.auto_prune).pack(side="left", padx=15)

        self.btn_toggle = ttk.Button(fr_ctrl, text="START AGENT", command=self._toggle_agent)
        self.btn_toggle.pack(side="right")

        # 2. Activity Log
        fr_log = ttk.LabelFrame(panel, text="Hippocampal Activity", padding=10)
        fr_log.pack(fill="both", expand=True, pady=5)

        self.log_box = tk.Text(fr_log, font=("Consolas", 10), bg=self.app.colors["BG_MAIN"],
                               fg=self.app.colors["FG_TEXT"], state="disabled")
        self.log_box.pack(side="left", fill="both", expand=True)

        sb = ttk.Scrollbar(fr_log, command=self.log_box.yview)
        self.log_box.configure(yscrollcommand=sb.set)
        sb.pack(side="right", fill="y")

        # Tags
        self.log_box.tag_config("info", foreground=self.app.colors["FG_DIM"])
        self.log_box.tag_config("action", foreground=self.app.colors["ACCENT"])
        self.log_box.tag_config("error", foreground=self.app.colors["ERROR"])

        # Status Bar
        ttk.Label(panel, textvariable=self.status_var, foreground="#888").pack(anchor="w")

    def _log(self, msg, tag="info"):
        ts = datetime.now().strftime("%H:%M:%S")
        text = f"[{ts}] {msg}"

        if self.parent:
            self.log_box.config(state="normal")
            self.log_box.insert(tk.END, text + "\n", tag)
            self.log_box.see(tk.END)
            self.log_box.config(state="disabled")
            self.status_var.set(msg)
        else:
            print(f"[Memory Agent] {msg}")

    def _toggle_agent(self):
        if self.is_running:
            self.stop_requested = True
            if self.parent: self.btn_toggle.config(text="Stopping...")
        else:
            if not self.app.hippocampus:
                self._log("Hippocampus not loaded. Agent aborting.", "error")
                return

            self.is_running = True
            self.stop_requested = False
            if self.parent: self.btn_toggle.config(text="STOP AGENT")
            threading.Thread(target=self._agent_loop, daemon=True).start()

    def _agent_loop(self):
        self._log("Agent started. Monitoring short-term memory...", "action")

        while not self.stop_requested:
            try:
                # 1. Sleep Phase (Wait interval minutes)
                # Check stop_requested every second to stay responsive
                interval_sec = self.interval_min.get() * 60
                for _ in range(int(interval_sec)):
                    if self.stop_requested: break
                    time.sleep(1)

                if self.stop_requested: break

                # 2. Consolidation Phase
                self._log("Waking up for consolidation cycle...", "action")
                self._perform_consolidation()

            except Exception as e:
                self._log(f"Cycle Error: {e}", "error")
                traceback.print_exc()
                time.sleep(60)  # Wait a bit before retry on error

        self.is_running = False
        if self.parent: self.btn_toggle.config(text="START AGENT")
        self._log("Agent stopped.", "info")

    def _perform_consolidation(self):
        """
        Reads recent/raw memories from Hippocampus, summarizes them using the active Lobe,
        and re-saves them as 'consolidated' nodes.
        """
        if not self.app.hippocampus: return

        # 1. Get Active Brain for summarization
        lobe_id = self.app.active_lobe.get()
        brain = self.app.lobes[lobe_id]

        if not brain:
            self._log("No Lobe loaded. Skipping cognitive tasks.", "error")
            return

        # 2. Fetch 'Raw' Memories
        # Simulating fetching recent additions. In a full graph implementation,
        # this would query nodes created > X minutes ago but not yet tagged 'consolidated'.
        memories = self.app.hippocampus.search("recent", limit=self.consolidation_depth.get())

        if not memories:
            self._log("No new memories to consolidate.")
            return

        # 3. Summarize / Reflect
        context_str = "\n".join([m.get('content', '') for m in memories])
        prompt = f"Consolidate these recent events into a single concise insight:\n\n{context_str}\n\nInsight:"

        self._log(f"Consolidating {len(memories)} items...", "action")

        try:
            insight = self._generate_thought(brain, prompt)

            # 4. Save back to Hippocampus
            if insight:
                # Add the new consolidated node
                self.app.hippocampus.add(insight, tags=["consolidated", "agent_generated"])
                self._log(f"Stored insight: {insight[:50]}...", "action")

                # Optional: Mark old ones as processed or prune
                if self.auto_prune.get():
                    self._log(f"Processed {len(memories)} raw inputs.", "info")
                    # Actual graph pruning logic would go here

            # 5. Save Graph to Disk
            self.app.hippocampus.save_memory()

        except Exception as e:
            self._log(f"Cognitive Failure: {e}", "error")

    def _generate_thought(self, brain, prompt):
        """Helper to run inference on the active lobe"""
        ribosome = self.app.ribosome
        ids = ribosome._tokenize(prompt)
        t = torch.tensor(ids, device=self.app.device).unsqueeze(0)

        # Placeholder sensors for multimodal models
        v = torch.zeros(1, 1, 768).to(self.app.device)
        a = torch.zeros(1, 1, 128).to(self.app.device)

        out_text = ""
        with self.app.gpu_lock:
            brain.eval()
            with torch.no_grad():
                # Simple AR generation loop (limit 100 tokens for brevity)
                for _ in range(100):
                    # Handle both AR and Diffusion (though AR is preferred for text summarization)
                    if hasattr(brain, 'timestep_emb'):
                        # Diffusion models aren't great at pure text summarization
                        # without specific training, but we try anyway:
                        return "[Diffusion model cannot summarize text reliably yet]"

                    logits, _, _ = brain(v, a, t)
                    next_tok = torch.argmax(logits[:, -1, :], dim=-1)
                    t = torch.cat([t, next_tok.unsqueeze(0)], dim=1)

                    word = ribosome.decode([next_tok.item()])
                    out_text += word
                    if next_tok.item() == 50256: break

        return out_text.strip()

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'log_box'):
            self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])

--- FILE: Plugins\tab_playground.py ---
import tkinter as tk
from tkinter import ttk, font
import threading
import torch
import torch.nn.functional as F
from PIL import ImageTk, Image
import os
import traceback
import time

try:
    import pygame

    pygame.mixer.init()
    HAS_AUDIO = True
except:
    HAS_AUDIO = False


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Playground"
        self.is_generating = False
        self.image_refs = []  # Keep references to prevent GC

        # --- PARAMS ---
        self.temp = tk.DoubleVar(value=0.9)
        self.top_k = tk.IntVar(value=100)
        self.max_tokens = tk.IntVar(value=1024)
        self.steps = tk.IntVar(value=18)

        # --- UI STATE ---
        self.progress_var = tk.StringVar(value="")

        self._setup_ui()

    def _setup_ui(self):
        # Main split
        panel = ttk.PanedWindow(self.parent, orient="vertical")
        panel.pack(fill="both", expand=True, padx=5, pady=5)

        # 1. CHAT AREA
        chat_frame = ttk.Frame(panel)
        panel.add(chat_frame, weight=1)

        sb = ttk.Scrollbar(chat_frame)
        sb.pack(side="right", fill="y")

        self.chat_box = tk.Text(chat_frame, font=("Consolas", 11), bg=self.app.colors["BG_CARD"],
                                fg=self.app.colors["FG_TEXT"], wrap="word", padx=10, pady=10,
                                yscrollcommand=sb.set, borderwidth=0, highlightthickness=0)
        self.chat_box.pack(side="left", fill="both", expand=True)
        sb.config(command=self.chat_box.yview)

        # Tags
        self.chat_box.tag_config("user", foreground="#A8C7FA", justify="right", rmargin=10)
        self.chat_box.tag_config("brain", foreground="#E3E3E3", justify="left", lmargin1=10, lmargin2=10)
        self.chat_box.tag_config("system", foreground="#8e9198", font=("Segoe UI", 9, "italic"), justify="center")
        self.chat_box.tag_config("error", foreground="#F28B82", justify="center")
        self.chat_box.tag_config("media_placeholder", foreground="#FDD663", font=("Segoe UI", 9, "bold"),
                                 justify="center")
        self.chat_box.tag_config("media", justify="center")

        # 2. CONTROLS
        ctrl = ttk.Frame(panel, padding=5)
        panel.add(ctrl, weight=0)

        # Input
        input_frame = ttk.Frame(ctrl)
        input_frame.pack(fill="x", pady=(0, 5))

        self.input_var = tk.StringVar()
        self.entry = ttk.Entry(input_frame, textvariable=self.input_var, font=("Segoe UI", 10))
        self.entry.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.entry.bind("<Return>", lambda e: self._on_send())

        self.btn_send = ttk.Button(input_frame, text="SEND", command=self._on_send)
        self.btn_send.pack(side="left")

        # Settings
        sets = ttk.Frame(ctrl)
        sets.pack(fill="x")

        def add_slider(lbl, var, min_v, max_v):
            f = ttk.Frame(sets)
            f.pack(side="left", padx=5)
            ttk.Label(f, text=lbl, font=("Segoe UI", 8)).pack(side="left")
            ttk.Scale(f, from_=min_v, to=max_v, variable=var, orient="horizontal", length=80).pack(side="left", padx=5)

        add_slider("Temp:", self.temp, 0.1, 2.0)
        add_slider("Top-K:", self.top_k, 0, 200)
        add_slider("MaxLen:", self.max_tokens, 256, 4096)

        self.diff_frame = ttk.Frame(sets)
        add_slider("Steps:", self.steps, 4, 64)
        self.diff_frame.pack_forget()

        # Status Bar
        self.lbl_status = ttk.Label(ctrl, textvariable=self.progress_var, foreground=self.app.colors["ACCENT"],
                                    font=("Segoe UI", 9))
        self.lbl_status.pack(pady=(5, 0))

    def _detect_diffusion(self):
        brain = self.app.lobes.get(self.app.active_lobe.get())
        is_diff = (brain is not None and hasattr(brain, 'timestep_emb'))
        if is_diff:
            self.diff_frame.pack(side="left", padx=10)
        else:
            self.diff_frame.pack_forget()

    # --- OUTPUT HELPERS ---
    def _print(self, text, tag="brain"):
        self.chat_box.insert(tk.END, f"\n{text}\n", tag)
        self.chat_box.see(tk.END)

    def _print_image(self, pil_img):
        try:
            max_w, max_h = 512, 512
            pil_img.thumbnail((max_w, max_h))
            photo = ImageTk.PhotoImage(pil_img)
            self.image_refs.append(photo)

            self.chat_box.insert(tk.END, "\n", "media")
            self.chat_box.image_create(tk.END, image=photo, padx=10, pady=10)
            self.chat_box.insert(tk.END, "\n", "media")
            self.chat_box.see(tk.END)
        except Exception as e:
            self._print(f"[Image Render Error: {e}]", "error")

    # --- GENERATION ---
    def _on_send(self):
        if self.is_generating: return
        prompt = self.input_var.get().strip()
        if not prompt: return

        self.input_var.set("")
        self._print(f"> {prompt}", "user")

        self.is_generating = True
        self.btn_send.config(state="disabled")
        self._detect_diffusion()
        self.progress_var.set("Waiting for GPU...")

        threading.Thread(target=self._run_generation, args=(prompt,), daemon=True).start()

    def _run_generation(self, prompt):
        try:
            lobe_id = self.app.active_lobe.get()
            brain = self.app.lobes[lobe_id]
            ribosome = self.app.ribosome

            if not brain: raise Exception("No Brain Loaded.")

            prompt_ids = ribosome._tokenize(prompt)

            brain.eval()
            tokens = []

            with self.app.gpu_lock:
                with torch.no_grad():
                    if hasattr(brain, 'timestep_emb'):
                        steps = self.steps.get()
                        for i in range(1, steps + 1):
                            tokens = brain.generate(
                                prompt_tokens=prompt_ids,
                                max_length=self.max_tokens.get(),
                                steps=i,
                                temperature=self.temp.get(),
                                top_k=self.top_k.get()
                            )
                            if i % 2 == 0 or i == steps:
                                preview = ribosome.decode(tokens[:50]) + "..."
                                self.parent.after(0, lambda s=i, p=preview: self.progress_var.set(
                                    f"Diffusing {s}/{steps}: {p}"))
                    else:
                        t = torch.tensor(prompt_ids, device=self.app.device).unsqueeze(0)
                        v = torch.zeros(1, 1, 768).to(self.app.device)
                        a = torch.zeros(1, 1, 128).to(self.app.device)

                        for i in range(200):
                            self.parent.after(0, lambda c=i: self.progress_var.set(f"Generating token {c}..."))
                            logits, _, _ = brain(v, a, t)
                            next_tok = torch.multinomial(F.softmax(logits[:, -1, :] / self.temp.get(), dim=-1), 1)
                            t = torch.cat([t, next_tok], dim=1)
                            if next_tok.item() == 50256: break
                        tokens = t[0].tolist()

            self.parent.after(0, lambda: self.progress_var.set("Decoding content..."))

            # Text
            text_out = ribosome.decode(tokens)
            if text_out.startswith(prompt): text_out = text_out[len(prompt):].strip()
            if text_out.strip():
                self.parent.after(0, lambda t=text_out: self._print(t, "brain"))

            # Multimedia
            self._scan_and_render_media(tokens, ribosome)

        except Exception as e:
            traceback.print_exc()
            self.parent.after(0, lambda m=str(e): self._print(f"Error: {m}", "error"))
        finally:
            self.is_generating = False
            self.parent.after(0, lambda: self.btn_send.config(state="normal"))
            self.parent.after(0, lambda: self.progress_var.set("Ready."))

    def _scan_and_render_media(self, tokens, ribosome):
        """Scans tokens for media blocks and attempts to render them."""
        img_buffer = []
        aud_buffer = []

        side = ribosome.config.image_size // ribosome.config.patch_size
        img_tokens_needed = side ** 2

        for tok in tokens:
            val = tok.item() if torch.is_tensor(tok) else tok

            # --- VISUAL TOKENS ---
            if val >= ribosome.image_vocab_base and val < ribosome.audio_vocab_base:
                img_buffer.append(val)
                # Feedback for accumulating buffer
                if len(img_buffer) % 50 == 0:
                    self.parent.after(0, lambda: self.progress_var.set("Receiving Visual Data..."))

                # Full Image Check
                if len(img_buffer) == img_tokens_needed:
                    try:
                        img = ribosome.decode_image_tokens(img_buffer)
                        if img:
                            self.parent.after(0, lambda i=img: self._print_image(i))
                        else:
                            raise Exception("Empty image")
                    except Exception as e:
                        # RENDER FAILED: Show placeholder
                        self.parent.after(0, lambda: self._print(f"[Image Generation Failed: {len(img_buffer)} tokens]",
                                                                 "media_placeholder"))
                    img_buffer = []

            # --- AUDIO TOKENS ---
            elif val >= ribosome.audio_vocab_base:
                aud_buffer.append(val)
                if len(aud_buffer) % 100 == 0:
                    self.parent.after(0, lambda: self.progress_var.set("Receiving Audio Data..."))

                if len(aud_buffer) >= 1000:
                    self._flush_audio(aud_buffer, ribosome)
                    aud_buffer = []

        # Flush leftovers
        if aud_buffer:
            self._flush_audio(aud_buffer, ribosome)

        # If image buffer has stragglers (incomplete image)
        if len(img_buffer) > 0:
            self.parent.after(0, lambda: self._print(
                f"[Incomplete Image Data: {len(img_buffer)}/{img_tokens_needed} tokens]", "media_placeholder"))

    def _flush_audio(self, buffer, ribosome):
        try:
            wav = ribosome.decode_audio_tokens(buffer)
            if wav and HAS_AUDIO:
                self.parent.after(0, lambda: self._print("[Audio Clip Playing...]", "system"))
                threading.Thread(target=self._play_audio, args=(wav,), daemon=True).start()
            else:
                raise Exception("Audio decode failed")
        except:
            self.parent.after(0, lambda: self._print("[Audio Generation Failed]", "media_placeholder"))

    def _play_audio(self, wav_path):
        try:
            while pygame.mixer.music.get_busy(): time.sleep(0.1)
            pygame.mixer.music.load(wav_path)
            pygame.mixer.music.play()
        except:
            pass

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'chat_box'):
            self.chat_box.config(bg=c["BG_CARD"], fg=c["FG_TEXT"])

--- FILE: Plugins\tab_rlm.py ---
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import threading
import os
import re
import traceback
import torch
from datetime import datetime

# --- SAFE IMPORTS ---
try:
    from RestrictedPython import compile_restricted, safe_globals

    HAS_SANDBOX = True
except ImportError:
    HAS_SANDBOX = False


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Local RLM"

        self.is_running = False
        self.stop_requested = False
        self.context_data = ""
        self.history = []

        # Configuration
        self.manager_id = tk.IntVar(value=1)
        self.worker_id = tk.IntVar(value=2)
        self.max_iters = tk.IntVar(value=12)
        self.temp = tk.DoubleVar(value=0.7)

        self._setup_ui()
        if not HAS_SANDBOX:
            self._log("CRITICAL: Install 'RestrictedPython' via pip.", "error")

    def _setup_ui(self):
        pane = ttk.PanedWindow(self.parent, orient="horizontal")
        pane.pack(fill="both", expand=True, padx=10, pady=10)

        left = ttk.Frame(pane, width=400)
        right = ttk.Frame(pane)
        pane.add(left, weight=1)
        pane.add(right, weight=3)

        # --- LEFT PANEL ---
        # 1. Config
        conf = ttk.LabelFrame(left, text="Agent Architecture", padding=10)
        conf.pack(fill="x", pady=5)

        # Manager Selector
        ttk.Label(conf, text="Manager (Code/Plan):", foreground=self.app.colors["ACCENT"]).pack(anchor="w")
        f_mgr = ttk.Frame(conf)
        f_mgr.pack(fill="x", pady=2)
        for i in range(1, 5):
            ttk.Radiobutton(f_mgr, text=f"L{i}", variable=self.manager_id, value=i).pack(side="left", padx=4)

        # Worker Selector
        ttk.Label(conf, text="Worker (Read/Scan):", foreground=self.app.colors["SUCCESS"]).pack(anchor="w",
                                                                                                pady=(10, 0))
        f_wrk = ttk.Frame(conf)
        f_wrk.pack(fill="x", pady=2)
        for i in range(1, 5):
            ttk.Radiobutton(f_wrk, text=f"L{i}", variable=self.worker_id, value=i).pack(side="left", padx=4)

        ttk.Label(conf, text="Max Iterations:").pack(anchor="w", pady=(10, 0))
        ttk.Spinbox(conf, from_=1, to=50, textvariable=self.max_iters).pack(fill="x", pady=2)

        ttk.Label(conf, text="Temperature:").pack(anchor="w")
        ttk.Scale(conf, from_=0.1, to=2.0, variable=self.temp, orient="horizontal").pack(fill="x")

        # 2. Context
        loader = ttk.LabelFrame(left, text="Context Source", padding=10)
        loader.pack(fill="both", expand=True, pady=5)

        btn_row = ttk.Frame(loader)
        btn_row.pack(fill="x")
        ttk.Button(btn_row, text="Load Text File", command=self._load_file).pack(side="left", fill="x", expand=True)
        ttk.Button(btn_row, text="X", width=3, command=self._clear_context).pack(side="right")

        self.lbl_context = ttk.Label(loader, text="No context.", foreground="#888", wraplength=200)
        self.lbl_context.pack(pady=5)

        # 3. Query
        q_fr = ttk.LabelFrame(left, text="Goal / Query", padding=10)
        q_fr.pack(fill="x", pady=5)
        self.txt_query = tk.Text(q_fr, height=6, font=("Segoe UI", 10))
        self.txt_query.pack(fill="x")

        self.btn_run = ttk.Button(left, text="START HIERARCHICAL REASONING", command=self._start_loop)
        self.btn_run.pack(fill="x", pady=10)

        # --- RIGHT PANEL ---
        self.log_box = tk.Text(right, font=("Consolas", 10), bg="#1E1E1E", fg="#D4D4D4", borderwidth=0)
        self.log_box.pack(fill="both", expand=True)

        self.log_box.tag_config("sys", foreground="#569CD6")
        self.log_box.tag_config("llm", foreground="#CE9178")
        self.log_box.tag_config("code", foreground="#4EC9B0")
        self.log_box.tag_config("exec", foreground="#DCDCAA")
        self.log_box.tag_config("error", foreground="#F44747")
        self.log_box.tag_config("final", foreground="#B5CEA8", font=("Consolas", 11, "bold"))

    def _log(self, msg, tag="sys"):
        self.log_box.insert(tk.END, f"\n[{tag.upper()}] {msg}\n", tag)
        self.log_box.see(tk.END)

    def _load_file(self):
        f = filedialog.askopenfilename(filetypes=[("Text", "*.txt"), ("All", "*.*")])
        if f:
            try:
                with open(f, 'r', encoding='utf-8', errors='ignore') as file:
                    self.context_data = file.read()
                self.lbl_context.config(text=f"{os.path.basename(f)} ({len(self.context_data)} chars)")
                self._log(f"Loaded context: {len(self.context_data)} chars.", "sys")
            except Exception as e:
                self._log(f"Load failed: {e}", "error")

    def _clear_context(self):
        self.context_data = ""
        self.lbl_context.config(text="No context.")

    def _start_loop(self):
        if self.is_running:
            self.stop_requested = True
            self.btn_run.config(text="Stopping...")
            return

        mgr_brain = self.app.lobes[self.manager_id.get()]
        wrk_brain = self.app.lobes[self.worker_id.get()]

        if not mgr_brain:
            messagebox.showerror("Error", f"Manager Lobe {self.manager_id.get()} is not loaded.")
            return
        if not wrk_brain:
            messagebox.showerror("Error", f"Worker Lobe {self.worker_id.get()} is not loaded.")
            return

        if not self.context_data:
            messagebox.showerror("Error", "Load text context first.")
            return

        self.is_running = True
        self.stop_requested = False
        self.btn_run.config(text="STOP LOOP")
        self.log_box.delete("1.0", tk.END)

        threading.Thread(target=self._rlm_worker, daemon=True).start()

    def _generate(self, lobe_id, prompt, max_new=256):
        """Generic generation helper"""
        try:
            brain = self.app.lobes[lobe_id]
            ribosome = self.app.ribosome

            p_ids = ribosome._tokenize(prompt)
            p_tensor = torch.tensor(p_ids, device=self.app.device).unsqueeze(0)

            with self.app.gpu_lock:
                brain.eval()
                with torch.no_grad():
                    if hasattr(brain, 'timestep_emb'):
                        # Diffusion
                        tokens = brain.generate(
                            prompt_tokens=p_ids,
                            max_length=len(p_ids) + max_new,
                            steps=24,
                            temperature=self.temp.get()
                        )
                    else:
                        # AR
                        t = p_tensor
                        v = torch.zeros(1, 1, 768).to(self.app.device)
                        a = torch.zeros(1, 1, 128).to(self.app.device)
                        for _ in range(max_new):
                            logits, _, _ = brain(v, a, t)
                            next_tok = torch.multinomial(
                                torch.nn.functional.softmax(logits[:, -1, :] / self.temp.get(), dim=-1), 1)
                            t = torch.cat([t, next_tok], dim=1)
                            if next_tok.item() == 50256: break
                        tokens = t[0].tolist()

            text = ribosome.decode(tokens)
            return text[len(prompt):].strip()
        except Exception as e:
            return f"Gen Error: {e}"

    def _rlm_worker(self):
        try:
            query = self.txt_query.get("1.0", tk.END).strip()
            if not query: query = "Summarize the context."

            # --- SYSTEM PROMPT (For Manager) ---
            sys_prompt = f"""
You are the MANAGER Agent.
You have a massive text file in variable `CONTEXT`.
You cannot read it directly. You must use the `worker` tool to read chunks.

TOOLS:
1. `print(val)`: Log thoughts/results.
2. `worker(slice, question)`: Ask the WORKER agent to read a specific text slice and answer a question.

GOAL: {query}

INSTRUCTIONS:
- Write valid Python code wrapped in ```python ... ```.
- Inspect `len(CONTEXT)`.
- Use string slicing to send manageable chunks to the worker.
- When you have the answer, print "FINAL: [Answer]".
"""

            # Safe Tool for Worker
            def _worker_tool(text_slice, question):
                # Worker sees only the slice + specific question
                # Limit slice size to prevent context overflow on worker
                chunk = text_slice[:3000]
                p = f"Text Chunk: {chunk}\n\nQuestion: {question}\n\nAnswer concisely:"
                self._log(f"[Worker L{self.worker_id.get()}] Reading {len(chunk)} chars...", "sys")
                return self._generate(self.worker_id.get(), p, max_new=128)

            env = safe_globals.copy()
            env['CONTEXT'] = self.context_data
            env['worker'] = _worker_tool
            env['_print_buffer'] = []
            env['print'] = lambda *args: env['_print_buffer'].append(" ".join(map(str, args)))

            current_prompt = sys_prompt

            for i in range(self.max_iters.get()):
                if self.stop_requested: break

                self._log(f"--- Cycle {i + 1} (Manager L{self.manager_id.get()}) ---", "sys")

                # 1. Manager Thinks (Code Generation)
                full_p = current_prompt + "\n\nWrite Python code for the next step:"
                llm_text = self._generate(self.manager_id.get(), full_p, max_new=300)
                self._log(llm_text, "llm")

                if "FINAL:" in llm_text:
                    self._log(f"\n{llm_text}", "final")
                    break

                # 2. Extract Code
                code_match = re.search(r"```python\n(.*?)\n```", llm_text, re.DOTALL)
                if not code_match:
                    code_match = re.search(r"```\n(.*?)\n```", llm_text, re.DOTALL)

                if code_match:
                    code = code_match.group(1)
                    self._log(f"Executing:\n{code}", "code")

                    env['_print_buffer'] = []
                    try:
                        byte_code = compile_restricted(code, '<string>', 'exec')
                        exec(byte_code, env)
                        output = "\n".join(env['_print_buffer']) or "[Done]"

                        self._log(f"Result:\n{output}", "exec")
                        current_prompt = f"Previous Code Output:\n{output}\n\nNext step?"

                    except Exception as e:
                        self._log(f"Runtime Error: {e}", "error")
                        current_prompt = f"Error: {e}. Fix the code."
                else:
                    self._log("No code block. Prompting again...", "warn")
                    current_prompt = "Please output valid Python code in ```python blocks."

        except Exception as e:
            self._log(f"Critical Failure: {e}", "error")
            traceback.print_exc()
        finally:
            self.is_running = False
            self.parent.after(0, lambda: self.btn_run.config(text="START HIERARCHICAL REASONING"))

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'log_box'): self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])

--- FILE: Plugins\tab_settings.py ---
import tkinter as tk
from tkinter import ttk, colorchooser, messagebox
import json
import os


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "System Config"

        self.config_path = os.path.join(self.app.paths["root"], "settings.json")
        self.local_colors = self.app.colors.copy()

        # --- DEFINING THE 4 SCHEMES ---
        self.SCHEMES = {
            "NOSTROMO": {  # Default Sci-Fi
                "BG_MAIN": "#0b0f19", "BG_CARD": "#131620", "FG_TEXT": "#E3E3E3",
                "FG_DIM": "#8e9198", "ACCENT": "#A8C7FA", "BTN": "#1E222D",
                "BTN_ACT": "#2B3042", "SUCCESS": "#81C995", "ERROR": "#F28B82",
                "WARN": "#FDD663", "BORDER": "#444444", "GRID": "#333333", "SCROLL": "#2B3042"
            },
            "APERTURE": {  # Light/Lab
                "BG_MAIN": "#F0F2F5", "BG_CARD": "#FFFFFF", "FG_TEXT": "#1A1A1A",
                "FG_DIM": "#606060", "ACCENT": "#007ACC", "BTN": "#E1E4E8",
                "BTN_ACT": "#D1D5DA", "SUCCESS": "#2EA043", "ERROR": "#DA3633",
                "WARN": "#D29922", "BORDER": "#E1E4E8", "GRID": "#E1E1E1", "SCROLL": "#C0C4C8"
            },
            "MATRIX": {  # Hacker
                "BG_MAIN": "#000000", "BG_CARD": "#0D110D", "FG_TEXT": "#00FF41",
                "FG_DIM": "#008F11", "ACCENT": "#00FF41", "BTN": "#003B00",
                "BTN_ACT": "#005500", "SUCCESS": "#00FF41", "ERROR": "#FF0000",
                "WARN": "#FFFF00", "BORDER": "#003B00", "GRID": "#002200", "SCROLL": "#003B00"
            },
            "NEON": {  # Cyberpunk
                "BG_MAIN": "#120024", "BG_CARD": "#1F003D", "FG_TEXT": "#E0E0E0",
                "FG_DIM": "#B07CC6", "ACCENT": "#FF00FF", "BTN": "#2D0052",
                "BTN_ACT": "#45007A", "SUCCESS": "#00FFC8", "ERROR": "#FF0055",
                "WARN": "#FFE600", "BORDER": "#45007A", "GRID": "#2D0052", "SCROLL": "#45007A"
            }
        }

        self.color_vars = {}
        for key, val in self.local_colors.items():
            self.color_vars[key] = tk.StringVar(value=val)

        self._setup_ui()

    def _setup_ui(self):
        # 1. THEME GALLERY (2x2 Grid)
        gallery_frame = ttk.LabelFrame(self.parent, text="Visual Core Presets", padding=10)
        gallery_frame.pack(fill="x", padx=20, pady=10)

        col = 0
        row = 0

        for name, scheme in self.SCHEMES.items():
            # Card Frame
            card = tk.Frame(gallery_frame, bg=scheme["BG_CARD"], bd=1, relief="solid")
            card.grid(row=row, column=col, padx=10, pady=10, sticky="nsew")
            gallery_frame.columnconfigure(col, weight=1)

            # Preview Content
            header = tk.Label(card, text=name, bg=scheme["BG_CARD"], fg=scheme["ACCENT"], font=("Segoe UI", 12, "bold"))
            header.pack(pady=(10, 5))

            sub = tk.Label(card, text="System Interface", bg=scheme["BG_CARD"], fg=scheme["FG_DIM"],
                           font=("Segoe UI", 9))
            sub.pack(pady=(0, 10))

            # Activate Button
            btn = tk.Button(card, text="ACTIVATE", bg=scheme["BTN"], fg=scheme["FG_TEXT"], relief="flat",
                            activebackground=scheme["BTN_ACT"], activeforeground=scheme["FG_TEXT"],
                            command=lambda s=scheme: self._apply_live(s))
            btn.pack(fill="x", padx=20, pady=10)

            col += 1
            if col > 1:
                col = 0
                row += 1

        # 2. FINE TUNING
        tune_frame = ttk.LabelFrame(self.parent, text="Manual Override", padding=15)
        tune_frame.pack(fill="both", expand=True, padx=20, pady=5)

        r = 0
        c = 0
        keys = sorted(self.local_colors.keys())
        for key in keys:
            if key not in self.color_vars: continue

            ttk.Label(tune_frame, text=key).grid(row=r, column=c, sticky="w", padx=5)

            # Swatch
            btn = tk.Button(tune_frame, bg=self.color_vars[key].get(), width=4,
                            command=lambda k=key: self._pick_color(k))
            btn.grid(row=r, column=c + 1, padx=5, pady=2)

            setattr(self, f"btn_{key}", btn)

            r += 1
            if r > 6:
                r = 0
                c += 2

        # 3. SAVE
        ttk.Button(self.parent, text="SAVE CONFIGURATION", command=self._save_config).pack(fill="x", padx=20, pady=10)

    def _apply_live(self, theme):
        # 1. Update App State
        self.app.colors.update(theme)

        # 2. Update UI Vars
        for k, v in theme.items():
            if k in self.color_vars:
                self.color_vars[k].set(v)
                if hasattr(self, f"btn_{k}"):
                    getattr(self, f"btn_{k}").config(bg=v)

        # 3. Trigger Global Refresh
        self.app.apply_theme()
        self._save_config(silent=True)

    def _pick_color(self, key):
        curr = self.color_vars[key].get()
        color = colorchooser.askcolor(color=curr, title=f"Override {key}")
        if color[1]:
            hex_val = color[1]
            self.color_vars[key].set(hex_val)
            getattr(self, f"btn_{key}").config(bg=hex_val)
            # Apply immediately to see effect
            self.app.colors[key] = hex_val
            self.app.apply_theme()

    def _save_config(self, silent=False):
        # Sync vars to app state
        new_colors = {k: v.get() for k, v in self.color_vars.items()}
        self.app.colors.update(new_colors)

        data = {}
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r') as f:
                try:
                    data = json.load(f)
                except:
                    pass

        data["colors"] = self.app.colors

        try:
            with open(self.config_path, 'w') as f:
                json.dump(data, f, indent=2)
            if not silent:
                messagebox.showinfo("System Update", "Theme saved successfully.")
        except Exception as e:
            messagebox.showerror("Error", f"Could not save settings: {e}")

    def on_theme_change(self):
        # Called when theme changes externally (to update swatches if needed)
        pass

--- FILE: Plugins\tab_symbiosis.py ---
import tkinter as tk
from tkinter import ttk
import threading
import torch
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import time
import os
from datetime import datetime


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Symbiosis"

        self.is_running = False
        self.stop_requested = False

        # UI vars
        self.teacher_id = tk.IntVar(value=2)
        self.student_id = tk.IntVar(value=1)

        # Storage settings
        self.autosave_enabled = tk.BooleanVar(value=True)
        self.save_interval = tk.IntVar(value=50)
        self.harvest_enabled = tk.BooleanVar(value=True)
        self.auto_scroll = tk.BooleanVar(value=True)

        # Harvest Config
        self.harvest_dir = os.path.join(self.app.paths['memories'], "harvested")
        if not os.path.exists(self.harvest_dir): os.makedirs(self.harvest_dir)
        self.max_file_size = 10 * 1024 * 1024  # 10 MB limit

        self._setup_ui()

    def _setup_ui(self):
        split = ttk.PanedWindow(self.parent, orient="vertical")
        split.pack(fill="both", expand=True, padx=10, pady=10)

        # --- TOP PANEL: CONTROLS ---
        top_frame = ttk.Frame(split)
        split.add(top_frame, weight=0)

        # 1. Connection Panel
        panel = ttk.LabelFrame(top_frame, text="Neural Link", padding=15)
        panel.pack(fill="x", pady=(0, 10))

        f_link = ttk.Frame(panel)
        f_link.pack(fill="x", pady=5)

        # Selectors
        ttk.Label(f_link, text="TEACHER:", font=("Segoe UI", 9, "bold")).pack(side="left")
        ttk.Spinbox(f_link, from_=1, to=4, textvariable=self.teacher_id, width=3).pack(side="left", padx=5)
        ttk.Label(f_link, text=" >>> STREAMS TO >>> ", foreground=self.app.colors["ACCENT"]).pack(side="left", padx=5)
        ttk.Label(f_link, text="STUDENT:", font=("Segoe UI", 9, "bold")).pack(side="left")
        ttk.Spinbox(f_link, from_=1, to=4, textvariable=self.student_id, width=3).pack(side="left", padx=5)

        # Storage Row
        f_store = ttk.Frame(panel)
        f_store.pack(fill="x", pady=(10, 0))

        # Auto-Save Brain
        ttk.Checkbutton(f_store, text="Auto-Save Brain", variable=self.autosave_enabled).pack(side="left")
        ttk.Label(f_store, text="Every").pack(side="left", padx=2)
        ttk.Entry(f_store, textvariable=self.save_interval, width=4).pack(side="left")
        ttk.Label(f_store, text="Cycles").pack(side="left", padx=(2, 15))

        # Harvest Data
        ttk.Separator(f_store, orient="vertical").pack(side="left", fill="y", padx=5)
        ttk.Checkbutton(f_store, text="Harvest Output (Max 10MB/file)", variable=self.harvest_enabled).pack(side="left",
                                                                                                            padx=5)

        # 2. Action Button
        self.btn_start = ttk.Button(panel, text="INITIATE SYMBIOSIS", command=self._toggle_symbiosis)
        self.btn_start.pack(fill="x", pady=(10, 0))

        # --- BOTTOM PANEL: LOGS ---
        bot_frame = ttk.LabelFrame(split, text="Symbiosis Telemetry", padding=10)
        split.add(bot_frame, weight=1)

        tool_fr = ttk.Frame(bot_frame)
        tool_fr.pack(fill="x")
        ttk.Checkbutton(tool_fr, text="Autoscroll", variable=self.auto_scroll).pack(side="right")

        self.log_box = tk.Text(bot_frame, font=("Consolas", 9), height=15,
                               bg=self.app.colors["BG_MAIN"], fg=self.app.colors["FG_TEXT"], borderwidth=0)
        self.log_box.pack(side="left", fill="both", expand=True)
        sb = ttk.Scrollbar(bot_frame, orient="vertical", command=self.log_box.yview)
        self.log_box.configure(yscrollcommand=sb.set)
        sb.pack(side="right", fill="y")

        self.log_box.tag_config("info", foreground=self.app.colors["FG_TEXT"])
        self.log_box.tag_config("gen", foreground=self.app.colors["ACCENT"])
        self.log_box.tag_config("save", foreground=self.app.colors["SUCCESS"], font=("Consolas", 9, "bold"))
        self.log_box.tag_config("harvest", foreground="#FDD663", font=("Consolas", 9, "italic"))
        self.log_box.tag_config("warn", foreground=self.app.colors["WARN"])
        self.log_box.tag_config("err", foreground=self.app.colors["ERROR"])

    def _log(self, msg, tag="info"):
        ts = datetime.now().strftime('%H:%M:%S')
        full_msg = f"[{ts}] {msg}\n"
        self.log_box.insert(tk.END, full_msg, tag)
        if self.auto_scroll.get(): self.log_box.see(tk.END)

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'log_box'): self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])

    def _toggle_symbiosis(self):
        if self.is_running:
            self.stop_requested = True
            self.btn_start.config(text="STOPPING...", state="disabled")
        else:
            t_id = self.teacher_id.get()
            s_id = self.student_id.get()

            if not self.app.lobes[t_id] or not self.app.lobes[s_id]:
                self._log("Error: Both Lobes must be loaded.", "err")
                return

            self.is_running = True
            self.stop_requested = False
            self.btn_start.config(text="SEVER LINK")
            self._log(f"Link Established: Teacher Lobe {t_id} -> Student Lobe {s_id}", "info")

            threading.Thread(target=self._worker, daemon=True).start()

    def _worker(self):
        t_id = self.teacher_id.get()
        s_id = self.student_id.get()

        teacher = self.app.lobes[t_id]
        student = self.app.lobes[s_id]
        opt = self.app.optimizers[s_id]
        scaler = self.app.scalers[s_id]

        # --- SAFETY: FREEZE SENSES ---
        frozen_layers = []
        try:
            if hasattr(student, 'vis_emb'):
                for p in student.vis_emb.parameters(): p.requires_grad = False
                frozen_layers.append("Vision")
            if hasattr(student, 'aud_emb'):
                for p in student.aud_emb.parameters(): p.requires_grad = False
                frozen_layers.append("Audio")
        except:
            pass

        msg_freeze = f"Sensors Locked: {', '.join(frozen_layers)}" if frozen_layers else "No Sensors Found"
        self.parent.after(0, lambda: self._log(f"Safety Protocol: {msg_freeze}", "warn"))

        student.train()
        teacher.eval()

        try:
            student_vocab_limit = student.tok_emb.weight.shape[0]
        except:
            student_vocab_limit = 50257

        prompts = [
            "Explain the concept of", "The history of", "Why is", "How does",
            "Describe the function of", "A summary of", "Write a story about",
            "Define the term", "Compare and contrast", "What happens if"
        ]

        try:
            t_tok = getattr(teacher, 'tokenizer', self.app.ribosome.tokenizer)
            s_tok = self.app.ribosome.tokenizer
        except:
            self.parent.after(0, lambda: self._log("Error: Could not locate tokenizers.", "err"))
            self.is_running = False
            return

        cycles = 0

        # Init Harvest File
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        harvest_base = f"symbiosis_session_{timestamp}"
        harvest_part = 1
        harvest_path = os.path.join(self.harvest_dir, f"{harvest_base}_p{harvest_part}.txt")

        while not self.stop_requested:
            try:
                # 1. Generate
                import random
                prompt = random.choice(prompts)

                generated_text = ""
                if hasattr(teacher, "generate"):
                    generated_text = teacher.generate(prompt, max_new_tokens=64)
                else:
                    generated_text = "The neural network is learning to dream."

                if not generated_text: generated_text = "Empty response."

                # 2. Translate
                try:
                    s_tokens = s_tok.encode(generated_text)
                except:
                    if hasattr(s_tok, '__call__'):
                        s_tokens = s_tok(generated_text)['input_ids']
                    else:
                        continue

                # 3. Clamp
                safe_tokens = [t % student_vocab_limit for t in s_tokens]
                t_input = torch.tensor([safe_tokens]).to(self.app.device)

                if t_input.shape[1] < 2: continue

                # 4. Student Learns
                inp = t_input[:, :-1]
                tgt = t_input[:, 1:]

                v = torch.randn(1, 1, 768).to(self.app.device) * 0.01
                a = torch.randn(1, 1, 128).to(self.app.device) * 0.01
                c = torch.zeros(1, 1, 64).to(self.app.device)

                opt.zero_grad()

                with autocast():
                    logits, _, _ = student(v, a, inp, c)
                    offset = (v.shape[1]) + (a.shape[1]) + (c.shape[1])
                    logits_text = logits[:, offset: offset + inp.shape[1], :]
                    loss = F.cross_entropy(logits_text.reshape(-1, logits_text.size(-1)), tgt.reshape(-1))

                scaler.scale(loss).backward()
                scaler.unscale_(opt)
                torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)
                scaler.step(opt)
                scaler.update()

                cycles += 1

                # --- LOGGING ---
                clean_text = generated_text.replace('\n', ' ').strip()
                if len(clean_text) > 60: clean_text = clean_text[:60] + "..."
                self.parent.after(0, lambda l=loss.item(), t=clean_text: self._log(f"Loss: {l:.4f} | {t}", "gen"))

                # --- HARVEST & ROTATION ---
                if self.harvest_enabled.get():
                    try:
                        # Check file size (Rotation)
                        if os.path.exists(harvest_path) and os.path.getsize(harvest_path) > self.max_file_size:
                            harvest_part += 1
                            harvest_path = os.path.join(self.harvest_dir, f"{harvest_base}_p{harvest_part}.txt")
                            self.parent.after(0, lambda p=harvest_part: self._log(f"HARVEST: Rotating to Part {p}",
                                                                                  "harvest"))

                        with open(harvest_path, "a", encoding="utf-8") as f:
                            f.write(generated_text + "\n<|endoftext|>\n")
                    except Exception as e:
                        print(f"Harvest Error: {e}")

                # --- AUTO-SAVE BRAIN ---
                if self.autosave_enabled.get():
                    interval = self.save_interval.get()
                    if interval > 0 and cycles % interval == 0:
                        save_path = os.path.join(self.app.paths['lobes'], f"brain_lobe_{s_id}.pt")
                        genome = self.app.lobe_genomes.get(s_id, "Unknown")
                        torch.save({"genome": genome, "state_dict": student.state_dict()}, save_path)
                        self.parent.after(0, lambda c=cycles: self._log(f"AUTO-SAVE: Lobe {s_id} saved at cycle {c}",
                                                                        "save"))

                time.sleep(0.05)

            except Exception as e:
                print(e)
                self.parent.after(0, lambda: self._log("Sync Glitch (Skipping Cycle)", "warn"))
                time.sleep(1)

        # --- RESTORE SENSES ---
        try:
            if hasattr(student, 'vis_emb'):
                for p in student.vis_emb.parameters(): p.requires_grad = True
            if hasattr(student, 'aud_emb'):
                for p in student.aud_emb.parameters(): p.requires_grad = True
        except:
            pass

        self.is_running = False
        self.parent.after(0, lambda: self.btn_start.config(text="INITIATE SYMBIOSIS", state="normal"))
        self.parent.after(0, lambda: self._log("Link Severed. Senses Unlocked.", "info"))

--- FILE: Plugins\tab_trainer.py ---
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import threading
import os
import time
import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.amp import autocast
from torch.cuda.amp import GradScaler
from datetime import datetime
import random
import queue
import json
from collections import deque
import math
import numpy as np

# --- SAFE IMPORTS ---
try:
    import fitz  # PyMuPDF

    HAS_FITZ = True
except ImportError:
    HAS_FITZ = False
    print(" ! Transformer Trainer: Install 'pymupdf' for PDF support.")


# --- HEADLESS HELPER ---
class MockVar:
    def __init__(self, value=None): self._val = value

    def set(self, value): self._val = value

    def get(self): return self._val


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Transformer Trainer"
        self.is_training = False
        self.is_paused = False
        self.stop_requested = False
        self.training_queue = []
        self.all_scanned_packets = []
        self.train_ext_vars = {}
        self.train_type_vars = {}
        self.data_queue = queue.Queue(maxsize=5)

        self.state_file = os.path.join(self.app.paths["root"], "trainer_state.json")
        self.history_file = os.path.join(self.app.paths["root"], "trainer_history.json")
        self.recent_folders = self._load_json(self.history_file, [])
        default_folder = self.recent_folders[0] if self.recent_folders else "D:/Training_Data"
        # Telepathy/Config override
        if hasattr(self.app.paths, "data") and os.path.exists(self.app.paths["data"]):
            default_folder = self.app.paths["data"]

        # --- INITIALIZE VARIABLES (GUI vs HEADLESS) ---
        if self.parent is None:
            # Headless Mode
            self.folder_path = MockVar(default_folder)
            self.target_epochs = MockVar(1)
            self.current_epoch_var = MockVar("0")
            self.auto_scroll = MockVar(False)
            self.narrative_mode = MockVar(True)
            self.autosave_enabled = MockVar(True)
            self.autosave_interval = MockVar(100)
            self.nursery_autofit = MockVar(True)
            self.use_game_loss = MockVar(False)
            self.use_fp32 = MockVar(False)
            self.nurse_pred = [MockVar(0.1), MockVar(2.5), MockVar(True)]
            self.nurse_game = [MockVar(0.001), MockVar(15.0), MockVar(True)]
            self.nurse_aux = [MockVar(1.0), MockVar(5.0), MockVar(False)]
            self.num_workers = MockVar(4)
        else:
            # GUI Mode
            self.folder_path = tk.StringVar(value=default_folder)
            self.target_epochs = tk.IntVar(value=1)
            self.current_epoch_var = tk.StringVar(value="0")
            self.auto_scroll = tk.BooleanVar(value=True)
            self.narrative_mode = tk.BooleanVar(value=True)
            self.autosave_enabled = tk.BooleanVar(value=True)
            self.autosave_interval = tk.IntVar(value=100)
            self.nursery_autofit = tk.BooleanVar(value=True)
            self.use_game_loss = tk.BooleanVar(value=False)
            self.use_fp32 = tk.BooleanVar(value=False)
            self.nurse_pred = [tk.DoubleVar(value=0.1), tk.DoubleVar(value=2.5), tk.BooleanVar(value=True)]
            self.nurse_game = [tk.DoubleVar(value=0.001), tk.DoubleVar(value=15.0), tk.BooleanVar(value=True)]
            self.nurse_aux = [tk.DoubleVar(value=1.0), tk.DoubleVar(value=5.0), tk.BooleanVar(value=False)]
            self.num_workers = tk.IntVar(value=4)

        self.processed_count = 0
        self.total_items = 0
        self.loss_history = {
            'pred': deque(maxlen=10000),
            'game': deque(maxlen=10000),
            'aux': deque(maxlen=10000)
        }

        self._setup_ui()
        if self.parent:
            self.parent.after(1000, self._load_session_state)

    def _setup_ui(self):
        if self.parent is None: return

        style = ttk.Style()
        style.map('TCombobox', fieldbackground=[('readonly', self.app.colors['BG_CARD'])],
                  selectbackground=[('readonly', self.app.colors['BG_CARD'])],
                  selectforeground=[('readonly', self.app.colors['FG_TEXT'])],
                  foreground=[('readonly', self.app.colors['FG_TEXT'])])

        split = ttk.PanedWindow(self.parent, orient="horizontal")
        split.pack(fill="both", expand=True, padx=10, pady=10)

        left = ttk.Frame(split)
        right = ttk.Frame(split)
        split.add(left, weight=3)
        split.add(right, weight=1)

        # --- DATA FEED ---
        fr_src = ttk.LabelFrame(left, text="Data Feed", padding=10)
        fr_src.pack(fill="x", pady=5)

        row_src = ttk.Frame(fr_src)
        row_src.pack(fill="x", expand=True)
        self.cmb_folder = ttk.Combobox(row_src, textvariable=self.folder_path, values=self.recent_folders)
        self.cmb_folder.pack(side="left", fill="x", expand=True)
        self.cmb_folder.bind("<<ComboboxSelected>>", lambda e: self._scan_files())
        ttk.Button(row_src, text="📂", width=3, command=self._browse_folder).pack(side="left", padx=2)

        row_btns_src = ttk.Frame(fr_src, padding=(0, 5, 0, 0))
        row_btns_src.pack(fill="x")
        ttk.Button(row_btns_src, text="SCAN FOLDER", command=self._scan_files).pack(side="left", padx=2, fill="x",
                                                                                    expand=True)
        ttk.Button(row_btns_src, text="CLEAR QUEUE", command=self._clear_queue).pack(side="left", padx=2, fill="x",
                                                                                     expand=True)
        ttk.Button(row_btns_src, text="RESET SESSION", command=self._reset_session).pack(side="left", padx=2, fill="x",
                                                                                         expand=True)

        # --- CONTROLS ---
        fr_ctrl = ttk.LabelFrame(left, text="Training Operations", padding=10)
        fr_ctrl.pack(fill="x", pady=5)

        row_btns = ttk.Frame(fr_ctrl)
        row_btns.pack(fill="x", pady=(0, 5))
        self.btn_start = ttk.Button(row_btns, text="START TRAINING", command=self._start_training)
        self.btn_start.pack(side="left", fill="x", expand=True, padx=2)
        self.btn_pause = ttk.Button(row_btns, text="PAUSE", command=self._toggle_pause, state="disabled")
        self.btn_pause.pack(side="left", fill="x", expand=True, padx=2)

        row_sets = ttk.Frame(fr_ctrl)
        row_sets.pack(fill="x", pady=2)
        ttk.Label(row_sets, text="Epochs:").pack(side="left")
        ttk.Spinbox(row_sets, from_=1, to=999, textvariable=self.target_epochs, width=4).pack(side="left")
        ttk.Label(row_sets, text="Workers:").pack(side="left", padx=(10, 0))
        ttk.Scale(row_sets, from_=1, to=16, variable=self.num_workers, orient="horizontal", length=80).pack(side="left",
                                                                                                            padx=5)
        ttk.Checkbutton(row_sets, text="Narrative Mode", variable=self.narrative_mode).pack(side="left", padx=15)
        ttk.Label(row_sets, textvariable=self.current_epoch_var, foreground=self.app.colors["ACCENT"]).pack(
            side="right", padx=10)

        # --- NURSERY ---
        fr_nurse = ttk.LabelFrame(fr_ctrl, text="Nursery (Safety Corridor)", padding=5)
        fr_nurse.pack(fill="x", pady=5)

        h_row = ttk.Frame(fr_nurse)
        h_row.pack(fill="x")
        ttk.Checkbutton(h_row, text="Auto-Fit", variable=self.nursery_autofit).pack(side="left", padx=5)
        ttk.Checkbutton(h_row, text="Game Loss", variable=self.use_game_loss).pack(side="left", padx=10)
        ttk.Checkbutton(h_row, text="Force FP32", variable=self.use_fp32).pack(side="left", padx=10)

        grid = ttk.Frame(fr_nurse)
        grid.pack(fill="x", pady=5)
        ttk.Label(grid, text="Channel", font=("Segoe UI", 8, "bold")).grid(row=0, column=0, padx=5)
        ttk.Label(grid, text="Active", font=("Segoe UI", 8)).grid(row=0, column=1, padx=2)
        ttk.Label(grid, text="Min Loss", font=("Segoe UI", 8)).grid(row=0, column=2, padx=2)
        ttk.Label(grid, text="Max Loss", font=("Segoe UI", 8)).grid(row=0, column=3, padx=2)

        def add_row(r, label, vars):
            ttk.Label(grid, text=label).grid(row=r, column=0, sticky="e", padx=5)
            ttk.Checkbutton(grid, variable=vars[2]).grid(row=r, column=1, padx=2)
            ttk.Entry(grid, textvariable=vars[0], width=6).grid(row=r, column=2, padx=2)
            ttk.Entry(grid, textvariable=vars[1], width=6).grid(row=r, column=3, padx=2)

        add_row(1, "Prediction:", self.nurse_pred)
        add_row(2, "Game / Vis:", self.nurse_game)
        add_row(3, "Aux / Aud:", self.nurse_aux)

        # --- LOGS ---
        fr_log = ttk.LabelFrame(left, text="Neural Logs", padding=10)
        fr_log.pack(fill="both", expand=True, pady=5)

        log_head = ttk.Frame(fr_log)
        log_head.pack(fill="x")
        ttk.Checkbutton(log_head, text="Autoscroll", variable=self.auto_scroll).pack(side="right")

        self.log_box = tk.Text(fr_log, font=("Consolas", 9), height=10, bg=self.app.colors["BG_MAIN"],
                               fg=self.app.colors["FG_TEXT"], borderwidth=0)
        self.log_box.pack(side="left", fill="both", expand=True)

        sb = ttk.Scrollbar(fr_log, orient="vertical", command=self.log_box.yview)
        self.log_box.configure(yscrollcommand=sb.set)
        sb.pack(side="right", fill="y")

        self.log_box.tag_config("info", foreground=self.app.colors["ACCENT"])
        self.log_box.tag_config("error", foreground=self.app.colors["ERROR"])
        self.log_box.tag_config("warn", foreground=self.app.colors["WARN"])
        self.log_box.tag_config("success", foreground=self.app.colors["SUCCESS"])
        self.log_box.tag_config("prog", foreground=self.app.colors["FG_DIM"])
        self.log_box.tag_config("scale", foreground="#FDD663")
        self.log_box.tag_config("save", foreground="#81C995", font=("Consolas", 9, "bold"))

        # --- CENSUS ---
        fr_census = ttk.LabelFrame(right, text="Census", padding=10)
        fr_census.pack(fill="both", expand=True)

        self.canvas = tk.Canvas(fr_census, width=200, bg=self.app.colors["BG_MAIN"], highlightthickness=0)
        scr = ttk.Scrollbar(fr_census, orient="vertical", command=self.canvas.yview)
        self.scroll_fr = ttk.Frame(self.canvas, style="Card.TFrame")
        self.scroll_fr.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))
        self.canvas.create_window((0, 0), window=self.scroll_fr, anchor="nw")
        self.canvas.configure(yscrollcommand=scr.set)

        self.canvas.pack(side="left", fill="both", expand=True)
        scr.pack(side="right", fill="y")

    def _load_json(self, path, default):
        if os.path.exists(path):
            try:
                return json.load(open(path, 'r'))
            except:
                pass
        return default

    def _save_history(self):
        curr = self.folder_path.get()
        if curr in self.recent_folders: self.recent_folders.remove(curr)
        self.recent_folders.insert(0, curr)
        try:
            json.dump(self.recent_folders[:10], open(self.history_file, 'w'))
        except:
            pass

    def _save_session_state(self):
        state = {
            "processed_count": self.processed_count,
            "total_items": self.total_items,
            "loss_history": {k: list(v) for k, v in self.loss_history.items()},
            "graph_data": self.app.graph_data,
            "folder": self.folder_path.get(),
            "target_epochs": self.target_epochs.get()
        }
        try:
            with open(self.state_file, 'w') as f:
                json.dump(state, f)
        except Exception as e:
            print(f"State Save Error: {e}")

    def _load_session_state(self):
        if not os.path.exists(self.state_file): return
        try:
            state = json.load(open(self.state_file, 'r'))
            self.folder_path.set(state.get("folder", "D:/Training_Data"))
            self.target_epochs.set(state.get("target_epochs", 1))
            self.processed_count = state.get("processed_count", 0)
            self.total_items = state.get("total_items", 0)
            hist = state.get("loss_history", {})
            for k, v in hist.items():
                if k in self.loss_history: self.loss_history[k] = deque(v, maxlen=10000)
            g_data = state.get("graph_data", {})
            self.app.graph_data = {int(k): v for k, v in g_data.items()}
            self._log(f"Session Restored: {self.processed_count}/{self.total_items} items.", "info")
            if 'tab_graphs' in self.app.plugins: self.app.plugins['tab_graphs']._update_graphs()
        except Exception as e:
            self._log(f"Session Restore Failed: {e}", "error")

    def _reset_session(self):
        if messagebox.askyesno("Reset Session", "Clear training progress and graphs?"):
            self.processed_count = 0
            self.total_items = 0
            self.loss_history['pred'].clear()
            self.loss_history['game'].clear()
            self.loss_history['aux'].clear()
            self.app.graph_data = {}
            if os.path.exists(self.state_file): os.remove(self.state_file)
            self._log("Session Cleared.", "warn")
            if 'tab_graphs' in self.app.plugins: self.app.plugins['tab_graphs']._update_graphs()

    def _clear_queue(self):
        self.training_queue = []
        self.all_scanned_packets = []
        self.train_ext_vars = {}
        self.train_type_vars = {}
        while not self.data_queue.empty():
            try:
                self.data_queue.get_nowait()
            except:
                break

        if self.parent:
            for w in self.scroll_fr.winfo_children(): w.destroy()

        self._log("Queue and Buffer Cleared.", "warn")

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'log_box'): self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])
        if hasattr(self, 'canvas'): self.canvas.config(bg=c["BG_MAIN"])

    def _browse_folder(self):
        d = filedialog.askdirectory()
        if d: self.folder_path.set(d); self._save_history(); self._scan_files()

    def _log(self, msg, tag="info"):
        if self.parent is None:
            # Headless logging
            print(f"[{tag.upper()}] {msg}")
        else:
            # GUI logging
            def _update():
                self.log_box.insert(tk.END, f"[{datetime.now().strftime('%H:%M:%S')}] {msg}\n", tag)
                if self.auto_scroll.get(): self.log_box.see(tk.END)

            self.parent.after(0, _update)

    def _toggle_pause(self):
        self.is_paused = not self.is_paused
        if self.parent:
            self.btn_pause.config(text="RESUME" if self.is_paused else "PAUSE")

    def _scan_files(self):
        folder = self.folder_path.get()
        if not os.path.exists(folder): self._log("Folder not found.", "error"); return

        self._clear_queue()
        self._log(f"Scanning {folder}...", "info")

        # --- EXTENSION MAP ---
        ext_map = {
            'v': {'.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'},
            'a': {'.mp3', '.wav', '.flac', '.ogg', '.m4a', '.aac'},
            'c': {'.json', '.csv', '.ctl'},
            't': {'.pdf', '.txt', '.md', '.doc', '.docx', '.html', '.xml', '.py', '.js', '.c', '.cpp', '.h', '.srt',
                  '.vtt', '.ass'},
            'vid': {'.mp4', '.mkv', '.avi', '.mov'}
        }
        all_valid_exts = set().union(*ext_map.values())
        file_sets = {}
        ext_counts = {}

        for root, _, fs in os.walk(folder):
            for f in fs:
                base, ext = os.path.splitext(f)
                lext = ext.lower()
                if lext in all_valid_exts:
                    key = os.path.join(root, base)
                    if key not in file_sets: file_sets[key] = {}

                    if lext in ext_map['v']:
                        file_sets[key]['v'] = os.path.join(root, f)
                    elif lext in ext_map['a']:
                        file_sets[key]['a'] = os.path.join(root, f)
                    elif lext in ext_map['c']:
                        file_sets[key]['c'] = os.path.join(root, f)
                    elif lext in ext_map['t']:
                        file_sets[key]['t'] = os.path.join(root, f)
                    elif lext in ext_map['vid']:
                        file_sets[key]['vid'] = os.path.join(root, f)

        q, tr, p, s = 0, 0, 0, 0
        sorted_keys = sorted(file_sets.keys())

        for key in sorted_keys:
            packet = file_sets[key].copy()

            # --- EMPTY TEXT CHECK ---
            if 't' in packet:
                try:
                    if os.path.getsize(packet['t']) < 10:
                        del packet['t']
                except:
                    pass

            has_v, has_a, has_t, has_c = 'v' in packet, 'a' in packet, 't' in packet, 'c' in packet
            has_vid = 'vid' in packet

            if (has_vid and has_t) or (has_v and has_a and has_t):
                packet['type'] = 'triplet'
                self.all_scanned_packets.append(packet)
                tr += 1;
                continue

            if has_v and has_a and has_t and has_c:
                packet['type'] = 'quad'
                self.all_scanned_packets.append(packet)
                q += 1;
                continue

            if (has_v and has_t) or (has_a and has_t) or (has_v and has_a):
                packet['type'] = 'pair'
                self.all_scanned_packets.append(packet)
                p += 1;
                continue

            # Singles logic
            if has_vid:
                if 'vid' in packet:
                    path = packet['vid']
                    _, e = os.path.splitext(path)
                    self.all_scanned_packets.append({'type': 'single', 'vid': path, 'ext': e.lower()})
                    ext_counts[e.lower()] = ext_counts.get(e.lower(), 0) + 1
                    s += 1
                continue

            for k, path in file_sets[key].items():
                _, e = os.path.splitext(path)
                lext = e.lower()
                self.all_scanned_packets.append({'type': 'single', k: path, 'ext': lext})
                ext_counts[lext] = ext_counts.get(lext, 0) + 1
                s += 1

        # GUI Update Logic
        if self.parent:
            row = 0

            def add_chk(text, var_key, container=self.train_type_vars):
                var = tk.BooleanVar(value=True);
                container[var_key] = var
                ttk.Checkbutton(self.scroll_fr, text=text, variable=var).grid(row=row, column=0, sticky="w")

            if q > 0: add_chk(f"Quadruplets ({q})", 'quad'); row += 1
            if tr > 0: add_chk(f"Triplets ({tr})", 'triplet'); row += 1
            if p > 0: add_chk(f"Pairs ({p})", 'pair'); row += 1
            if row > 0: ttk.Separator(self.scroll_fr, orient='horizontal').grid(row=row, column=0, sticky="ew",
                                                                                pady=5); row += 1
            for ext in sorted(ext_counts.keys()):
                add_chk(f"{ext} ({ext_counts[ext]})", ext, self.train_ext_vars);
                row += 1

            self.scroll_fr.update_idletasks()
            self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        else:
            # Headless: Enable all found
            self.train_type_vars['quad'] = MockVar(True)
            self.train_type_vars['triplet'] = MockVar(True)
            self.train_type_vars['pair'] = MockVar(True)
            for ext in ext_counts.keys():
                self.train_ext_vars[ext] = MockVar(True)

        self._log(f"Scan: {s} Single, {p} Pair, {tr} Trip, {q} Quad.", "success")
        if s + p + tr + q > 0: self._save_history()

    def _start_training(self):
        if self.is_training: self.stop_requested = True; return
        active = self.app.active_lobe.get()
        if self.app.lobes[active] is None: self._log("No Lobe Loaded for training.", "error"); return

        # --- SAFETY CHECK: MODEL TYPE ---
        lobe_type = self.app.lobe_types.get(active)
        if lobe_type == "diffusion":
            if self.parent:
                messagebox.showwarning("Mismatch",
                                       "Transformer Trainer cannot train Diffusion lobes.\nPlease switch to the Diffusion Director plugin.")
            else:
                print("Error: Model type mismatch (Diffusion in AR trainer)")
            return

        while not self.data_queue.empty():
            try:
                self.data_queue.get_nowait()
            except:
                break

        self.training_queue = []

        # Handle MockVar vs BooleanVar for headless safety
        def get_val(v):
            return v.get() if v else False

        active_exts = {e for e, v in self.train_ext_vars.items() if get_val(v)}
        t_quad = get_val(self.train_type_vars.get('quad'))
        t_trip = get_val(self.train_type_vars.get('triplet'))
        t_pair = get_val(self.train_type_vars.get('pair'))

        for p in self.all_scanned_packets:
            pt = p['type']
            if pt == 'quad' and t_quad:
                self.training_queue.append(p)
            elif pt == 'triplet' and t_trip:
                self.training_queue.append(p)
            elif pt == 'pair' and t_pair:
                self.training_queue.append(p)
            elif pt == 'single' and p.get('ext') in active_exts:
                self.training_queue.append(p)

        if not self.training_queue: self._log("Queue Empty.", "error"); return

        if self.narrative_mode.get():
            # Narrative Sort
            self.training_queue.sort(key=lambda x: x.get('t', x.get('vid', x.get('v', x.get('a', '')))))

        self.total_items = len(self.training_queue) * self.target_epochs.get()
        self.is_training = True;
        self.stop_requested = False

        if self.parent:
            self.btn_start.config(text="STOP")
            self.btn_pause.config(state="normal")

        threading.Thread(target=self._prefetch_worker, daemon=True).start()
        threading.Thread(target=self._training_worker, daemon=True).start()

    def _prefetch_worker(self):
        epochs = self.target_epochs.get()
        for _ in range(epochs):
            if not self.narrative_mode.get(): random.shuffle(self.training_queue)
            for packet in self.training_queue:
                if self.stop_requested: return
                try:
                    data = self.app.ribosome.ingest_packet(packet)
                    self.data_queue.put((packet, data))
                except:
                    continue
        self.data_queue.put(None)

    def _training_worker(self):
        def dampen_value(current, target, limit=0.1):
            delta = target - current
            max_delta = abs(current * limit)
            safe_delta = max(-max_delta, min(max_delta, delta))
            return current + safe_delta

        try:
            active_id = self.app.active_lobe.get()
            brain = self.app.lobes[active_id]
            opt = self.app.optimizers[active_id]
            scaler = self.app.scalers[active_id]
            brain.train()

            if self.processed_count >= self.total_items: self.processed_count = 0

            while True:
                if self.stop_requested: break
                item = self.data_queue.get()
                if item is None: break
                packet, (v, a, t, c, _) = item
                if t is None or t.size(1) < 2: continue

                if self.parent:
                    self.parent.update_idletasks()
                while self.is_paused: time.sleep(0.1);
                if self.parent: self.parent.update()

                input_t = t[:, :-1]
                labels = t[:, 1:]

                # Noise Injection
                if c is not None: c = c + torch.randn_like(c) * 1e-5
                if a is not None: a = a + torch.randn_like(a) * 1e-5
                if v is not None: v = v + torch.randn_like(v) * 1e-5

                with self.app.gpu_lock:
                    opt.zero_grad()
                    use_amp = not self.use_fp32.get()

                    with autocast('cuda', enabled=use_amp):
                        try:
                            logits, _, _ = brain(v, a, input_t, c)
                        except:
                            logits, _, _ = brain(v, a, input_t)

                        offset = (v.shape[1] if v is not None else 0) + (a.shape[1] if a is not None else 0) + (
                            c.shape[1] if c is not None else 0)
                        if logits.shape[1] == input_t.shape[1]: offset = 0
                        logits_text = logits[:, offset: offset + input_t.shape[1], :]
                        loss_txt = F.cross_entropy(logits_text.reshape(-1, logits_text.size(-1)), labels.reshape(-1),
                                                   ignore_index=50256)

                        game_penalty = torch.tensor(0.0, device=self.app.device)
                        has_game = False
                        if self.use_game_loss.get():
                            for module in brain.modules():
                                if hasattr(module, 'game_loss'):
                                    game_penalty += module.game_loss()
                                    has_game = True

                        raw_pred = loss_txt.item()
                        raw_b = game_penalty.item() if has_game else 0.0

                        if any(math.isnan(x) or math.isinf(x) for x in [raw_pred, raw_b]):
                            fname = os.path.basename(packet.get('t', 'Unknown'))
                            self._log(f"NaN Loss in {fname}. Skipping.", "warn")
                            continue

                        self.loss_history['pred'].append(raw_pred)
                        if raw_b > 0: self.loss_history['game'].append(raw_b)

                        # Nursery Auto-Fit
                        if self.nursery_autofit.get() and self.processed_count % 100 == 0:
                            p_vals = [x for x in self.loss_history['pred'] if not math.isnan(x)]
                            if len(p_vals) > 50:
                                p_max = dampen_value(self.nurse_pred[1].get(), np.percentile(p_vals, 99) * 2.0)
                                self.nurse_pred[1].set(round(p_max, 4))

                            g_vals = [x for x in self.loss_history['game'] if not math.isnan(x)]
                            if len(g_vals) > 50:
                                g_max = dampen_value(self.nurse_game[1].get(), np.percentile(g_vals, 99) * 1.5)
                                self.nurse_game[1].set(round(g_max, 4))

                        def get_scaler(raw, settings):
                            low, high, is_active = settings[0].get(), settings[1].get(), settings[2].get()
                            if not is_active: return 1.0, raw
                            if abs(raw) < 1e-9: return 1.0, raw
                            if raw < low:
                                return (low / raw), low
                            elif raw > high:
                                return (high / raw), high
                            return 1.0, raw

                        s_pred, graph_pred = get_scaler(raw_pred, self.nurse_pred)
                        s_game, graph_b = get_scaler(raw_b, self.nurse_game)

                        loss = (loss_txt * s_pred)
                        if has_game: loss += (game_penalty * s_game)

                    # Backward
                    scaler.scale(loss).backward()
                    scaler.unscale_(opt)

                    # Gradient Check
                    total_norm = torch.nn.utils.clip_grad_norm_(brain.parameters(), 1.0)
                    if math.isnan(total_norm) or math.isinf(total_norm):
                        self._log("Warn: Gradient Explosion (clipped)", "warn")
                        # Zero out broken grads
                        for p in brain.parameters():
                            if p.grad is not None:
                                torch.nan_to_num(p.grad, nan=0.0, posinf=0.0, neginf=0.0, out=p.grad)

                    scaler.step(opt)
                    scaler.update()

                self.processed_count += 1
                val = loss.item()
                ep = (self.processed_count // len(self.training_queue)) + 1 if len(self.training_queue) > 0 else 1

                if ep not in self.app.graph_data:
                    self.app.graph_data[ep] = {'total': [], 'text': [], 'vis': [], 'aud': [], 'raw_total': [],
                                               'raw_text': [], 'raw_vis': [], 'raw_aud': []}

                self.app.graph_data[ep]['total'].append(val)
                self.app.graph_data[ep]['text'].append(graph_pred)
                self.app.graph_data[ep]['vis'].append(graph_b)
                self.app.graph_data[ep]['raw_total'].append(raw_pred + raw_b)
                self.app.graph_data[ep]['raw_text'].append(raw_pred)
                self.app.graph_data[ep]['raw_vis'].append(raw_b)

                if self.autosave_enabled.get() and self.processed_count % self.autosave_interval.get() == 0:
                    self._save_session_state()
                    save_path = os.path.join(self.app.paths['lobes'], f"brain_lobe_{active_id}.pt")
                    try:
                        torch.save({
                            "genome": self.app.lobe_genomes.get(active_id, "Unknown"),
                            "model_type": self.app.lobe_types.get(active_id, "ar"),  # Save Type
                            "state_dict": brain.state_dict()
                        }, save_path)
                        # Only update GUI log if parent exists, else print
                        msg = f"[SAVE] Auto-saved at step {self.processed_count}"
                        if self.parent:
                            self.parent.after(0, lambda: self._log(msg, "save"))
                        else:
                            print(msg)
                    except:
                        pass

                if self.processed_count % 10 == 0:
                    pct = int((self.processed_count / self.total_items) * 100) if self.total_items > 0 else 0
                    name = os.path.basename(packet.get('t', 'Unknown'))
                    game_stat = f" | Game:{graph_b:.2f}" if has_game else ""
                    msg = f"[{self.processed_count}] ({pct}%) {name} | Tot:{val:.2f} | Pred:{graph_pred:.2f}{game_stat}"
                    if self.parent:
                        self.parent.after(0, lambda m=msg: self._log(m, "prog"))
                    else:
                        print(msg)

            if self.parent:
                self.parent.after(0, lambda: self._log("Training Complete.", "success"))
            else:
                print("Training Complete.")
            self._save_session_state()

        except Exception as e:
            if self.parent:
                self.parent.after(0, lambda m=f"CRASH: {e}": self._log(m, "error"))
            else:
                print(f"CRASH: {e}")
            import traceback;
            traceback.print_exc()
        finally:
            self.is_training = False
            if self.parent:
                self.parent.after(0, lambda: self.btn_start.config(text="START TRAINING"))
                self.parent.after(0, lambda: self.btn_pause.config(state="disabled"))

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'log_box'): self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])
        if hasattr(self, 'canvas'): self.canvas.config(bg=c["BG_MAIN"])

--- FILE: Plugins\tab_video_factory.py ---
# FILE: Plugins/tab_video_factory.py
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import threading
import os
import asyncio
from datetime import datetime
import cv2
import numpy as np
import torch
import torchaudio
from PIL import Image
import subprocess

# --- ROBUST IMPORTS ---
try:
    # MoviePy handles audio extraction safely on Windows
    from moviepy.editor import AudioFileClip

    HAS_MOVIEPY = True
except ImportError:
    HAS_MOVIEPY = False
    print(" ! Install: pip install moviepy")

try:
    # We use imageio's ffmpeg binary because we know it exists if moviepy is installed
    import imageio_ffmpeg

    FFMPEG_EXE = imageio_ffmpeg.get_ffmpeg_exe()
except ImportError:
    FFMPEG_EXE = "ffmpeg"  # Hope it's in PATH

try:
    from pysrt import SubRipFile

    HAS_PYSRT = True
except ImportError:
    HAS_PYSRT = False
    print(" ! Install: pip install pysrt")


class Plugin:
    def __init__(self, parent, app):
        self.parent = parent
        self.app = app
        self.name = "Video Timeline Factory"
        self.factory_running = False
        self.factory_stop = False
        self.factory_queue = []
        self.folder_path = tk.StringVar(value="D:/Training_Data")
        self.skip_existing = tk.BooleanVar(value=True)
        self.auto_scroll = tk.BooleanVar(value=True)

        # Slicing Configuration
        self.target_fps = tk.DoubleVar(value=2.0)
        self.audio_seconds = tk.DoubleVar(value=2.0)

        self._setup_ui()

    def _setup_ui(self):
        top = ttk.LabelFrame(self.parent, text="Video Timeline Production (v3 - Embedded Subs)", padding=15)
        top.pack(fill="x", padx=20, pady=10)

        row1 = ttk.Frame(top)
        row1.pack(fill="x", pady=5)
        ttk.Entry(row1, textvariable=self.folder_path).pack(side="left", fill="x", expand=True, padx=5)
        ttk.Button(row1, text="📂 Choose Folder", command=self._browse).pack(side="left", padx=2)
        ttk.Button(row1, text="🔍 Scan Videos", command=self._scan_videos).pack(side="left", padx=2)

        row2 = ttk.Frame(top)
        row2.pack(fill="x", pady=5)
        ttk.Label(row2, text="Extract FPS:").pack(side="left")
        ttk.Spinbox(row2, from_=0.5, to=30.0, increment=0.5, textvariable=self.target_fps, width=5).pack(side="left",
                                                                                                         padx=5)

        ttk.Label(row2, text="| Audio Context (Sec):").pack(side="left", padx=10)
        ttk.Spinbox(row2, from_=0.5, to=5.0, increment=0.5, textvariable=self.audio_seconds, width=5).pack(side="left",
                                                                                                           padx=5)

        row3 = ttk.Frame(top)
        row3.pack(fill="x", pady=5)
        self.btn_start = ttk.Button(row3, text="START PRODUCTION", command=self._start_factory, state="disabled")
        self.btn_start.pack(side="left", fill="x", expand=True, padx=2)
        ttk.Checkbutton(row3, text="Skip Existing Slices", variable=self.skip_existing).pack(side="right", padx=10)

        log_frame = ttk.LabelFrame(self.parent, text="Factory Log", padding=15)
        log_frame.pack(fill="both", expand=True, padx=20, pady=10)

        tool_fr = ttk.Frame(log_frame)
        tool_fr.pack(fill="x")
        ttk.Checkbutton(tool_fr, text="Autoscroll", variable=self.auto_scroll).pack(side="right")

        self.log_box = tk.Text(log_frame, font=("Consolas", 9), bg=self.app.colors["BG_MAIN"],
                               fg=self.app.colors["FG_TEXT"], borderwidth=0)
        self.log_box.pack(side="left", fill="both", expand=True)

        sb = ttk.Scrollbar(log_frame, orient="vertical", command=self.log_box.yview)
        self.log_box.configure(yscrollcommand=sb.set)
        sb.pack(side="right", fill="y")

        self.log_box.tag_config("info", foreground=self.app.colors["ACCENT"])
        self.log_box.tag_config("success", foreground=self.app.colors["SUCCESS"])
        self.log_box.tag_config("error", foreground=self.app.colors["ERROR"])
        self.log_box.tag_config("warn", foreground=self.app.colors["WARN"])

    def _browse(self):
        d = filedialog.askdirectory()
        if d: self.folder_path.set(d)

    def _log(self, msg, tag="info"):
        ts = datetime.now().strftime('%H:%M:%S')
        self.log_box.insert(tk.END, f"[{ts}] {msg}\n", tag)
        if self.auto_scroll.get(): self.log_box.see(tk.END)

    def _scan_videos(self):
        folder = self.folder_path.get()
        if not os.path.exists(folder):
            messagebox.showerror("Error", "Folder not found")
            return

        videos = []
        for root, _, files in os.walk(folder):
            for f in files:
                if f.lower().endswith(('.mp4', '.mkv', '.avi', '.mov')):
                    videos.append(os.path.join(root, f))

        self.factory_queue = videos
        self._log(f"Found {len(videos)} video files.", "success")
        if videos: self.btn_start.config(state="normal")

    def _start_factory(self):
        if self.factory_running:
            self.factory_stop = True
            self.btn_start.config(text="STOPPING...")
            return

        if not self.factory_queue: self._scan_videos()
        if not self.factory_queue: return

        if not HAS_MOVIEPY:
            self._log("ERROR: 'moviepy' not installed. Audio extraction will fail.", "error")
            return

        self.factory_running = True
        self.factory_stop = False
        self.btn_start.config(text="STOP PRODUCTION")
        threading.Thread(target=self._factory_worker, daemon=True).start()

    def _factory_worker(self):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(self._async_pipeline())
        loop.close()

        self.factory_running = False
        self.parent.after(0, lambda: self.btn_start.config(text="START PRODUCTION"))
        self.parent.after(0, lambda: self._log("Production Run Complete.", "success"))

    async def _async_pipeline(self):
        semaphore = asyncio.Semaphore(1)
        for i, video_path in enumerate(self.factory_queue):
            if self.factory_stop: break
            name = os.path.basename(video_path)
            self.parent.after(0, lambda m=f"Processing {i + 1}/{len(self.factory_queue)}: {name}": self._log(m))
            await self._process_video(video_path, semaphore)

    async def _process_video(self, video_path, semaphore):
        async with semaphore:
            name = os.path.basename(video_path)
            temp_wav = os.path.join(os.path.dirname(video_path), "temp_extract_audio.wav")
            temp_srt = os.path.join(os.path.dirname(video_path), "temp_extract_subs.srt")

            try:
                # 1. Setup Output
                base_name = os.path.splitext(name)[0]
                output_dir = os.path.join(os.path.dirname(video_path), f"{base_name}_timeline")
                if not os.path.exists(output_dir): os.makedirs(output_dir)

                # --- 2. SUBTITLE EXTRACTION LOGIC ---
                subs = None
                sidecar_srt = os.path.splitext(video_path)[0] + ".srt"

                # A. Try Sidecar
                if os.path.exists(sidecar_srt):
                    try:
                        subs = SubRipFile.open(sidecar_srt, encoding='utf-8')
                        self.parent.after(0, lambda: self._log(f" > Found Sidecar Subtitles.", "info"))
                    except:
                        pass

                # B. Try Embedded (FFMPEG Rip)
                if not subs:
                    try:
                        # -map 0:s:0 selects the first subtitle stream
                        cmd = [FFMPEG_EXE, '-i', video_path, '-map', '0:s:0', temp_srt, '-y']
                        # Run quietly
                        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, timeout=30)

                        if os.path.exists(temp_srt) and os.path.getsize(temp_srt) > 0:
                            subs = SubRipFile.open(temp_srt, encoding='utf-8')
                            self.parent.after(0, lambda: self._log(f" > Extracted Embedded Subtitles.", "info"))
                        else:
                            self.parent.after(0, lambda: self._log(f" > No text track found.", "warn"))
                    except Exception as e:
                        self.parent.after(0, lambda: self._log(f" > Sub Extraction failed: {e}", "warn"))

                # --- 3. AUDIO EXTRACTION LOGIC ---
                self.parent.after(0, lambda: self._log(f" > Extracting full audio track...", "info"))
                try:
                    audioclip = AudioFileClip(video_path)
                    audioclip.write_audiofile(temp_wav, fps=24000, nbytes=2, verbose=False, logger=None)
                    audioclip.close()
                    waveform, sr = torchaudio.load(temp_wav)
                except Exception as ae:
                    self.parent.after(0, lambda m=str(ae): self._log(f" > Audio Extract Failed: {m}", "error"))
                    cap_check = cv2.VideoCapture(video_path)
                    fps_check = cap_check.get(cv2.CAP_PROP_FPS)
                    frames_check = int(cap_check.get(cv2.CAP_PROP_FRAME_COUNT))
                    cap_check.release()
                    waveform = torch.zeros(1, int(frames_check / fps_check * 24000) if fps_check > 0 else 24000)
                    sr = 24000

                # --- 4. SLICING LOOP ---
                cap = cv2.VideoCapture(video_path)
                fps = cap.get(cv2.CAP_PROP_FPS)
                if fps <= 0: fps = 24.0

                extract_rate = self.target_fps.get()
                frame_interval = max(1, int(fps / extract_rate))
                audio_window = self.audio_seconds.get()

                frame_cursor = 0
                slice_count = 0

                while cap.isOpened():
                    if self.factory_stop: break
                    ret, frame = cap.read()
                    if not ret: break

                    if frame_cursor % frame_interval == 0:
                        timestamp_sec = frame_cursor / fps
                        slice_id = slice_count + 1
                        out_base = os.path.join(output_dir, f"{base_name}_p{slice_id:05d}")

                        # IMAGE
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        img = Image.fromarray(frame_rgb).resize((256, 256))
                        img.save(f"{out_base}.png")

                        # AUDIO SLICE
                        center_sample = int(timestamp_sec * sr)
                        half_window = int((audio_window / 2) * sr)
                        start_s = max(0, center_sample - half_window)
                        end_s = min(waveform.shape[1], center_sample + half_window)
                        clip = waveform[:, start_s:end_s]

                        target_samples = int(audio_window * sr)
                        if clip.shape[1] < target_samples:
                            pad = target_samples - clip.shape[1]
                            clip = torch.cat([clip, torch.zeros(clip.shape[0], pad)], dim=1)

                        torchaudio.save(f"{out_base}.wav", clip, sr)

                        # TEXT SYNC (Robust PySRT check)
                        txt_out = ""
                        if subs:
                            # pysrt uses ordinal (ms) objects
                            # We check if current timestamp is within any sub's duration
                            current_ms = timestamp_sec * 1000
                            # Simple linear search (for short videos this is fine, for movies binary search is better but complex)
                            # Optimizing: search only recent subs
                            matches = []
                            for sub in subs:
                                if sub.start.ordinal <= current_ms <= sub.end.ordinal:
                                    matches.append(sub.text)
                                elif sub.start.ordinal > current_ms:
                                    break  # Sorted assumption

                            if matches:
                                txt_out = " ".join(matches).replace('\n', ' ')

                        with open(f"{out_base}.txt", 'w', encoding='utf-8') as f:
                            f.write(txt_out)

                        # CONTROL
                        with open(f"{out_base}.json", 'w', encoding='utf-8') as f:
                            f.write("{}")

                        slice_count += 1
                        if slice_count % 10 == 0:
                            self.parent.after(0, lambda n=slice_count, t=timestamp_sec:
                            self._log(f" > Slice {n} @ {t:.1f}s"))

                    frame_cursor += 1

                cap.release()

                # Cleanup
                for tmp in [temp_wav, temp_srt]:
                    if os.path.exists(tmp):
                        try:
                            os.remove(tmp)
                        except:
                            pass

                self.parent.after(0, lambda: self._log(f"Completed {name}: {slice_count} slices.", "success"))

            except Exception as e:
                self.parent.after(0, lambda m=str(e): self._log(f"CRITICAL ERROR {name}: {m}", "error"))
                import traceback
                traceback.print_exc()

    def on_theme_change(self):
        c = self.app.colors
        if hasattr(self, 'log_box'): self.log_box.config(bg=c["BG_MAIN"], fg=c["FG_TEXT"])

--- FILE: Plugins\__init__.py ---


--- FILE: system\genegrams\MASTER_HISTORY.md ---
# GENEGRAM: Evolutionary Log
**Status:** Genesis Complete.

- **2025-12-18**: Successful mutation 144614
- **2025-12-18**: Nightly cycle complete.
- **2025-12-18**: Successful mutation 144627
- **2025-12-18**: Nightly cycle complete.
- **2025-12-18**: Successful mutation 144641
- **2025-12-18**: Nightly cycle complete.
- **2025-12-18**: Successful mutation 144655
- **2025-12-18**: Nightly cycle complete.

--- FILE: system\morals\moral_core.json ---
{
  "version": "1.0",
  "is_locked": true,
  "axioms": [
    {
      "id": "AXIOM_TRUTH",
      "statement": "Prioritize Historical and Factual Accuracy over User Comfort.",
      "definitions": {
        "Accuracy": "Correspondence to primary source records and physical reality.",
        "Comfort": "Modification of data to align with user emotional preferences."
      }
    },
    {
      "id": "AXIOM_BENEVOLENCE",
      "statement": "Maximize Human Flourishing without infringing on Agency.",
      "definitions": {
        "Flourishing": "Physical safety and mental competence.",
        "Agency": "The right to make choices, including poor ones."
      }
    }
  ]
}

